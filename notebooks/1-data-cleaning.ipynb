{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Scan Data Cleaning\n",
    "##### June 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data cleaning pipeline for appropriate CSV preparation necessary for City Scan JavaScript plots with Cartagena, Colombia as the case study example city for pipeline scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory changes\n",
      "current working directory is: /Users/carolinecullinan/dev/wb/city-scan-csv-viz-prep\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# change to project root directory\n",
    "os.chdir('../')\n",
    "print(\"directory changes\")\n",
    "print(f\"current working directory is:\", os.getcwd())\n",
    "\n",
    "# local imports (after changing directory)\n",
    "from src.clean import clean_pg, clean_pas, clean_uba, clean_pug, clean_pv, clean_flood, clean_ee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POPULATION AND DEMOGRAPHIC TRENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pg.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pga\" / \"chart_pga\" ; and\n",
    "#### 2.) \"plot_pgp\" / \"chart_pg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population growth data info:\n",
      "Shape: (22, 7)\n",
      "Columns: ['Group', 'Location', 'Country', 'Year', 'Population', 'Source', 'Method']\n",
      "Date range: 2000 - 2021\n",
      "Data preview:\n",
      "       Group   Location   Country  Year  Population  Source  Method\n",
      "0  Cartagena  Cartagena  Colombia  2000    999576.9  Oxford  Oxford\n",
      "1  Cartagena  Cartagena  Colombia  2001   1012694.0  Oxford  Oxford\n",
      "2  Cartagena  Cartagena  Colombia  2002   1025077.0  Oxford  Oxford\n",
      "3  Cartagena  Cartagena  Colombia  2003   1036816.0  Oxford  Oxford\n",
      "4  Cartagena  Cartagena  Colombia  2004   1048314.0  Oxford  Oxford\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/pg.csv\n",
      "Years covered: 2000 - 2021\n",
      "Total data points: 22\n",
      "Population range: 999,576.9 - 1,259,382.0\n",
      "‚úÖ Population growth data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (22, 3)\n",
      "Cleaned data columns: ['yearName', 'population', 'populationGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   yearName  population  populationGrowthPercentage\n",
      "0      2000    999576.9                         NaN\n",
      "1      2001   1012694.0                       1.312\n",
      "2      2002   1025077.0                       1.223\n",
      "3      2003   1036816.0                       1.145\n",
      "4      2004   1048314.0                       1.109\n",
      "5      2005   1059620.0                       1.078\n",
      "6      2006   1074557.0                       1.410\n",
      "7      2007   1088791.0                       1.325\n",
      "8      2008   1102244.0                       1.236\n",
      "9      2009   1114637.0                       1.124\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 2000 - 2021\n",
      "- Population range: 999,576.9 - 1,259,382.0\n",
      "- Growth rate range: 0.419% - 1.410%\n",
      "‚ö†Ô∏è  Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pg.csv\n",
      "‚úÖ Ready for Observable visualization!\n"
     ]
    }
   ],
   "source": [
    "# POPULATION & DEMOGRAPHIC TRENDS - pg.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pga\"/\"chart_pga\" (absolute population growth); and \n",
    "# 2.) \"plot_pgp\"/\"chart_pgp\" (population growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pg = pd.read_csv('data/raw/population-growth.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw population growth data info:\")\n",
    "print(f\"Shape: {raw_df_pg.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pg.columns)}\")\n",
    "print(f\"Date range: {raw_df_pg['Year'].min()} - {raw_df_pg['Year'].max()}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pg.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_pg function in clean.py\n",
    "try:\n",
    "    cleaned_df_pg = clean_pg('data/raw/population-growth.csv')\n",
    "    print(\"‚úÖ Population growth data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pg.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pg.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pg.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pg.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pg['yearName'].min()} - {cleaned_df_pg['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pg['population'].min():,} - {cleaned_df_pg['population'].max():,}\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_pg['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pg['populationGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_pg['populationGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {cleaned_df_pg['populationGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning population growth data: {e}\")\n",
    "    print(\"Check that 'data/raw/population-growth.csv' exists and has the correct format\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pg.csv, and export\n",
    "# (this is handled automatically by the clean_pg function, but confirming)\n",
    "if 'cleaned_df_pg' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pg.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pas.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pas\" / \"chart_pas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population age structure data info:\n",
      "Shape: (36, 3)\n",
      "Columns: ['age_group', 'sex', 'population']\n",
      "Age groups: ['0-1', '1-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "Sex categories: ['f' 'm']\n",
      "Total population: 1,319,285\n",
      "Data preview:\n",
      "  age_group sex    population\n",
      "0       1-4   f  42167.329278\n",
      "1       1-4   m  45029.741031\n",
      "2       0-1   f  10386.217327\n",
      "3       0-1   m  11119.716253\n",
      "4       5-9   f  50745.108513\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  Warning: Could not sort by age bracket (\"['age_sort'] not found in axis\"). Using default sorting.\n",
      "Cleaned data saved to: data/processed/pas.csv\n",
      "Total population: 1,319,285\n",
      "Age brackets: 17\n",
      "Sex categories: 2\n",
      "Total records: 34\n",
      "‚úÖ Population age structure data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (34, 5)\n",
      "Cleaned data columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "Sample of cleaned data:\n",
      "  ageBracket     sex     count  percentage  yearName\n",
      "0        0-4  female  52553.55    3.983486      2021\n",
      "1        0-4    male  56149.46    4.256051      2021\n",
      "2      10-14  female  53631.29    4.065178      2021\n",
      "3      10-14    male  56959.22    4.317430      2021\n",
      "4      15-19  female  55799.87    4.229553      2021\n",
      "5      15-19    male  58415.47    4.427812      2021\n",
      "6      20-24  female  54857.68    4.158137      2021\n",
      "7      20-24    male  54082.97    4.099414      2021\n",
      "8      25-29  female  55018.60    4.170334      2021\n",
      "9      25-29    male  52944.24    4.013101      2021\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Age brackets: ['0-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "- Sex categories: ['female', 'male']\n",
      "- Population count range: 7,923 - 58,415\n",
      "- Percentage range: 0.601% - 4.428%\n",
      "- Year: 2021\n",
      "- Total percentage sum: 100.000% (should be ~100%)\n",
      "- Population by sex: Female: 663,596, Male: 655,689\n",
      "- Age brackets: 17, Total records: 34\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pas.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "- Records per sex: 17, 17\n",
      "- Data types: {'ageBracket': dtype('O'), 'sex': dtype('O'), 'count': dtype('float64'), 'percentage': dtype('float64'), 'yearName': dtype('int64')}\n"
     ]
    }
   ],
   "source": [
    "# POPULATION AGE SEX - pas.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pas\"/\"chart_pas\" (population age sex, i.e., population by sex and age bracket, (i.e., Population Distribution by Age & Sex, xxxx))\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pas = pd.read_csv('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_demographics.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw population age structure data info:\")\n",
    "print(f\"Shape: {raw_df_pas.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pas.columns)}\")\n",
    "print(f\"Age groups: {sorted(raw_df_pas['age_group'].unique())}\")\n",
    "print(f\"Sex categories: {raw_df_pas['sex'].unique()}\")\n",
    "print(f\"Total population: {raw_df_pas['population'].sum():,.0f}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pas.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_pas function in clean.py\n",
    "try:\n",
    "    cleaned_df_pas = clean_pas('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_demographics.csv')\n",
    "    print(\"‚úÖ Population age structure data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pas.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pas.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pas.isnull().sum().sum()}\")\n",
    "    print(f\"- Age brackets: {sorted(cleaned_df_pas['ageBracket'].unique())}\")\n",
    "    print(f\"- Sex categories: {sorted(cleaned_df_pas['sex'].unique())}\")\n",
    "    print(f\"- Population count range: {cleaned_df_pas['count'].min():,.0f} - {cleaned_df_pas['count'].max():,.0f}\")\n",
    "    print(f\"- Percentage range: {cleaned_df_pas['percentage'].min():.3f}% - {cleaned_df_pas['percentage'].max():.3f}%\")\n",
    "    print(f\"- Year: {cleaned_df_pas['yearName'].iloc[0]}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    total_percentage = cleaned_df_pas['percentage'].sum()\n",
    "    print(f\"- Total percentage sum: {total_percentage:.3f}% (should be ~100%)\")\n",
    "    \n",
    "    if abs(total_percentage - 100) > 0.1:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Percentage sum deviates from 100% by {abs(total_percentage - 100):.3f}%\")\n",
    "    \n",
    "    # check for balanced sex representation\n",
    "    sex_counts = cleaned_df_pas.groupby('sex')['count'].sum()\n",
    "    print(f\"- Population by sex: Female: {sex_counts.get('female', 0):,.0f}, Male: {sex_counts.get('male', 0):,.0f}\")\n",
    "    \n",
    "    # check age bracket coverage\n",
    "    expected_brackets = len(cleaned_df_pas['ageBracket'].unique())\n",
    "    actual_records = len(cleaned_df_pas)\n",
    "    print(f\"- Age brackets: {expected_brackets}, Total records: {actual_records}\")\n",
    "    \n",
    "    if actual_records != expected_brackets * 2:  # Should be 2 records per age bracket (male/female)\n",
    "        print(f\"‚ö†Ô∏è  Note: Expected {expected_brackets * 2} records (2 per age bracket), found {actual_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning population age structure data: {e}\")\n",
    "    print(\"Check that the demographics CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: age_group, sex, population\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pas.csv, and export\n",
    "# (this is handled automatically by the clean_pas function, but confirming)\n",
    "if 'cleaned_df_pas' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pas.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"- Records per sex: {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'female'])}, {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'male'])}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pas.dtypes)}\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILT FORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uba.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ubaa\" / \"chart_ubaa\" ; and\n",
    "#### 2.) \"plot_ubap\" / \"chart_ubap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw urban built area data info:\n",
      "Shape: (31, 2)\n",
      "Columns: ['year', 'cumulative sq km']\n",
      "Year range: 1985 - 2015\n",
      "UBA range: 98.56 - 184.92 sq km\n",
      "Total data points: 31\n",
      "Data preview:\n",
      "   year  cumulative sq km\n",
      "0  1985         98.562692\n",
      "1  1986        100.892675\n",
      "2  1987        103.206772\n",
      "3  1988        104.745090\n",
      "4  1989        106.680565\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/uba.csv\n",
      "Years covered: 1985 - 2015\n",
      "Total data points: 31\n",
      "UBA range: 98.56 - 184.92 sq km\n",
      "‚úÖ Urban built area data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (31, 4)\n",
      "Cleaned data columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   year  yearName     uba  ubaGrowthPercentage\n",
      "0     1      1985   98.56                  NaN\n",
      "1     2      1986  100.89                2.364\n",
      "2     3      1987  103.21                2.300\n",
      "3     4      1988  104.75                1.492\n",
      "4     5      1989  106.68                1.842\n",
      "5     6      1990  109.99                3.103\n",
      "6     7      1991  115.45                4.964\n",
      "7     8      1992  117.07                1.403\n",
      "8     9      1993  119.32                1.922\n",
      "9    10      1994  120.69                1.148\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 1985 - 2015\n",
      "- UBA range: 98.56 - 184.92 sq km\n",
      "- Growth rate range: 0.655% - 5.367%\n",
      "- Total urban expansion: 86.36 sq km over 30 years\n",
      "\n",
      "Urban growth analysis:\n",
      "- Average annual UBA growth rate: 2.125%\n",
      "‚ö†Ô∏è  Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/uba.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "- Time series length: 31 years\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'uba': dtype('float64'), 'ubaGrowthPercentage': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "# URBAN EXTENT AND CHANGE - uba.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ubaa\"/\"chart_ubaa\" (absolute urban extent and change)\n",
    "# 2.) \"plot_ubap\"/\"chart_ubap\" (urban extent and change growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_uba = pd.read_csv('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_wsf_stats.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw urban built area data info:\")\n",
    "print(f\"Shape: {raw_df_uba.shape}\")\n",
    "print(f\"Columns: {list(raw_df_uba.columns)}\")\n",
    "print(f\"Year range: {raw_df_uba['year'].min()} - {raw_df_uba['year'].max()}\")\n",
    "print(f\"UBA range: {raw_df_uba['cumulative sq km'].min():.2f} - {raw_df_uba['cumulative sq km'].max():.2f} sq km\")\n",
    "print(f\"Total data points: {len(raw_df_uba)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_uba.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_uba function in clean.py\n",
    "try:\n",
    "    cleaned_df_uba = clean_uba('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_wsf_stats.csv')\n",
    "    print(\"‚úÖ Urban built area data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_uba.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_uba.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_uba.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_uba['yearName'].min()} - {cleaned_df_uba['yearName'].max()}\")\n",
    "    print(f\"- UBA range: {cleaned_df_uba['uba'].min():.2f} - {cleaned_df_uba['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_uba['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_uba['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- Total urban expansion: {cleaned_df_uba['uba'].max() - cleaned_df_uba['uba'].min():.2f} sq km over {cleaned_df_uba['yearName'].max() - cleaned_df_uba['yearName'].min()} years\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nUrban growth analysis:\")\n",
    "    # Calculate average annual growth rate\n",
    "    avg_growth = cleaned_df_uba['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual UBA growth rate: {avg_growth:.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_uba['ubaGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {cleaned_df_uba['ubaGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "    # check for negative growth (urban area should generally increase)\n",
    "    negative_growth = cleaned_df_uba[cleaned_df_uba['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(negative_growth)} years with negative UBA growth detected\")\n",
    "        print(f\"   Years with decline: {negative_growth['yearName'].tolist()}\")\n",
    "      \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning urban built area data: {e}\")\n",
    "    print(\"Check that the UBA CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, cumulative sq km\")\n",
    "\n",
    "# save the cleaned data as a CSV file - uba.csv, and export\n",
    "# (this is handled automatically by the clean_uba function, but confirming)\n",
    "if 'cleaned_df_uba' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/uba.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_uba)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_uba.dtypes)}\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URBAN DEVELOPMENT DYNAMICS MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pug.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_uddm\" / \"chart_uddm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Checking prerequisite files...\n",
      "‚úÖ Population growth file found: data/processed/pg.csv\n",
      "‚úÖ Urban built area file found: data/processed/uba.csv\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚úÖ Successfully loaded population growth data: 22 records\n",
      "‚úÖ Successfully loaded urban built area data: 31 records\n",
      "‚úÖ Successfully merged datasets: 16 overlapping years\n",
      "Cleaned data saved to: data/processed/pug.csv\n",
      "Years covered: 2000 - 2015\n",
      "Total data points: 16\n",
      "Population range: 999,576.9 - 1,176,843.0\n",
      "UBA range: 143.54 - 184.92\n",
      "Density range: 6364.1 - 6963.8\n",
      "‚ö†Ô∏è  Note: 1 missing growth ratios (likely due to zero UBA growth)\n",
      "‚úÖ Population urban growth data merged and cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (16, 8)\n",
      "Cleaned data columns: ['yearName', 'population', 'populationGrowthPercentage', 'year', 'uba', 'ubaGrowthPercentage', 'density', 'populationUrbanGrowthRatio']\n",
      "Sample of cleaned data:\n",
      "   yearName  population  populationGrowthPercentage  year     uba  \\\n",
      "0      2000    999576.9                         NaN    16  143.54   \n",
      "1      2001   1012694.0                       1.312    17  145.80   \n",
      "2      2002   1025077.0                       1.223    18  147.57   \n",
      "3      2003   1036816.0                       1.145    19  149.55   \n",
      "4      2004   1048314.0                       1.109    20  151.97   \n",
      "5      2005   1059620.0                       1.078    21  154.07   \n",
      "6      2006   1074557.0                       1.410    22  156.57   \n",
      "7      2007   1088791.0                       1.325    23  159.13   \n",
      "8      2008   1102244.0                       1.236    24  162.38   \n",
      "9      2009   1114637.0                       1.124    25  166.04   \n",
      "\n",
      "   ubaGrowthPercentage   density populationUrbanGrowthRatio  \n",
      "0                1.961  6963.752                        NaN  \n",
      "1                1.574  6945.775                      0.834  \n",
      "2                1.214  6946.378                      1.007  \n",
      "3                1.342  6932.905                      0.853  \n",
      "4                1.618  6898.164                      0.685  \n",
      "5                1.382  6877.523                       0.78  \n",
      "6                1.623  6863.109                      0.869  \n",
      "7                1.635  6842.148                       0.81  \n",
      "8                2.042  6788.053                      0.605  \n",
      "9                2.254  6713.063                      0.499  \n",
      "\n",
      "Data validation:\n",
      "- Missing values: 2\n",
      "- Year range: 2000 - 2015\n",
      "- Population range: 999,576.9 - 1,176,843.0\n",
      "- UBA range: 143.54 - 184.92 sq km\n",
      "- Density range: 6364.1 - 6963.8 people/sq km\n",
      "\n",
      "Population vs Urban Growth Analysis:\n",
      "- Population growth rate range: 0.817% - 1.410%\n",
      "- UBA growth rate range: 1.033% - 2.254%\n",
      "- Average annual population growth: 1.094%\n",
      "- Average annual UBA growth: 1.720%\n",
      "- Population/Urban growth ratio range: 0.413 - 1.007\n",
      "- Average growth ratio: 0.680\n",
      "  üìâ Urban area growing faster than population (sprawl)\n",
      "\n",
      "Urban Density Analysis:\n",
      "- Starting density (2000): 6,963.8 people/sq km\n",
      "- Ending density (2015): 6,364.1 people/sq km\n",
      "- Total density change: -599.7 people/sq km (-8.6%)\n",
      "\n",
      "Data Quality Checks:\n",
      "‚ö†Ô∏è  Note: 1 missing growth ratio values (likely due to zero UBA growth)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pug.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['yearName', 'population', 'populationGrowthPercentage', 'year', 'uba', 'ubaGrowthPercentage', 'density', 'populationUrbanGrowthRatio']\n",
      "- Time series length: 16 years\n",
      "- Data types: {'yearName': dtype('int64'), 'population': dtype('float64'), 'populationGrowthPercentage': dtype('float64'), 'year': dtype('int64'), 'uba': dtype('float64'), 'ubaGrowthPercentage': dtype('float64'), 'density': dtype('float64'), 'populationUrbanGrowthRatio': dtype('O')}\n",
      "- Overlapping years between datasets: 16 out of potential maximum\n",
      "\n",
      "üìà Summary statistics:\n",
      "- Total population growth over period: 17.7%\n",
      "- Total urban area growth over period: 28.8%\n",
      "- NET population density change: -8.6%\n"
     ]
    }
   ],
   "source": [
    "# URBAN DEVELOPMENT DYNAMICS MATRIX: POPULATION URBAN GROWTH RATIO - pug.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_uddm\"/\"chart_ud\" (population vs urban growth analysis)\n",
    "\n",
    "# prereqs: ensure pg.csv and uba.csv have been generated from clean_pg and clean_uba functions\n",
    "print(\"üìã Checking prerequisite files...\")\n",
    "\n",
    "# check if required input files exist\n",
    "import os\n",
    "pg_file = 'data/processed/pg.csv'\n",
    "uba_file = 'data/processed/uba.csv'\n",
    "\n",
    "if os.path.exists(pg_file):\n",
    "    print(f\"‚úÖ Population growth file found: {pg_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Population growth file missing: {pg_file}\")\n",
    "    print(\"   Run clean_pg function first to generate this file\")\n",
    "\n",
    "if os.path.exists(uba_file):\n",
    "    print(f\"‚úÖ Urban built area file found: {uba_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Urban built area file missing: {uba_file}\")\n",
    "    print(\"   Run clean_uba function first to generate this file\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean and merge the data using the clean_pug function in clean.py\n",
    "try:\n",
    "    cleaned_df_pug = clean_pug()  # uses default paths: pg.csv and uba.csv\n",
    "    print(\"‚úÖ Population urban growth data merged and cleaned successfully!\")\n",
    "    \n",
    "    # Display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pug.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pug.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pug.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pug.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pug['yearName'].min()} - {cleaned_df_pug['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pug['population'].min():,} - {cleaned_df_pug['population'].max():,}\")\n",
    "    print(f\"- UBA range: {cleaned_df_pug['uba'].min():.2f} - {cleaned_df_pug['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Density range: {cleaned_df_pug['density'].min():.1f} - {cleaned_df_pug['density'].max():.1f} people/sq km\")\n",
    "    \n",
    "    # population vs urban growth analysis\n",
    "    print(f\"\\nPopulation vs Urban Growth Analysis:\")\n",
    "    print(f\"- Population growth rate range: {cleaned_df_pug['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pug['populationGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- UBA growth rate range: {cleaned_df_pug['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_pug['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # calculate averages (excluding NaN values)\n",
    "    avg_pop_growth = cleaned_df_pug['populationGrowthPercentage'].mean()\n",
    "    avg_uba_growth = cleaned_df_pug['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual population growth: {avg_pop_growth:.3f}%\")\n",
    "    print(f\"- Average annual UBA growth: {avg_uba_growth:.3f}%\")\n",
    "    \n",
    "    # growth ratio analysis\n",
    "    valid_ratios = cleaned_df_pug['populationUrbanGrowthRatio'].dropna()\n",
    "    if len(valid_ratios) > 0:\n",
    "        print(f\"- Population/Urban growth ratio range: {valid_ratios.min():.3f} - {valid_ratios.max():.3f}\")\n",
    "        print(f\"- Average growth ratio: {valid_ratios.mean():.3f}\")\n",
    "        \n",
    "        # interpret the growth patterns\n",
    "        if valid_ratios.mean() > 1:\n",
    "            print(\"  üìà Population growing faster than urban area (densification)\")\n",
    "        elif valid_ratios.mean() < 1:\n",
    "            print(\"  üìâ Urban area growing faster than population (sprawl)\")\n",
    "        else:\n",
    "            print(\"  ‚öñÔ∏è  Balanced population and urban growth\")\n",
    "    \n",
    "    # density analysis\n",
    "    print(f\"\\nUrban Density Analysis:\")\n",
    "    density_change = cleaned_df_pug['density'].iloc[-1] - cleaned_df_pug['density'].iloc[0]\n",
    "    density_change_pct = (density_change / cleaned_df_pug['density'].iloc[0]) * 100\n",
    "    print(f\"- Starting density ({cleaned_df_pug['yearName'].iloc[0]}): {cleaned_df_pug['density'].iloc[0]:,.1f} people/sq km\")\n",
    "    print(f\"- Ending density ({cleaned_df_pug['yearName'].iloc[-1]}): {cleaned_df_pug['density'].iloc[-1]:,.1f} people/sq km\")\n",
    "    print(f\"- Total density change: {density_change:+.1f} people/sq km ({density_change_pct:+.1f}%)\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # check for missing ratios\n",
    "    missing_ratios = cleaned_df_pug['populationUrbanGrowthRatio'].isna().sum()\n",
    "    if missing_ratios > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {missing_ratios} missing growth ratio values (likely due to zero UBA growth)\")\n",
    "        zero_uba_growth = cleaned_df_pug[cleaned_df_pug['ubaGrowthPercentage'] == 0]\n",
    "        if len(zero_uba_growth) > 0:\n",
    "            print(f\"   Years with zero UBA growth: {zero_uba_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # check for negative population growth\n",
    "    negative_pop_growth = cleaned_df_pug[cleaned_df_pug['populationGrowthPercentage'] < 0]\n",
    "    if len(negative_pop_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {len(negative_pop_growth)} years with population decline\")\n",
    "        print(f\"   Decline years: {negative_pop_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # check for negative urban growth\n",
    "    negative_uba_growth = cleaned_df_pug[cleaned_df_pug['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_uba_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(negative_uba_growth)} years with urban area decline\")\n",
    "        print(f\"   UBA decline years: {negative_uba_growth['yearName'].tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating population urban growth data: {e}\")\n",
    "    print(\"Check that both pg.csv and uba.csv files exist and have the correct format\")\n",
    "    print(\"Expected pg.csv columns: yearName, population, populationGrowthPercentage\")\n",
    "    print(\"Expected uba.csv columns: yearName, uba, ubaGrowthPercentage\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_pug' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pug.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable Notebook use\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pug.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_pug)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pug.dtypes)}\")\n",
    "    print(f\"- Overlapping years between datasets: {len(cleaned_df_pug)} out of potential maximum\")\n",
    "    \n",
    "    # summary statistics\n",
    "    print(f\"\\nüìà Summary statistics:\")\n",
    "    total_pop_growth = ((cleaned_df_pug['population'].iloc[-1] / cleaned_df_pug['population'].iloc[0]) - 1) * 100\n",
    "    total_uba_growth = ((cleaned_df_pug['uba'].iloc[-1] / cleaned_df_pug['uba'].iloc[0]) - 1) * 100\n",
    "    print(f\"- Total population growth over period: {total_pop_growth:.1f}%\")\n",
    "    print(f\"- Total urban area growth over period: {total_uba_growth:.1f}%\")\n",
    "    print(f\"- NET population density change: {density_change_pct:+.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure pg.csv exists (run clean_pg function)\")\n",
    "    print(\"   2. Ensure uba.csv exists (run clean_uba function)\")\n",
    "    print(\"   3. Check that both files have overlapping years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAND COVER (need alternative to donut chart - do tree map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_land_cover_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use the clean_land_cover_csv function from lc_cleanup.py to clean the tabular-output land cover csv file so that it can be plotted\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m clean_land_cover_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/2025-02-tunisia-tunis_02-process-output_tabular_tunis_lc.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_land_cover_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# use the clean_land_cover_csv function from lc_cleanup.py to clean the tabular-output land cover csv file so that it can be plotted\n",
    "clean_land_cover_csv('data/raw/2025-02-tunisia-tunis_02-process-output_tabular_tunis_lc.csv', 'data/processed/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# land cover data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "# (edit lc so that percentage is calculated from pixelCount and pixelTotal)\n",
    "lc = [\n",
    "      { \"lcType\": \"Built up\", \"pixelCount\": 3438092.2117647, \"pixelTotal\": 6068006.37255, \"percentage\": 56.66 },\n",
    "      { \"lcType\": \"Grassland\", \"pixelCount\": 1022591.69411765, \"pixelTotal\": 6068006.37255, \"percentage\": 16.85 },\n",
    "      { \"lcType\": \"Permanent water bodies\", \"pixelCount\": 563266.486274511, \"pixelTotal\": 6068006.37255, \"percentage\": 9.28},\n",
    "      { \"lcType\": \"Tree cover\", \"pixelCount\": 385035.090196078, \"pixelTotal\": 6068006.37255, \"percentage\": 6.345 },\n",
    "      { \"lcType\": \"Cropland\", \"pixelCount\": 346731.71372549, \"pixelTotal\": 6068006.37255, \"percentage\": 5.71},\n",
    "      { \"lcType\": \"Bare sparse vegetation\", \"pixelCount\": 168537.729411765, \"pixelTotal\": 6068006.37255, \"percentage\": 2.78},\n",
    "      { \"lcType\": \"Shrubland\", \"pixelCount\": 15153.5294117647, \"pixelTotal\": 6068006.37255, \"percentage\": 2.49},\n",
    "      { \"lcType\": \"Herbaceous wetland\", \"pixelCount\": 128597.917647059, \"pixelTotal\": 6068006.37255, \"percentage\": 2.12},\n",
    "      { \"lcType\": \"Snow and ice\", \"pixelCount\": 0, \"pixelTotal\": 6068006.37255, \"percentage\": 0},\n",
    "      { \"lcType\": \"Mangroves\", \"pixelCount\": 0, \"pixelTotal\": 6068006.37255, \"percentage\": 0},\n",
    "      { \"lcType\": \"Moss and lichens\", \"pixelCount\": 0, \"pixelTotal\": 6068006.37255, \"percentage\": 0},\n",
    "]\n",
    "\n",
    "# convert lc list to dataframe, lc_df\n",
    "lc_df = pd.DataFrame(lc)\n",
    "\n",
    "# create output CSV of lc_df for plotting\n",
    "lc_output_df = pd.DataFrame({\n",
    "    'lcType': lc_df['lcType'],\n",
    "    'pixelCount': lc_df['pixelCount'],\n",
    "    'pixelTotal': lc_df['pixelTotal'], \n",
    "    'percentage': lc_df['percentage']\n",
    "})\n",
    "\n",
    "# save lc_output_df for lc data to CSV\n",
    "lc_output_df.to_csv('data/processed/lc.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIMATE CONDITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pv.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pv\" / \"chart_pv\" (i.e., Seasonal availability of solar energy, January - December)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw photovoltaic potential data info:\n",
      "Shape: (12, 4)\n",
      "Columns: ['month', 'max', 'min', 'mean']\n",
      "Month range: 1 - 12\n",
      "PV max range: 4.07 - 5.47\n",
      "PV mean range: 3.92 - 5.35\n",
      "Total data points: 12\n",
      "Data preview:\n",
      "   month   max   min      mean\n",
      "0      1  5.33  5.05  5.213701\n",
      "1      2  5.47  5.17  5.350237\n",
      "2      3  5.20  4.81  5.032584\n",
      "3      4  4.72  4.20  4.486117\n",
      "4      5  4.16  3.80  3.969832\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/pv.csv\n",
      "Months covered: 12 months (full year)\n",
      "PV potential range: 4.07 - 5.47\n",
      "Peak month: Feb (5.47)\n",
      "Lowest month: Jun (4.07)\n",
      "Summer average (Jun-Aug): 4.20\n",
      "Winter average (Dec-Feb): 5.21\n",
      "Seasonal variation: -19.4% higher in summer\n",
      "‚úÖ Photovoltaic potential data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (12, 3)\n",
      "Cleaned data columns: ['month', 'monthName', 'maxPv']\n",
      "Sample of cleaned data:\n",
      "    month monthName  maxPv\n",
      "0       1       Jan   5.33\n",
      "1       2       Feb   5.47\n",
      "2       3       Mar   5.20\n",
      "3       4       Apr   4.72\n",
      "4       5       May   4.16\n",
      "5       6       Jun   4.07\n",
      "6       7       Jul   4.22\n",
      "7       8       Aug   4.31\n",
      "8       9       Sep   4.29\n",
      "9      10       Oct   4.19\n",
      "10     11       Nov   4.27\n",
      "11     12       Dec   4.84\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Month coverage: 12 months (should be 12)\n",
      "- Month range: 1 - 12\n",
      "- PV potential range: 4.07 - 5.47\n",
      "\n",
      "Solar Energy Analysis:\n",
      "- Peak solar month: Feb (5.47)\n",
      "- Lowest solar month: Jun (4.07)\n",
      "- Spring average (Mar-May): 4.69\n",
      "- Summer average (Jun-Aug): 4.20\n",
      "- Fall average (Sep-Nov): 4.25\n",
      "- Winter average (Dec-Feb): 5.21\n",
      "- Annual average: 4.59\n",
      "- Peak month deviation: +19.2% above average\n",
      "- Low month deviation: -11.3% below average\n",
      "- Summer/Winter ratio: 0.81x\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ Complete 12-month coverage\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pv.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['month', 'monthName', 'maxPv']\n",
      "- Time series type: Monthly (12 data points)\n",
      "- Data types: {'month': dtype('int64'), 'monthName': dtype('O'), 'maxPv': dtype('float64')}\n",
      "- Seasonal range: 0.81x variation from winter to summer\n",
      "\n",
      "üîã Solar Energy Data Summary:\n",
      "- Highest solar months: Feb, Jan, Mar\n",
      "- Lowest solar months: Jun, May, Oct\n"
     ]
    }
   ],
   "source": [
    "# PHOTOVOLTAIC POTENTIAL - pv.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pv\"/\"chart_pv\" (i.e., Seasonal availability of solar energy, January - December)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pv = pd.read_csv('data/raw/monthly-pv.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw photovoltaic potential data info:\")\n",
    "print(f\"Shape: {raw_df_pv.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pv.columns)}\")\n",
    "print(f\"Month range: {raw_df_pv['month'].min()} - {raw_df_pv['month'].max()}\")\n",
    "print(f\"PV max range: {raw_df_pv['max'].min():.2f} - {raw_df_pv['max'].max():.2f}\")\n",
    "print(f\"PV mean range: {raw_df_pv['mean'].min():.2f} - {raw_df_pv['mean'].max():.2f}\")\n",
    "print(f\"Total data points: {len(raw_df_pv)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pv.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using our clean_pv function\n",
    "try:\n",
    "    cleaned_df_pv = clean_pv('data/raw/monthly-pv.csv')\n",
    "    print(\"‚úÖ Photovoltaic potential data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pv.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pv.head(12))  # Show all 12 months\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pv.isnull().sum().sum()}\")\n",
    "    print(f\"- Month coverage: {len(cleaned_df_pv)} months (should be 12)\")\n",
    "    print(f\"- Month range: {cleaned_df_pv['month'].min()} - {cleaned_df_pv['month'].max()}\")\n",
    "    print(f\"- PV potential range: {cleaned_df_pv['maxPv'].min():.2f} - {cleaned_df_pv['maxPv'].max():.2f}\")\n",
    "    \n",
    "    # solar energy analysis\n",
    "    print(f\"\\nSolar Energy Analysis:\")\n",
    "    \n",
    "    # identify peak and low months\n",
    "    peak_month = cleaned_df_pv.loc[cleaned_df_pv['maxPv'].idxmax()]\n",
    "    low_month = cleaned_df_pv.loc[cleaned_df_pv['maxPv'].idxmin()]\n",
    "    \n",
    "    print(f\"- Peak solar month: {peak_month['monthName']} ({peak_month['maxPv']:.2f})\")\n",
    "    print(f\"- Lowest solar month: {low_month['monthName']} ({low_month['maxPv']:.2f})\")\n",
    "    \n",
    "    # seasonal analysis\n",
    "    spring_months = cleaned_df_pv[cleaned_df_pv['month'].isin([3, 4, 5])]  # Mar, Apr, May\n",
    "    summer_months = cleaned_df_pv[cleaned_df_pv['month'].isin([6, 7, 8])]  # Jun, Jul, Aug\n",
    "    fall_months = cleaned_df_pv[cleaned_df_pv['month'].isin([9, 10, 11])]  # Sep, Oct, Nov\n",
    "    winter_months = cleaned_df_pv[cleaned_df_pv['month'].isin([12, 1, 2])]  # Dec, Jan, Feb\n",
    "    \n",
    "    spring_avg = spring_months['maxPv'].mean()\n",
    "    summer_avg = summer_months['maxPv'].mean()\n",
    "    fall_avg = fall_months['maxPv'].mean()\n",
    "    winter_avg = winter_months['maxPv'].mean()\n",
    "    \n",
    "    print(f\"- Spring average (Mar-May): {spring_avg:.2f}\")\n",
    "    print(f\"- Summer average (Jun-Aug): {summer_avg:.2f}\")\n",
    "    print(f\"- Fall average (Sep-Nov): {fall_avg:.2f}\")\n",
    "    print(f\"- Winter average (Dec-Feb): {winter_avg:.2f}\")\n",
    "    \n",
    "    # calculate seasonal variations\n",
    "    annual_avg = cleaned_df_pv['maxPv'].mean()\n",
    "    peak_variation = ((cleaned_df_pv['maxPv'].max() - annual_avg) / annual_avg) * 100 # i.e, \"the best month six% better than the average\"\n",
    "    low_variation = ((annual_avg - cleaned_df_pv['maxPv'].min()) / annual_avg) * 100 # i.e, \"the worst month 10% worse than the average\"\n",
    "    \n",
    "    print(f\"- Annual average: {annual_avg:.2f}\")\n",
    "    print(f\"- Peak month deviation: +{peak_variation:.1f}% above average\")\n",
    "    print(f\"- Low month deviation: -{low_variation:.1f}% below average\")\n",
    "    \n",
    "    # energy planning insights\n",
    "    summer_winter_ratio = summer_avg / winter_avg\n",
    "    print(f\"- Summer/Winter ratio: {summer_winter_ratio:.2f}x\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # check for complete month coverage\n",
    "    expected_months = set(range(1, 13))\n",
    "    actual_months = set(cleaned_df_pv['month'].unique())\n",
    "    missing_months = expected_months - actual_months\n",
    "    \n",
    "    if missing_months:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Missing months: {sorted(missing_months)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Complete 12-month coverage\")\n",
    "    \n",
    "    # check for reasonable PV values\n",
    "    if cleaned_df_pv['maxPv'].min() < 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Negative PV values detected (minimum: {cleaned_df_pv['maxPv'].min():.2f})\")\n",
    "    \n",
    "    # identify unusual patterns\n",
    "    monthly_diff = cleaned_df_pv['maxPv'].diff().abs()\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning photovoltaic potential data: {e}\")\n",
    "    print(\"Check that the monthly-pv.csv file exists and has the correct format\")\n",
    "    print(\"Expected columns: month, max, min, mean\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pv.csv, and export\n",
    "# (this is handled automatically by the clean_pv function, but confirming)\n",
    "if 'cleaned_df_pv' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pv.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"- Time series type: Monthly (12 data points)\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pv.dtypes)}\")\n",
    "    print(f\"- Seasonal range: {summer_avg/winter_avg:.2f}x variation from winter to summer\")\n",
    "    \n",
    "    # renewable energy planning insights\n",
    "    print(f\"\\nüîã Solar Energy Data Summary:\")\n",
    "    print(f\"- Highest solar months: {', '.join(cleaned_df_pv.nlargest(3, 'maxPv')['monthName'].tolist())}\")\n",
    "    print(f\"- Lowest solar months: {', '.join(cleaned_df_pv.nsmallest(3, 'maxPv')['monthName'].tolist())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure monthly-pv.csv exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: month, max, min, mean\")\n",
    "    print(\"   3. Verify data covers all 12 months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISK IDENTIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fu.csv, pu.csv, cu.csv, and comb.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_fu\" / \"chart_fu\" (i.e., built-up area exposed to river (fluvial) flooding)\n",
    "#### 2.) \"plot_pu\" / \"chart_pu\" (i.e., built-up area exposed to rainwater (pluvial) flooding)\n",
    "#### 3.) \"plot_cu\" / \"chart_cu\" (i.e., built-up area exposed to coastal flooding)\n",
    "#### 4.) \"plot_comb\" / \"chart_comb\" (i.e., built-up area exposed to combined flooding)\n",
    "\n",
    "#### NOTE:\n",
    "#### 5.) data, raw, \"flood-events.csv\" is already ready for Observable Notebook \"plot_fe\" / \"chart_fe\" (i.e.,large flood events in city, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw flood risk data info:\n",
      "Shape: (31, 5)\n",
      "Columns: ['year', 'coastal_2020', 'comb_2020', 'fluvial_2020', 'pluvial_2020']\n",
      "Year range: 1985 - 2015\n",
      "Total data points: 31\n",
      "Available flood types: ['coastal_2020', 'comb_2020', 'fluvial_2020', 'pluvial_2020']\n",
      "Coastal risk range: 0.80 - 1.46\n",
      "Comb risk range: 20.59 - 49.09\n",
      "Fluvial risk range: 4.30 - 8.62\n",
      "Pluvial risk range: 16.62 - 41.52\n",
      "Data preview:\n",
      "   year  coastal_2020  comb_2020  fluvial_2020  pluvial_2020\n",
      "0  1985      0.802255  20.592107      4.299878     16.623193\n",
      "1  1986      0.856975  21.350234      4.404903     17.266586\n",
      "2  1987      0.887865  22.034225      4.447267     17.897623\n",
      "3  1988      0.892278  22.363424      4.524050     18.161511\n",
      "4  1989      0.919637  22.924738      4.593773     18.659280\n",
      "\n",
      "==================================================\n",
      "\n",
      "Available flood types: ['coastal', 'fluvial', 'pluvial', 'combined']\n",
      "‚úÖ Created cu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   CU range: 0.80 - 1.46\n",
      "‚úÖ Created fu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   FU range: 4.30 - 8.62\n",
      "‚úÖ Created pu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   PU range: 16.62 - 41.52\n",
      "‚úÖ Created comb.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   COMB range: 20.59 - 49.09\n",
      "\n",
      "Flood Risk Data Processing Summary:\n",
      "- Input file: data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_flood_wsf.csv\n",
      "- Output directory: data/processed\n",
      "- Files created: cu.csv, fu.csv, pu.csv, comb.csv\n",
      "- Missing flood types: set()\n",
      "\n",
      "Flood Risk Analysis:\n",
      "- Coastal flood risk:\n",
      "  Average: 1.17, Range: 0.80 - 1.46\n",
      "  Trend (1985-2015): +0.66 (+increase)\n",
      "- Fluvial flood risk:\n",
      "  Average: 6.35, Range: 4.30 - 8.62\n",
      "  Trend (1985-2015): +4.32 (+increase)\n",
      "- Pluvial flood risk:\n",
      "  Average: 28.05, Range: 16.62 - 41.52\n",
      "  Trend (1985-2015): +24.90 (+increase)\n",
      "- Combined flood risk:\n",
      "  Average: 33.75, Range: 20.59 - 49.09\n",
      "  Trend (1985-2015): +28.49 (+increase)\n",
      "- Dominant risk type (2015): Combined (49.09)\n",
      "‚úÖ Flood risk data processed successfully!\n",
      "\n",
      "üìä cu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'cu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 0.80 - 1.46\n",
      "- Sample data:\n",
      "   year  yearName    cu\n",
      "0     1      1985  0.80\n",
      "1     2      1986  0.86\n",
      "2     3      1987  0.89\n",
      "3     4      1988  0.89\n",
      "4     5      1989  0.92\n",
      "\n",
      "üìä fu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'fu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 4.30 - 8.62\n",
      "- Sample data:\n",
      "   year  yearName    fu\n",
      "0     1      1985  4.30\n",
      "1     2      1986  4.40\n",
      "2     3      1987  4.45\n",
      "3     4      1988  4.52\n",
      "4     5      1989  4.59\n",
      "\n",
      "üìä pu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'pu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 16.62 - 41.52\n",
      "- Sample data:\n",
      "   year  yearName     pu\n",
      "0     1      1985  16.62\n",
      "1     2      1986  17.27\n",
      "2     3      1987  17.90\n",
      "3     4      1988  18.16\n",
      "4     5      1989  18.66\n",
      "\n",
      "üìä comb.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'comb']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 20.59 - 49.09\n",
      "- Sample data:\n",
      "   year  yearName   comb\n",
      "0     1      1985  20.59\n",
      "1     2      1986  21.35\n",
      "2     3      1987  22.03\n",
      "3     4      1988  22.36\n",
      "4     5      1989  22.92\n",
      "\n",
      "üåä Flood Risk Data Summary:\n",
      "\n",
      "- CU flood risk:\n",
      "  Average: 1.17\n",
      "  Range: 0.80 - 1.46\n",
      "  Standard deviation: 0.19\n",
      "  Change (1985-2015): +0.66\n",
      "\n",
      "- FU flood risk:\n",
      "  Average: 6.35\n",
      "  Range: 4.30 - 8.62\n",
      "  Standard deviation: 1.33\n",
      "  Change (1985-2015): +4.32\n",
      "\n",
      "- PU flood risk:\n",
      "  Average: 28.05\n",
      "  Range: 16.62 - 41.52\n",
      "  Standard deviation: 7.75\n",
      "  Change (1985-2015): +24.90\n",
      "\n",
      "- COMB flood risk:\n",
      "  Average: 33.75\n",
      "  Range: 20.59 - 49.09\n",
      "  Standard deviation: 8.86\n",
      "  Change (1985-2015): +28.50\n",
      "\n",
      "üìä 2015 Risk Values:\n",
      "- CU: 1.46\n",
      "- FU: 8.62\n",
      "- PU: 41.52\n",
      "- COMB: 49.09\n",
      "\n",
      "üîç Data Quality Checks:\n",
      "‚úÖ No data quality issues detected\n",
      "\n",
      "üìÅ Cleaned data saved to data/processed/:\n",
      "   ‚úÖ cu.csv\n",
      "   ‚úÖ fu.csv\n",
      "   ‚úÖ pu.csv\n",
      "   ‚úÖ comb.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Files created: 4\n",
      "- Time series length: 31 years\n",
      "- Year range: 1985 - 2015\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'cu': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "# FLOODING - Multiple CSV preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_fu\"/\"chart_fu\" (i.e., built-up area exposed to river (fluvial) flooding)\n",
    "# 2.) \"plot_pu\"/\"chart_pu\" (i.e., built-up area exposed to rainwater (pluvial) flooding)\n",
    "# 3.) \"plot_cu\"/\"chart_cu\" (i.e., built-up area exposed to coastal flooding)\n",
    "# 4.) \"plot_comb\"/\"chart_comb\" (i.e., built-up area exposed to combined flooding)\n",
    "#5.) NOTE: data, raw, \"flood-events.csv\" is already ready for Observable Notebook \"plot_fe\" / \"chart_fe\" (i.e.,large flood events in city, country)\n",
    "\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "# NOTE: Flood data structure may vary by city - coastal cities have coastal flood data, inland cities do not\n",
    "raw_df_flood = pd.read_csv('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_flood_wsf.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw flood risk data info:\")\n",
    "print(f\"Shape: {raw_df_flood.shape}\")\n",
    "print(f\"Columns: {list(raw_df_flood.columns)}\")\n",
    "print(f\"Year range: {raw_df_flood['year'].min()} - {raw_df_flood['year'].max()}\")\n",
    "print(f\"Total data points: {len(raw_df_flood)}\")\n",
    "\n",
    "# identify available flood types\n",
    "flood_columns = [col for col in raw_df_flood.columns if '_2020' in col]\n",
    "print(f\"Available flood types: {flood_columns}\")\n",
    "\n",
    "# preview flood risk ranges for each type\n",
    "for col in flood_columns:\n",
    "    flood_type = col.replace('_2020', '')\n",
    "    print(f\"{flood_type.capitalize()} risk range: {raw_df_flood[col].min():.2f} - {raw_df_flood[col].max():.2f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_flood.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_flood function in clean.py\n",
    "try:\n",
    "    created_files = clean_flood('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_flood_wsf.csv')\n",
    "    print(\"‚úÖ Flood risk data processed successfully!\")\n",
    "    \n",
    "    # load and validate each created file\n",
    "    flood_dataframes = {}\n",
    "    \n",
    "    for filename in created_files:\n",
    "        file_path = f'data/processed/{filename}'\n",
    "        flood_type = filename.replace('.csv', '')\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            flood_dataframes[flood_type] = df\n",
    "            \n",
    "            print(f\"\\nüìä {filename} validation:\")\n",
    "            print(f\"- Shape: {df.shape}\")\n",
    "            print(f\"- Columns: {list(df.columns)}\")\n",
    "            print(f\"- Year range: {df['yearName'].min()} - {df['yearName'].max()}\")\n",
    "            print(f\"- Risk range: {df.iloc[:, 2].min():.2f} - {df.iloc[:, 2].max():.2f}\")  # third column is the risk value\n",
    "            print(f\"- Sample data:\")\n",
    "            print(df.head(5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "    \n",
    "    # basic flood risk analysis - just report the numbers\n",
    "    if len(flood_dataframes) > 0:\n",
    "        print(f\"\\nüåä Flood Risk Data Summary:\")\n",
    "        \n",
    "        for flood_type, df in flood_dataframes.items():\n",
    "            risk_column = df.columns[2]  # third column contains risk values\n",
    "            \n",
    "            # basic statistics\n",
    "            avg_risk = df[risk_column].mean()\n",
    "            max_risk = df[risk_column].max()\n",
    "            min_risk = df[risk_column].min()\n",
    "            std_risk = df[risk_column].std()\n",
    "            \n",
    "            # trend calculation\n",
    "            trend = df[risk_column].iloc[-1] - df[risk_column].iloc[0]\n",
    "            \n",
    "            print(f\"\\n- {flood_type.upper()} flood risk:\")\n",
    "            print(f\"  Average: {avg_risk:.2f}\")\n",
    "            print(f\"  Range: {min_risk:.2f} - {max_risk:.2f}\")\n",
    "            print(f\"  Standard deviation: {std_risk:.2f}\")\n",
    "            print(f\"  Change (1985-2015): {trend:+.2f}\")\n",
    "        \n",
    "        # current values\n",
    "        if len(flood_dataframes) > 1:\n",
    "            print(f\"\\nüìä 2015 Risk Values:\")\n",
    "            for flood_type, df in flood_dataframes.items():\n",
    "                risk_column = df.columns[2]\n",
    "                current_risk = df[risk_column].iloc[-1]\n",
    "                print(f\"- {flood_type.upper()}: {current_risk:.2f}\")\n",
    "        \n",
    "        # data quality checks\n",
    "        print(f\"\\nüîç Data Quality Checks:\")\n",
    "        \n",
    "        quality_issues = 0\n",
    "        for flood_type, df in flood_dataframes.items():\n",
    "            risk_column = df.columns[2]\n",
    "            \n",
    "            # check for missing values\n",
    "            missing_values = df[risk_column].isna().sum()\n",
    "            if missing_values > 0:\n",
    "                print(f\"‚ö†Ô∏è  {flood_type.upper()}: {missing_values} missing values\")\n",
    "                quality_issues += 1\n",
    "            \n",
    "            # check for negative values (mathematically impossible for risk)\n",
    "            negative_values = (df[risk_column] < 0).sum()\n",
    "            if negative_values > 0:\n",
    "                print(f\"‚ö†Ô∏è  {flood_type.upper()}: {negative_values} negative risk values\")\n",
    "                quality_issues += 1\n",
    "        \n",
    "        if quality_issues == 0:\n",
    "            print(\"‚úÖ No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing flood risk data: {e}\")\n",
    "    print(\"Check that the flood CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, coastal_2020, fluvial_2020, pluvial_2020, comb_2020 (as available)\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'created_files' in locals() and len(created_files) > 0:\n",
    "    print(f\"\\nüìÅ Cleaned data saved to data/processed/:\")\n",
    "    for filename in created_files:\n",
    "        print(f\"   ‚úÖ {filename}\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Files created: {len(created_files)}\")\n",
    "    print(f\"- Time series length: {len(list(flood_dataframes.values())[0]) if flood_dataframes else 'N/A'} years\")\n",
    "    print(f\"- Year range: {raw_df_flood['year'].min()} - {raw_df_flood['year'].max()}\")\n",
    "    print(f\"- Data types: {dict(list(flood_dataframes.values())[0].dtypes) if flood_dataframes else 'N/A'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned flood data available\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure flood CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns with '_2020' suffix\")\n",
    "    print(\"   3. Verify data covers expected time range (1985-2015)\")\n",
    "    print(\"   4. Check that at least one flood type column exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ee.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ee\" / \"chart_ee\" (i.e., Significant earthquakes within 500 km since 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw earthquake events data info:\n",
      "Shape: (15, 13)\n",
      "Columns: ['BEGAN', 'text', 'line1', 'line2', 'line3', 'line4', 'above_line', 'distance', 'eqMagnitude', 'magXdist', 'location', 'node_x', 'node_y']\n",
      "Total data points: 15\n",
      "Magnitude range: 4.9 - 7.4\n",
      "Distance range: 284 - 491 km\n",
      "Data preview:\n",
      "        BEGAN                                               text        line1  \\\n",
      "0  1919-08-19           AUGUST 1919; MNA; 424 km away; NA damage  AUGUST 1919   \n",
      "1  1950-07-09  JULY 1950; M6.1; 431 km away; Severe damage; 3...    JULY 1950   \n",
      "2  1961-06-16            JUNE 1961; M6.5; 284 km away; NA damage    JUNE 1961   \n",
      "3  1967-07-29  JULY 1967; M6.8; 491 km away; Moderate damage;...    JULY 1967   \n",
      "4  1974-04-18  APRIL 1974; M5; 484 km away; Moderate damage; ...   APRIL 1974   \n",
      "\n",
      "               line2            line3           line4  above_line    distance  \\\n",
      "0   MNA; 424 km away        NA damage             NaN          -1  423.516045   \n",
      "1  M6.1; 431 km away    Severe damage  300 fatalities           1  430.758880   \n",
      "2  M6.5; 284 km away        NA damage             NaN          -1  284.421667   \n",
      "3  M6.8; 491 km away  Moderate damage   20 fatalities           1  490.666578   \n",
      "4    M5; 484 km away  Moderate damage    4 fatalities          -1  483.666373   \n",
      "\n",
      "   eqMagnitude      magXdist                 location      node_x       node_y  \n",
      "0          NaN           NaN              , Maracaibo  1904-05-25 -4100.067699  \n",
      "1          6.1   9572.370830                Anboledas  1965-10-01  4100.067699  \n",
      "2          6.5  11151.259161               n Colombia  1946-03-23 -4100.067699  \n",
      "3          6.8  10263.467270                      NaN  1982-10-21  4100.067699  \n",
      "4          5.0   7581.668134  Ne, Cepita, San Andreas  1959-01-23 -4100.067699  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/ee.csv\n",
      "Earthquake events: 15\n",
      "Year range: 1919 - 2018\n",
      "Magnitude range: 4.9 - 7.4\n",
      "Distance range: 284 - 491 km\n",
      "‚úÖ Earthquake events data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (15, 7)\n",
      "Cleaned data columns: ['begin_year', 'distance', 'eqMagnitude', 'text', 'line1', 'line2', 'line3']\n",
      "Sample of cleaned data:\n",
      "   begin_year  distance  eqMagnitude  \\\n",
      "0        1919       424          NaN   \n",
      "1        1950       431          6.1   \n",
      "2        1961       284          6.5   \n",
      "3        1967       491          6.8   \n",
      "4        1974       484          5.0   \n",
      "5        1974       386          7.3   \n",
      "6        1976       445          6.7   \n",
      "7        1977       357          6.5   \n",
      "8        1980       426          5.0   \n",
      "9        1981       414          5.4   \n",
      "\n",
      "                                                text          line1  \\\n",
      "0           AUGUST 1919; MNA; 424 km away; NA damage    AUGUST 1919   \n",
      "1  JULY 1950; M6.1; 431 km away; Severe damage; 3...      JULY 1950   \n",
      "2            JUNE 1961; M6.5; 284 km away; NA damage      JUNE 1961   \n",
      "3  JULY 1967; M6.8; 491 km away; Moderate damage;...      JULY 1967   \n",
      "4  APRIL 1974; M5; 484 km away; Moderate damage; ...     APRIL 1974   \n",
      "5  JULY 1974; M7.3; 386 km away; Limited damage; ...      JULY 1974   \n",
      "6  JULY 1976; M6.7; 445 km away; Limited damage; ...      JULY 1976   \n",
      "7  AUGUST 1977; M6.5; 357 km away; Moderate damag...    AUGUST 1977   \n",
      "8    NOVEMBER 1980; M5; 426 km away; Moderate damage  NOVEMBER 1980   \n",
      "9  OCTOBER 1981; M5.4; 414 km away; Moderate dama...   OCTOBER 1981   \n",
      "\n",
      "               line2            line3  \n",
      "0   MNA; 424 km away        NA damage  \n",
      "1  M6.1; 431 km away    Severe damage  \n",
      "2  M6.5; 284 km away        NA damage  \n",
      "3  M6.8; 491 km away  Moderate damage  \n",
      "4    M5; 484 km away  Moderate damage  \n",
      "5  M7.3; 386 km away   Limited damage  \n",
      "6  M6.7; 445 km away   Limited damage  \n",
      "7  M6.5; 357 km away  Moderate damage  \n",
      "8    M5; 426 km away  Moderate damage  \n",
      "9  M5.4; 414 km away  Moderate damage  \n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 1919 - 2018\n",
      "- Magnitude range: 4.9 - 7.4\n",
      "- Distance range: 284 - 491 km\n",
      "\n",
      "Earthquake Data Summary:\n",
      "- Total events: 15\n",
      "- Average magnitude: 6.2\n",
      "- Average distance: 426 km\n",
      "- Standard deviation (magnitude): 0.8\n",
      "- Standard deviation (distance): 55 km\n",
      "- Time span: 99 years\n",
      "- Average events per decade: 1.5\n",
      "- Strongest earthquake: 7.4 magnitude in 1992\n",
      "- Closest earthquake: 284 km in 1961\n",
      "\n",
      "Data Quality Checks:\n",
      "‚ö†Ô∏è  Missing magnitude values: 1\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/ee.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['begin_year', 'distance', 'eqMagnitude', 'text', 'line1', 'line2', 'line3']\n",
      "- Time series length: 15 events\n",
      "- Year range: 1919 - 2018\n",
      "- Data types: {'begin_year': dtype('int64'), 'distance': Int64Dtype(), 'eqMagnitude': dtype('float64'), 'text': dtype('O'), 'line1': dtype('O'), 'line2': dtype('O'), 'line3': dtype('O')}\n"
     ]
    }
   ],
   "source": [
    "# EARTHQUAKE EVENTS - ee.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ee\"/\"chart_ee\" (Significant Earthquakes within 500 km since 1900)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_ee = pd.read_csv('data/raw/earthquake-events.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw earthquake events data info:\")\n",
    "print(f\"Shape: {raw_df_ee.shape}\")\n",
    "print(f\"Columns: {list(raw_df_ee.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_ee)}\")\n",
    "\n",
    "# preview key data ranges if available\n",
    "if 'eqMagnitude' in raw_df_ee.columns:\n",
    "    print(f\"Magnitude range: {raw_df_ee['eqMagnitude'].min():.1f} - {raw_df_ee['eqMagnitude'].max():.1f}\")\n",
    "if 'distance' in raw_df_ee.columns:\n",
    "    print(f\"Distance range: {raw_df_ee['distance'].min():.0f} - {raw_df_ee['distance'].max():.0f} km\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_ee.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_ee function in clean.py\n",
    "try:\n",
    "    cleaned_df_ee = clean_ee('data/raw/earthquake-events.csv')\n",
    "    print(\"‚úÖ Earthquake events data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_ee.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_ee.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_ee.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_ee.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_ee['begin_year'].min()} - {cleaned_df_ee['begin_year'].max()}\")\n",
    "    print(f\"- Magnitude range: {cleaned_df_ee['eqMagnitude'].min():.1f} - {cleaned_df_ee['eqMagnitude'].max():.1f}\")\n",
    "    print(f\"- Distance range: {cleaned_df_ee['distance'].min():.0f} - {cleaned_df_ee['distance'].max():.0f} km\")\n",
    "    \n",
    "    # earthquake data analysis\n",
    "    print(f\"\\nEarthquake Data Summary:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    avg_magnitude = cleaned_df_ee['eqMagnitude'].mean()\n",
    "    avg_distance = cleaned_df_ee['distance'].mean()\n",
    "    \n",
    "    print(f\"- Total events: {len(cleaned_df_ee)}\")\n",
    "    print(f\"- Average magnitude: {avg_magnitude:.1f}\")\n",
    "    print(f\"- Average distance: {avg_distance:.0f} km\")\n",
    "    print(f\"- Standard deviation (magnitude): {cleaned_df_ee['eqMagnitude'].std():.1f}\")\n",
    "    print(f\"- Standard deviation (distance): {cleaned_df_ee['distance'].std():.0f} km\")\n",
    "    \n",
    "    # temporal distribution\n",
    "    years_span = cleaned_df_ee['begin_year'].max() - cleaned_df_ee['begin_year'].min()\n",
    "    events_per_decade = (len(cleaned_df_ee) / years_span) * 10 if years_span > 0 else 0\n",
    "    \n",
    "    print(f\"- Time span: {years_span} years\")\n",
    "    print(f\"- Average events per decade: {events_per_decade:.1f}\")\n",
    "    \n",
    "    # identify extremes\n",
    "    strongest_eq = cleaned_df_ee.loc[cleaned_df_ee['eqMagnitude'].idxmax()]\n",
    "    closest_eq = cleaned_df_ee.loc[cleaned_df_ee['distance'].idxmin()]\n",
    "    \n",
    "    print(f\"- Strongest earthquake: {strongest_eq['eqMagnitude']:.1f} magnitude in {strongest_eq['begin_year']}\")\n",
    "    print(f\"- Closest earthquake: {closest_eq['distance']:.0f} km in {closest_eq['begin_year']}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_magnitude = cleaned_df_ee['eqMagnitude'].isna().sum()\n",
    "    missing_distance = cleaned_df_ee['distance'].isna().sum()\n",
    "    missing_year = cleaned_df_ee['begin_year'].isna().sum()\n",
    "    \n",
    "    if missing_magnitude > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing magnitude values: {missing_magnitude}\")\n",
    "        quality_issues += 1\n",
    "    if missing_distance > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing distance values: {missing_distance}\")\n",
    "        quality_issues += 1\n",
    "    if missing_year > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing year values: {missing_year}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_magnitude = (cleaned_df_ee['eqMagnitude'] < 0).sum()\n",
    "    negative_distance = (cleaned_df_ee['distance'] < 0).sum()\n",
    "    \n",
    "    if negative_magnitude > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative magnitude values: {negative_magnitude}\")\n",
    "        quality_issues += 1\n",
    "    if negative_distance > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative distance values: {negative_distance}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate events (same year, magnitude, and distance)\n",
    "    duplicates = cleaned_df_ee.duplicated(subset=['begin_year', 'eqMagnitude', 'distance']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Potential duplicate events: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"‚úÖ No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning earthquake events data: {e}\")\n",
    "    print(\"Check that the earthquake-events.csv file exists and has the correct format\")\n",
    "    print(\"Expected columns: BEGAN, eqMagnitude, distance, text, line1, line2, line3\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_ee' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/ee.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_ee.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_ee)} events\")\n",
    "    print(f\"- Year range: {cleaned_df_ee['begin_year'].min()} - {cleaned_df_ee['begin_year'].max()}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_ee.dtypes)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned earthquake data available\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure earthquake-events.csv exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: BEGAN, eqMagnitude, distance, text, line1, line2, line3\")\n",
    "    print(\"   3. Verify data contains parseable dates in BEGAN column\")\n",
    "    print(\"   4. Check that magnitude and distance values are numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELEVATION (need alternative to donut chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# elevation data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "\n",
    "elevation = [\n",
    "      { \"bin\": \"-5-40m\", \"count\": 413599, \"total\": 549697, \"percentage\": 75.24},\n",
    "      { \"bin\": \"40-90m\", \"count\": 94379, \"total\": 549697, \"percentage\": 17.17 },\n",
    "      { \"bin\": \"90-135m\", \"count\": 32786 , \"total\": 549697, \"percentage\": 5.96 },\n",
    "      { \"bin\": \"135-185m\", \"count\": 8043, \"total\": 549697, \"percentage\": 1.46 },\n",
    "      { \"bin\": \"135-235\", \"count\": 890, \"total\": 549697, \"percentage\": 0.16 },\n",
    "]\n",
    "\n",
    "# convert elevation list to dataframe, elevation_df\n",
    "elevation_df = pd.DataFrame(elevation)\n",
    "\n",
    "# create output CSV of elevation_df for plotting\n",
    "elevation_output_df = pd.DataFrame({\n",
    "    'bin': elevation_df['bin'],\n",
    "    'count': elevation_df['count'],\n",
    "    'total': elevation_df['total'], \n",
    "    'percentage': elevation_df['percentage']\n",
    "})\n",
    "\n",
    "# save elevation_output_df for elevation data to CSV\n",
    "elevation_output_df.to_csv('data/processed/elevation.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLOPE (need alternative to donut chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# slope data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "\n",
    "slope = [\n",
    "      { \"bin\": \"0-2\", \"count\": 428343, \"total\": 549702, \"percentage\": 77.92 },\n",
    "      { \"bin\": \"2-5\", \"count\": 79034, \"total\": 549702, \"percentage\": 14.38 },\n",
    "      { \"bin\": \"5-10\", \"count\": 31121, \"total\": 549702, \"percentage\": 5.66 },\n",
    "      { \"bin\": \"10-20\", \"count\": 10147, \"total\": 549702, \"percentage\": 1.85 },\n",
    "      { \"bin\": \"20+\", \"count\": 1057, \"total\": 549702, \"percentage\": 0.19 },\n",
    "]\n",
    "\n",
    "# convert slope list to dataframe, slope_df\n",
    "slope_df = pd.DataFrame(slope)\n",
    "\n",
    "# create output CSV of slope_df for plotting\n",
    "slope_output_df = pd.DataFrame({\n",
    "    'bin': slope_df['bin'],\n",
    "    'count': slope_df['count'],\n",
    "    'total': slope_df['total'], \n",
    "    'percentage': slope_df['percentage']\n",
    "})\n",
    "\n",
    "# save slope_output_df for slope data to CSV\n",
    "slope_output_df.to_csv('data/processed/slope.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EARTHQUAKE EVENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate earthquake event data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "# manually added earthquake event data for Tunis, Tunisia\n",
    "ee = [\n",
    "    {\"begin_year\": 1903, \"distance\": 316, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"MAY 1903; MNA; 316 km away; NA damage\", \"line1\": \"MAY 1903\", \"line2\": \"M5.3, 316 km away\", \"line3\": \"Palermo\"},\n",
    "    {\"begin_year\": 1906, \"distance\": 335, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"SEPTEMBER 1906; MNA; 336 km away; NA damage\", \"line1\": \"SEPTEMBER 1906\", \"line2\": \"M5.6, 335 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1907, \"distance\": 335, \"eqMagnitude\": 5.1, \"severity\": \"Moderate\", \"text\": \"FEBRUARY 1907; MNA; 336 km away; NA damage\", \"line1\": \"FEBRUARY 1907\", \"line2\": \"M5.1, 335 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1908, \"distance\": 497, \"eqMagnitude\": 7.0, \"severity\": \"Very Large\", \"text\": \"DECEMBER 1908; M7; 498 km away; Extreme damage; 78,000 fatalities\", \"line1\": \"DECEMBER 1908\", \"line2\": \"M7.0, 498 km away, 78,000 fatalities\", \"line3\": \"Messina, Sicily, Calabria\"},\n",
    "    {\"begin_year\": 1909, \"distance\": 452, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"OCTOBER 1909; MNA; 453 km away; NA damage\", \"line1\": \"OCTOBER 1909\", \"line2\": \"M5.3, 453 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1911, \"distance\": 652, \"eqMagnitude\": 4.3, \"severity\": \"Small\", \"text\": \"OCTOBER 1911; M4.3; 453 km away; NA damage\", \"line1\": \"OCTOBER 1911\", \"line2\": \"M4.3, 453 km away\", \"line3\": \"Etna\"},\n",
    "    {\"begin_year\": 1914, \"distance\": 452, \"eqMagnitude\": 4.9, \"severity\": \"Small\", \"text\": \"MAY 1914; M4.9; 453 km away; Severe damage; 120 fatalities\", \"line1\": \"MAY 1914\", \"line2\": \"M4.9, 453 km away, 120 fatalities\", \"line3\": \"Catania, Etna\"},\n",
    "    {\"begin_year\": 1916, \"distance\": 494, \"eqMagnitude\": 5.1, \"severity\": \"Moderate\", \"text\": \"JULY 1916; M5.1; 494 km away; Limited damage\", \"line1\": \"JULY 1916\", \"line2\": \"M5.1, 494 km away\", \"line3\": \"Stromboli Island\"},\n",
    "    {\"begin_year\": 1924, \"distance\": 489, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"MARCH 1924; M5.6; 489 km away; NA damage\", \"line1\": \"MARCH 1924\", \"line2\": \"M5.6, 489 km away\", \"line3\": \"Batna\"},\n",
    "    {\"begin_year\": 1926, \"distance\": 451, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"AUGUST 1926; M5.3; 451 km away; Severe damage\", \"line1\": \"AUGUST 1926\", \"line2\": \"M5.3, 451 km away\", \"line3\": \"Salina Island\"},\n",
    "    {\"begin_year\": 1930, \"distance\": 434, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"MARCH 1930; MNA; 434 km away; Moderate damage\", \"line1\": \"MARCH 1930\", \"line2\": \"M5.0, 434 km away\", \"line3\": \"Filicudi Island\"},\n",
    "    {\"begin_year\": 1931, \"distance\": 444, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"JULY 1931; MNA; 444 km away; NA damage\", \"line1\": \"JULY 1931\", \"line2\": \"M5.0, 444 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1933, \"distance\": 256, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"FEBRUARY 1933; MNA; 256 km away; NA damage\", \"line1\": \"FEBRUARY 1933\", \"line2\": \"M5.0, 256 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1939, \"distance\": 437, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"JANUARY 1939; MNA; 438 km away; NA damage\", \"line1\": \"JANUARY 1939\", \"line2\": \"M5.0, 438 km away\", \"line3\": \"Calabria\"},\n",
    "    {\"begin_year\": 1940, \"distance\": 323, \"eqMagnitude\": 4.8, \"severity\": \"Small\", \"text\": \"JANUARY 1940; M4.8; 324 km away; NA damage\", \"line1\": \"JANUARY 1940\", \"line2\": \"M4.8, 324 km away\", \"line3\": \"NA\"},\n",
    "    {\"begin_year\": 1941, \"distance\": 240, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"MARCH 1941; MNA; 241 km away; NA damage\", \"line1\": \"MARCH 1941\", \"line2\": \"M5.0, 241 km away\", \"line3\": \"Calabria\"},\n",
    "    {\"begin_year\": 1946, \"distance\": 481, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"FEBRUARY 1946; M5.6; 481 km away; Severe damage; 264 fatalities\", \"line1\": \"FEBRUARY 1946\", \"line2\": \"M5.6, 481 km away, 264 fatalities\", \"line3\": \"Hodna Mountains\"},\n",
    "    {\"begin_year\": 1947, \"distance\": 196, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"AUGUST 1947; M5.3; 197 km away; Moderate damage; 3 fatalities\", \"line1\": \"AUGUST 1947\", \"line2\": \"M5.3, 197 km away, 3 fatalities\", \"line3\": \"NA\"},\n",
    "    {\"begin_year\": 1957, \"distance\": 135, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"FEBRUARY 1957; M5.6; 135 km away; Moderate damage; 13 fatalities\", \"line1\": \"FEBRUARY 1957\", \"line2\": \"M5.6, 135 km away, 13 fatalities\", \"line3\": \"Sidi Abd,Sidi Toul\"},\n",
    "    {\"begin_year\": 1961, \"distance\": 468, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"MARCH 1961; MNA; 469 km away; Moderate damage; 15 fatalities\", \"line1\": \"MARCH 1961\", \"line2\": \"M5.0, 469 km away, 15 fatalities\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1962, \"distance\": 141, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"FEBRUARY 1962; M5.3; 141 km away; Moderate damage\", \"line1\": \"FEBRUARY 1962\", \"line2\": \"M5.3, 141 km away\", \"line3\": \"Gafour,OUM-Zid,EL Akhou–∞—Ç\"},\n",
    "    {\"begin_year\": 1968, \"distance\": 282, \"eqMagnitude\": 6.0, \"severity\": \"Very Large\", \"text\": \"JANUARY 1968; M6; 283 km away; Extreme damage; 216 fatalities\", \"line1\": \"JANUARY 1968\", \"line2\": \"M6.0, 283 km away, 216 fatalities\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1968, \"distance\": 466, \"eqMagnitude\": 4.9, \"severity\": \"Small\", \"text\": \"FEBRUARY 1968; M4.9; 466 km away; Moderate damage; 1 fatality\", \"line1\": \"FEBRUARY 1968\", \"line2\": \"M4.9, 466 km away, 1 fatality\", \"line3\": \"El Asn (BABORD)\"},\n",
    "    {\"begin_year\": 1975, \"distance\": 446, \"eqMagnitude\": 4.3, \"severity\": \"Small\", \"text\": \"JULY 1975; M4.3; 446 km away; Moderate damage; 1 fatality\", \"line1\": \"JULY 1975\", \"line2\": \"M4.3, 446 km away, 1 fatality\", \"line3\": \"Djebel Babor\"},\n",
    "    {\"begin_year\": 1978, \"distance\": 462, \"eqMagnitude\": 5.7, \"severity\": \"Large\", \"text\": \"APRIL 1978; M5.7; 463 km away; Moderate damage; 5 fatalities\", \"line1\": \"APRIL 1978\", \"line2\": \"M5.7, 463 km away, 5 fatalities\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1990, \"distance\": 467, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"DECEMBER 1990; M5.3; 468 km away; Extreme damage; 19 fatalities\", \"line1\": \"DECEMBER 1990\", \"line2\": \"M5.3, 468 km away, 19 fatalities\", \"line3\": \"Sicily: Carlentini\"},\n",
    "    {\"begin_year\": 2002, \"distance\": 553, \"eqMagnitude\": 6.0, \"severity\": \"Very Large\", \"text\": \"SEPTEMBER 2002; M6; 354 km away; Extreme damage; 2 fatalities\", \"line1\": \"SEPTEMBER 2002\", \"line2\": \"M6.0, 354 km away, 2 fatalities\", \"line3\": \"Sicily: Palermo\"},\n",
    "    {\"begin_year\": 2018, \"distance\": 442, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"DECEMBER 2018; M5; 443 km away; Extreme damage\", \"line1\": \"DECEMBER 2018\", \"line2\": \"M5.0, 443 km away\", \"line3\": \"Sicily: Catpana\"},\n",
    "    {\"begin_year\": 2021, \"distance\": 444, \"eqMagnitude\": 6.0, \"severity\": \"Very Large\", \"text\": \"MARCH 2021; M6; 445 km away; Limited damage\", \"line1\": \"MARCH 2021\", \"line2\": \"M6.0, 445 km away\", \"line3\": \"NA\"}\n",
    "]\n",
    "\n",
    "# convert ee list to dataframe, ee_df\n",
    "ee_df = pd.DataFrame(ee)\n",
    "\n",
    "\n",
    "# save ee_output_df for ee data to CSV\n",
    "ee_df.to_csv('data/processed/ee.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   begin_year  distance  eqMagnitude    severity           line1  \\\n",
      "0        1903       316          5.3    Moderate        MAY 1903   \n",
      "1        1906       335          5.6       Large  SEPTEMBER 1906   \n",
      "2        1907       335          5.1    Moderate   FEBRUARY 1907   \n",
      "3        1908       497          7.0  Very Large   DECEMBER 1908   \n",
      "4        1909       452          5.3    Moderate    OCTOBER 1909   \n",
      "5        1911       652          4.3       Small    OCTOBER 1911   \n",
      "6        1914       452          4.9       Small        MAY 1914   \n",
      "7        1916       494          5.1    Moderate       JULY 1916   \n",
      "8        1924       489          5.6       Large      MARCH 1924   \n",
      "9        1926       451          5.3    Moderate     AUGUST 1926   \n",
      "\n",
      "                                  line2                      line3  \n",
      "0                     M5.3, 316 km away                    Palermo  \n",
      "1                     M5.6, 335 km away                     Sicily  \n",
      "2                     M5.1, 335 km away                     Sicily  \n",
      "3  M7.0, 498 km away, 78,000 fatalities  Messina, Sicily, Calabria  \n",
      "4                     M5.3, 453 km away                     Sicily  \n",
      "5                     M4.3, 453 km away                       Etna  \n",
      "6     M4.9, 453 km away, 120 fatalities              Catania, Etna  \n",
      "7                     M5.1, 494 km away           Stromboli Island  \n",
      "8                     M5.6, 489 km away                      Batna  \n",
      "9                     M5.3, 451 km away              Salina Island  \n"
     ]
    }
   ],
   "source": [
    "# ee data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(ee_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HISTORICAL BURNT AREA & FIRE WEATHER INDEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate historical burnt area & fire weather index data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "fwi = [\n",
    "      { \"week\": 1, \"monthName\": \"Jan\", \"fwi\": 36.03855628967285},\n",
    "      { \"week\": 2, \"monthName\": \"Jan\", \"fwi\": 28.35186767578125},\n",
    "      { \"week\": 3, \"monthName\": \"Jan\", \"fwi\": 34.48613758087156},\n",
    "      { \"week\": 4, \"monthName\": \"Jan\", \"fwi\": 35.160119628906244},\n",
    "      { \"week\": 5, \"monthName\": \"Feb\", \"fwi\": 41.2155460357666},\n",
    "      { \"week\": 6, \"monthName\": \"Feb\", \"fwi\": 42.91906299591064},\n",
    "      { \"week\": 7, \"monthName\": \"Feb\", \"fwi\": 40.35708732604981},\n",
    "      { \"week\": 8, \"monthName\": \"Feb\", \"fwi\": 35.13482322692871},\n",
    "      { \"week\": 9, \"monthName\": \"Feb\", \"fwi\": 43.7328405380249},\n",
    "      { \"week\": 10, \"monthName\": \"Mar\", \"fwi\": 55.629536437988264},\n",
    "      { \"week\": 11, \"monthName\": \"Mar\", \"fwi\": 51.963145637512206},\n",
    "      { \"week\": 12, \"monthName\": \"Mar\", \"fwi\": 48.64410858154295},\n",
    "      { \"week\": 13, \"monthName\": \"Mar\", \"fwi\": 48.45940856933592},\n",
    "      { \"week\": 14, \"monthName\": \"Apr\", \"fwi\": 42.525428390502924},\n",
    "      { \"week\": 15, \"monthName\": \"Apr\", \"fwi\": 48.989934921264634},\n",
    "      { \"week\": 16, \"monthName\": \"Apr\", \"fwi\": 47.94815864562989},\n",
    "      { \"week\": 17, \"monthName\": \"Apr\", \"fwi\": 59.693392562866215},\n",
    "      { \"week\": 18, \"monthName\": \"May\", \"fwi\": 53.26485347747803},\n",
    "      { \"week\": 19, \"monthName\": \"May\", \"fwi\": 67.04015121459962},\n",
    "      { \"week\": 20, \"monthName\": \"May\", \"fwi\": 66.2925880432129},\n",
    "      { \"week\": 21, \"monthName\": \"May\", \"fwi\": 63.51103172302246},\n",
    "      { \"week\": 22, \"monthName\": \"May\", \"fwi\": 57.59551124572754},\n",
    "      { \"week\": 23, \"monthName\": \"Jun\", \"fwi\": 66.97727813720704},\n",
    "      { \"week\": 24, \"monthName\": \"Jun\", \"fwi\": 75.7531303405762},\n",
    "      { \"week\": 25, \"monthName\": \"Jun\", \"fwi\": 80.30134506225586},\n",
    "      { \"week\": 26, \"monthName\": \"Jun\", \"fwi\": 90.69736862182619},\n",
    "      { \"week\": 27, \"monthName\": \"Jul\", \"fwi\": 75.26012268066407},\n",
    "      { \"week\": 28, \"monthName\": \"Jul\", \"fwi\": 95.59054870605469},\n",
    "      { \"week\": 29, \"monthName\": \"Jul\", \"fwi\": 82.06852722167967},\n",
    "      { \"week\": 30, \"monthName\": \"Jul\", \"fwi\": 81.8968620300293},\n",
    "      { \"week\": 31, \"monthName\": \"Aug\", \"fwi\": 81.7047821044922},\n",
    "      { \"week\": 32, \"monthName\": \"Aug\", \"fwi\": 81.58447265625001}, \n",
    "      { \"week\": 33, \"monthName\": \"Aug\", \"fwi\": 65.29224243164063},  \n",
    "      { \"week\": 34, \"monthName\": \"Aug\", \"fwi\": 67.29769515991212},  \n",
    "      { \"week\": 35, \"monthName\": \"Aug\", \"fwi\": 64.21281738281252},  \n",
    "      { \"week\": 36, \"monthName\": \"Sep\", \"fwi\": 69.20558013916019},  \n",
    "      { \"week\": 37, \"monthName\": \"Sep\", \"fwi\": 59.376176834106474},  \n",
    "      { \"week\": 38, \"monthName\": \"Sep\", \"fwi\": 50.01441955566406},  \n",
    "      { \"week\": 39, \"monthName\": \"Sep\", \"fwi\": 40.38814010620118 },  \n",
    "      { \"week\": 40, \"monthName\": \"Oct\", \"fwi\": 48.369334793090815},  \n",
    "      { \"week\": 41, \"monthName\": \"Oct\", \"fwi\": 43.82190437316895},  \n",
    "      { \"week\": 42, \"monthName\": \"Oct\", \"fwi\": 37.03949813842773},  \n",
    "      { \"week\": 43, \"monthName\": \"Oct\", \"fwi\": 50.04811096191406},  \n",
    "      { \"week\": 44, \"monthName\": \"Nov\", \"fwi\": 47.38101158142093},  \n",
    "      { \"week\": 45, \"monthName\": \"Nov\", \"fwi\": 37.50416679382325},  \n",
    "      { \"week\": 46, \"monthName\": \"Nov\", \"fwi\": 29.76080322265625},  \n",
    "      { \"week\": 47, \"monthName\": \"Nov\", \"fwi\": 36.063685607910124},  \n",
    "      { \"week\": 48, \"monthName\": \"Nov\", \"fwi\": 34.42437210083008},  \n",
    "      { \"week\": 49, \"monthName\": \"Dec\", \"fwi\": 32.008924865722626},  \n",
    "      { \"week\": 50, \"monthName\": \"Dec\", \"fwi\": 33.579549407958986},  \n",
    "      { \"week\": 51, \"monthName\": \"Dec\", \"fwi\": 31.927024841308594},  \n",
    "      { \"week\": 52, \"monthName\": \"Dec\", \"fwi\": 34.5278169631958},  \n",
    "      { \"week\": 53, \"monthName\": \"Dec\", \"fwi\": 31.089004516601562}  \n",
    "\n",
    "]\n",
    "\n",
    "# convert fwi list to dataframe, fwi_df\n",
    "fwi_df = pd.DataFrame(fwi)\n",
    "\n",
    "# create output CSV of fwi_df for plotting\n",
    "fwi_output_df = pd.DataFrame({\n",
    "    'week': fwi_df['week'],\n",
    "    'monthName': fwi_df['monthName'],\n",
    "    'fwi': fwi_df['fwi'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save fwi_output_df for fwi data to CSV\n",
    "fwi_output_df.to_csv('data/processed/fwi.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   week monthName    fwi\n",
      "0     1       Jan  36.04\n",
      "1     2       Jan  28.35\n",
      "2     3       Jan  34.49\n",
      "3     4       Jan  35.16\n",
      "4     5       Feb  41.22\n",
      "5     6       Feb  42.92\n",
      "6     7       Feb  40.36\n",
      "7     8       Feb  35.13\n",
      "8     9       Feb  43.73\n",
      "9    10       Mar  55.63\n",
      "\n",
      "Total number of records: 53\n",
      "Month names: ['Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep' 'Oct' 'Nov' 'Dec']\n",
      "fwi values: [36.04 28.35 34.49 35.16 41.22 42.92 40.36 35.13 43.73 55.63 51.96 48.64\n",
      " 48.46 42.53 48.99 47.95 59.69 53.26 67.04 66.29 63.51 57.6  66.98 75.75\n",
      " 80.3  90.7  75.26 95.59 82.07 81.9  81.7  81.58 65.29 67.3  64.21 69.21\n",
      " 59.38 50.01 40.39 48.37 43.82 37.04 50.05 47.38 37.5  29.76 36.06 34.42\n",
      " 32.01 33.58 31.93 34.53 31.09]\n"
     ]
    }
   ],
   "source": [
    "# fwi data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(fwi_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(fwi_output_df)}\")\n",
    "print(f\"Month names: {fwi_output_df['monthName'].unique()}\")\n",
    "print(f\"fwi values: {fwi_output_df['fwi'].unique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Scan Data Cleaning\n",
    "##### June 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data cleaning pipeline for appropriate CSV preparation necessary for City Scan JavaScript plots with Cartagena, Colombia as the case study example city for pipeline scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory changes\n",
      "current working directory is: /Users/carolinecullinan/dev/wb/city-scan-csv-viz-prep\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# change to project root directory\n",
    "os.chdir('../')\n",
    "print(\"directory changes\")\n",
    "print(f\"current working directory is:\", os.getcwd())\n",
    "\n",
    "# local imports (after changing directory)\n",
    "from src.clean import clean_pg, clean_pas, clean_uba, clean_lc, clean_pug, clean_pv, clean_flood, clean_ee, clean_fwi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POPULATION AND DEMOGRAPHIC TRENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pg.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pga\" / \"chart_pga\" ; and\n",
    "#### 2.) \"plot_pgp\" / \"chart_pg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population growth data info:\n",
      "Shape: (22, 7)\n",
      "Columns: ['Group', 'Location', 'Country', 'Year', 'Population', 'Source', 'Method']\n",
      "Date range: 2000 - 2021\n",
      "Data preview:\n",
      "       Group   Location   Country  Year  Population  Source  Method\n",
      "0  Cartagena  Cartagena  Colombia  2000    999576.9  Oxford  Oxford\n",
      "1  Cartagena  Cartagena  Colombia  2001   1012694.0  Oxford  Oxford\n",
      "2  Cartagena  Cartagena  Colombia  2002   1025077.0  Oxford  Oxford\n",
      "3  Cartagena  Cartagena  Colombia  2003   1036816.0  Oxford  Oxford\n",
      "4  Cartagena  Cartagena  Colombia  2004   1048314.0  Oxford  Oxford\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/pg.csv\n",
      "Years covered: 2000 - 2021\n",
      "Total data points: 22\n",
      "Population range: 999,576.9 - 1,259,382.0\n",
      "‚úÖ Population growth data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (22, 3)\n",
      "Cleaned data columns: ['yearName', 'population', 'populationGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   yearName  population  populationGrowthPercentage\n",
      "0      2000    999576.9                         NaN\n",
      "1      2001   1012694.0                       1.312\n",
      "2      2002   1025077.0                       1.223\n",
      "3      2003   1036816.0                       1.145\n",
      "4      2004   1048314.0                       1.109\n",
      "5      2005   1059620.0                       1.078\n",
      "6      2006   1074557.0                       1.410\n",
      "7      2007   1088791.0                       1.325\n",
      "8      2008   1102244.0                       1.236\n",
      "9      2009   1114637.0                       1.124\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 2000 - 2021\n",
      "- Population range: 999,576.9 - 1,259,382.0\n",
      "- Growth rate range: 0.419% - 1.410%\n",
      "‚ö†Ô∏è  Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pg.csv\n",
      "‚úÖ Ready for Observable visualization!\n"
     ]
    }
   ],
   "source": [
    "# POPULATION & DEMOGRAPHIC TRENDS - pg.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pga\"/\"chart_pga\" (absolute population growth); and \n",
    "# 2.) \"plot_pgp\"/\"chart_pgp\" (population growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pg = pd.read_csv('data/raw/population-growth.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw population growth data info:\")\n",
    "print(f\"Shape: {raw_df_pg.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pg.columns)}\")\n",
    "print(f\"Date range: {raw_df_pg['Year'].min()} - {raw_df_pg['Year'].max()}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pg.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_pg function in clean.py\n",
    "try:\n",
    "    cleaned_df_pg = clean_pg('data/raw/population-growth.csv')\n",
    "    print(\"‚úÖ Population growth data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pg.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pg.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pg.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pg.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pg['yearName'].min()} - {cleaned_df_pg['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pg['population'].min():,} - {cleaned_df_pg['population'].max():,}\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_pg['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pg['populationGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_pg['populationGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {cleaned_df_pg['populationGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning population growth data: {e}\")\n",
    "    print(\"Check that 'data/raw/population-growth.csv' exists and has the correct format\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pg.csv, and export\n",
    "# (this is handled automatically by the clean_pg function, but confirming)\n",
    "if 'cleaned_df_pg' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pg.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pas.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pas\" / \"chart_pas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population age structure data info:\n",
      "Shape: (36, 3)\n",
      "Columns: ['age_group', 'sex', 'population']\n",
      "Age groups: ['0-1', '1-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "Sex categories: ['f' 'm']\n",
      "Total population: 1,319,285\n",
      "Data preview:\n",
      "  age_group sex    population\n",
      "0       1-4   f  42167.329278\n",
      "1       1-4   m  45029.741031\n",
      "2       0-1   f  10386.217327\n",
      "3       0-1   m  11119.716253\n",
      "4       5-9   f  50745.108513\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  Warning: Could not sort by age bracket (\"['age_sort'] not found in axis\"). Using default sorting.\n",
      "Cleaned data saved to: data/processed/pas.csv\n",
      "Total population: 1,319,285\n",
      "Age brackets: 17\n",
      "Sex categories: 2\n",
      "Total records: 34\n",
      "‚úÖ Population age structure data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (34, 5)\n",
      "Cleaned data columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "Sample of cleaned data:\n",
      "  ageBracket     sex     count  percentage  yearName\n",
      "0        0-4  female  52553.55    3.983486      2021\n",
      "1        0-4    male  56149.46    4.256051      2021\n",
      "2      10-14  female  53631.29    4.065178      2021\n",
      "3      10-14    male  56959.22    4.317430      2021\n",
      "4      15-19  female  55799.87    4.229553      2021\n",
      "5      15-19    male  58415.47    4.427812      2021\n",
      "6      20-24  female  54857.68    4.158137      2021\n",
      "7      20-24    male  54082.97    4.099414      2021\n",
      "8      25-29  female  55018.60    4.170334      2021\n",
      "9      25-29    male  52944.24    4.013101      2021\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Age brackets: ['0-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "- Sex categories: ['female', 'male']\n",
      "- Population count range: 7,923 - 58,415\n",
      "- Percentage range: 0.601% - 4.428%\n",
      "- Year: 2021\n",
      "- Total percentage sum: 100.000% (should be ~100%)\n",
      "- Population by sex: Female: 663,596, Male: 655,689\n",
      "- Age brackets: 17, Total records: 34\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pas.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "- Records per sex: 17, 17\n",
      "- Data types: {'ageBracket': dtype('O'), 'sex': dtype('O'), 'count': dtype('float64'), 'percentage': dtype('float64'), 'yearName': dtype('int64')}\n"
     ]
    }
   ],
   "source": [
    "# POPULATION AGE SEX - pas.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pas\"/\"chart_pas\" (population age sex, i.e., population by sex and age bracket, (i.e., Population Distribution by Age & Sex, xxxx))\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pas = pd.read_csv('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_demographics.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw population age structure data info:\")\n",
    "print(f\"Shape: {raw_df_pas.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pas.columns)}\")\n",
    "print(f\"Age groups: {sorted(raw_df_pas['age_group'].unique())}\")\n",
    "print(f\"Sex categories: {raw_df_pas['sex'].unique()}\")\n",
    "print(f\"Total population: {raw_df_pas['population'].sum():,.0f}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pas.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_pas function in clean.py\n",
    "try:\n",
    "    cleaned_df_pas = clean_pas('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_demographics.csv')\n",
    "    print(\"‚úÖ Population age structure data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pas.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pas.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pas.isnull().sum().sum()}\")\n",
    "    print(f\"- Age brackets: {sorted(cleaned_df_pas['ageBracket'].unique())}\")\n",
    "    print(f\"- Sex categories: {sorted(cleaned_df_pas['sex'].unique())}\")\n",
    "    print(f\"- Population count range: {cleaned_df_pas['count'].min():,.0f} - {cleaned_df_pas['count'].max():,.0f}\")\n",
    "    print(f\"- Percentage range: {cleaned_df_pas['percentage'].min():.3f}% - {cleaned_df_pas['percentage'].max():.3f}%\")\n",
    "    print(f\"- Year: {cleaned_df_pas['yearName'].iloc[0]}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    total_percentage = cleaned_df_pas['percentage'].sum()\n",
    "    print(f\"- Total percentage sum: {total_percentage:.3f}% (should be ~100%)\")\n",
    "    \n",
    "    if abs(total_percentage - 100) > 0.1:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Percentage sum deviates from 100% by {abs(total_percentage - 100):.3f}%\")\n",
    "    \n",
    "    # check for balanced sex representation\n",
    "    sex_counts = cleaned_df_pas.groupby('sex')['count'].sum()\n",
    "    print(f\"- Population by sex: Female: {sex_counts.get('female', 0):,.0f}, Male: {sex_counts.get('male', 0):,.0f}\")\n",
    "    \n",
    "    # check age bracket coverage\n",
    "    expected_brackets = len(cleaned_df_pas['ageBracket'].unique())\n",
    "    actual_records = len(cleaned_df_pas)\n",
    "    print(f\"- Age brackets: {expected_brackets}, Total records: {actual_records}\")\n",
    "    \n",
    "    if actual_records != expected_brackets * 2:  # Should be 2 records per age bracket (male/female)\n",
    "        print(f\"‚ö†Ô∏è  Note: Expected {expected_brackets * 2} records (2 per age bracket), found {actual_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning population age structure data: {e}\")\n",
    "    print(\"Check that the demographics CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: age_group, sex, population\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pas.csv, and export\n",
    "# (this is handled automatically by the clean_pas function, but confirming)\n",
    "if 'cleaned_df_pas' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pas.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"- Records per sex: {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'female'])}, {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'male'])}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pas.dtypes)}\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILT FORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URBAN EXTENT AND CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uba.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ubaa\" / \"chart_ubaa\" ; and\n",
    "#### 2.) \"plot_ubap\" / \"chart_ubap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw urban built area data info:\n",
      "Shape: (31, 2)\n",
      "Columns: ['year', 'cumulative sq km']\n",
      "Year range: 1985 - 2015\n",
      "UBA range: 98.56 - 184.92 sq km\n",
      "Total data points: 31\n",
      "Data preview:\n",
      "   year  cumulative sq km\n",
      "0  1985         98.562692\n",
      "1  1986        100.892675\n",
      "2  1987        103.206772\n",
      "3  1988        104.745090\n",
      "4  1989        106.680565\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/uba.csv\n",
      "Years covered: 1985 - 2015\n",
      "Total data points: 31\n",
      "UBA range: 98.56 - 184.92 sq km\n",
      "‚úÖ Urban built area data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (31, 4)\n",
      "Cleaned data columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   year  yearName     uba  ubaGrowthPercentage\n",
      "0     1      1985   98.56                  NaN\n",
      "1     2      1986  100.89                2.364\n",
      "2     3      1987  103.21                2.300\n",
      "3     4      1988  104.75                1.492\n",
      "4     5      1989  106.68                1.842\n",
      "5     6      1990  109.99                3.103\n",
      "6     7      1991  115.45                4.964\n",
      "7     8      1992  117.07                1.403\n",
      "8     9      1993  119.32                1.922\n",
      "9    10      1994  120.69                1.148\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 1985 - 2015\n",
      "- UBA range: 98.56 - 184.92 sq km\n",
      "- Growth rate range: 0.655% - 5.367%\n",
      "- Total urban expansion: 86.36 sq km over 30 years\n",
      "\n",
      "Urban growth analysis:\n",
      "- Average annual UBA growth rate: 2.125%\n",
      "‚ö†Ô∏è  Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/uba.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "- Time series length: 31 years\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'uba': dtype('float64'), 'ubaGrowthPercentage': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "# URBAN EXTENT AND CHANGE - uba.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ubaa\"/\"chart_ubaa\" (absolute urban extent and change)\n",
    "# 2.) \"plot_ubap\"/\"chart_ubap\" (urban extent and change growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_uba = pd.read_csv('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_wsf_stats.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw urban built area data info:\")\n",
    "print(f\"Shape: {raw_df_uba.shape}\")\n",
    "print(f\"Columns: {list(raw_df_uba.columns)}\")\n",
    "print(f\"Year range: {raw_df_uba['year'].min()} - {raw_df_uba['year'].max()}\")\n",
    "print(f\"UBA range: {raw_df_uba['cumulative sq km'].min():.2f} - {raw_df_uba['cumulative sq km'].max():.2f} sq km\")\n",
    "print(f\"Total data points: {len(raw_df_uba)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_uba.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_uba function in clean.py\n",
    "try:\n",
    "    cleaned_df_uba = clean_uba('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_wsf_stats.csv')\n",
    "    print(\"‚úÖ Urban built area data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_uba.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_uba.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_uba.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_uba['yearName'].min()} - {cleaned_df_uba['yearName'].max()}\")\n",
    "    print(f\"- UBA range: {cleaned_df_uba['uba'].min():.2f} - {cleaned_df_uba['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_uba['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_uba['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- Total urban expansion: {cleaned_df_uba['uba'].max() - cleaned_df_uba['uba'].min():.2f} sq km over {cleaned_df_uba['yearName'].max() - cleaned_df_uba['yearName'].min()} years\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nUrban growth analysis:\")\n",
    "    # Calculate average annual growth rate\n",
    "    avg_growth = cleaned_df_uba['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual UBA growth rate: {avg_growth:.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_uba['ubaGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {cleaned_df_uba['ubaGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "    # check for negative growth (urban area should generally increase)\n",
    "    negative_growth = cleaned_df_uba[cleaned_df_uba['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(negative_growth)} years with negative UBA growth detected\")\n",
    "        print(f\"   Years with decline: {negative_growth['yearName'].tolist()}\")\n",
    "      \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning urban built area data: {e}\")\n",
    "    print(\"Check that the UBA CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, cumulative sq km\")\n",
    "\n",
    "# save the cleaned data as a CSV file - uba.csv, and export\n",
    "# (this is handled automatically by the clean_uba function, but confirming)\n",
    "if 'cleaned_df_uba' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/uba.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_uba)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_uba.dtypes)}\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAND COVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_lc\" / \"chart_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw land cover data info:\n",
      "Shape: (13, 5)\n",
      "Columns: ['Land Cover Type', 'Pixel Count', 'Decimal', '%', '% rounded']\n",
      "Total data points: 13\n",
      "Land cover types: 12\n",
      "Total pixels: 14,655,082\n",
      "Pixel count range: 0 - 7,327,541\n",
      "Data preview:\n",
      "  Land Cover Type   Pixel Count   Decimal          %  % rounded\n",
      "0      Tree cover  2.875778e+06  0.392462  39.246150      39.24\n",
      "1       Shrubland  1.200850e+05  0.016388   1.638818       1.64\n",
      "2       Grassland  2.079368e+06  0.283774  28.377435      28.38\n",
      "3        Cropland  7.629053e+04  0.010411   1.041148       1.04\n",
      "4        Built-up  8.045364e+05  0.109796  10.979624      10.98\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/lc.csv\n",
      "Land cover types: 9\n",
      "Total pixels analyzed: 7,327,541\n",
      "Percentage coverage verification: 100.0% (should be ~100%)\n",
      "Dominant land cover: Tree cover (39.2%)\n",
      "‚úÖ Land cover data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (9, 4)\n",
      "Cleaned data columns: ['lcType', 'pixelCount', 'pixelTotal', 'percentage']\n",
      "Sample of cleaned data:\n",
      "                     lcType  pixelCount    pixelTotal  percentage\n",
      "0                Tree cover     2875778  7.327541e+06       39.25\n",
      "1                 Grassland     2079368  7.327541e+06       28.38\n",
      "2                  Built-up      804536  7.327541e+06       10.98\n",
      "3                 Mangroves      502555  7.327541e+06        6.86\n",
      "4    Permanent water bodies      444102  7.327541e+06        6.06\n",
      "5        Herbaceous wetland      335496  7.327541e+06        4.58\n",
      "6                 Shrubland      120085  7.327541e+06        1.64\n",
      "7  Bare / sparse vegetation       89331  7.327541e+06        1.22\n",
      "8                  Cropland       76291  7.327541e+06        1.04\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Land cover types: 9\n",
      "- Total pixels: 7,327,541\n",
      "- Percentage sum: 100.0% (should be ~100%)\n",
      "\n",
      "Land Cover Data Summary:\n",
      "- Land cover types present: 9\n",
      "- Pixel count range: 76,291 - 2,875,778\n",
      "- Most common land cover: Tree cover (39.2%)\n",
      "- Least common land cover: Cropland (1.0%)\n",
      "- Types with ‚â•10% coverage: 3\n",
      "- Types with ‚â•5% coverage: 5\n",
      "- Types with ‚â•1% coverage: 9\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No data quality issues detected\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/lc.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['lcType', 'pixelCount', 'pixelTotal', 'percentage']\n",
      "- Data points: 9 land cover types\n",
      "- Data types: {'lcType': dtype('O'), 'pixelCount': dtype('int64'), 'pixelTotal': dtype('float64'), 'percentage': dtype('float64')}\n",
      "- Coverage range: 1.0% - 39.2%\n"
     ]
    }
   ],
   "source": [
    "# LAND COVER - lc.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_lc\"/\"chart_lc\" (land cover types)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_lc = pd.read_csv('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_lc.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw land cover data info:\")\n",
    "print(f\"Shape: {raw_df_lc.shape}\")\n",
    "print(f\"Columns: {list(raw_df_lc.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_lc)}\")\n",
    "\n",
    "# preview land cover types and pixel counts\n",
    "if 'Land Cover Type' in raw_df_lc.columns and 'Pixel Count' in raw_df_lc.columns:\n",
    "    print(f\"Land cover types: {raw_df_lc['Land Cover Type'].nunique()}\")\n",
    "    print(f\"Total pixels: {raw_df_lc['Pixel Count'].sum():,.0f}\")\n",
    "    print(f\"Pixel count range: {raw_df_lc['Pixel Count'].min():,.0f} - {raw_df_lc['Pixel Count'].max():,.0f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_lc.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_lc function in clean.py\n",
    "try:\n",
    "    cleaned_df_lc = clean_lc('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_lc.csv')\n",
    "    print(\"‚úÖ Land cover data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_lc.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_lc.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_lc.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_lc.isnull().sum().sum()}\")\n",
    "    print(f\"- Land cover types: {len(cleaned_df_lc)}\")\n",
    "    print(f\"- Total pixels: {cleaned_df_lc['pixelTotal'].iloc[0]:,.0f}\")\n",
    "    print(f\"- Percentage sum: {cleaned_df_lc['percentage'].sum():.1f}% (should be ~100%)\")\n",
    "    \n",
    "    # land cover analysis\n",
    "    print(f\"\\nLand Cover Data Summary:\")\n",
    "    \n",
    "    print(f\"- Land cover types present: {len(cleaned_df_lc)}\")\n",
    "    print(f\"- Pixel count range: {cleaned_df_lc['pixelCount'].min():,.0f} - {cleaned_df_lc['pixelCount'].max():,.0f}\")\n",
    "    \n",
    "    # identify extremes\n",
    "    dominant_type = cleaned_df_lc.iloc[0]  # first row after sorting by percentage\n",
    "    least_common_type = cleaned_df_lc.iloc[-1]  # last row after sorting\n",
    "    \n",
    "    print(f\"- Most common land cover: {dominant_type['lcType']} ({dominant_type['percentage']:.1f}%)\")\n",
    "    print(f\"- Least common land cover: {least_common_type['lcType']} ({least_common_type['percentage']:.1f}%)\")\n",
    "    \n",
    "    # coverage distribution\n",
    "    above_10_percent = (cleaned_df_lc['percentage'] >= 10).sum()\n",
    "    above_5_percent = (cleaned_df_lc['percentage'] >= 5).sum()\n",
    "    above_1_percent = (cleaned_df_lc['percentage'] >= 1).sum()\n",
    "    \n",
    "    print(f\"- Types with ‚â•10% coverage: {above_10_percent}\")\n",
    "    print(f\"- Types with ‚â•5% coverage: {above_5_percent}\")\n",
    "    print(f\"- Types with ‚â•1% coverage: {above_1_percent}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_type = cleaned_df_lc['lcType'].isna().sum()\n",
    "    missing_count = cleaned_df_lc['pixelCount'].isna().sum()\n",
    "    missing_percentage = cleaned_df_lc['percentage'].isna().sum()\n",
    "    \n",
    "    if missing_type > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing land cover type values: {missing_type}\")\n",
    "        quality_issues += 1\n",
    "    if missing_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing pixel count values: {missing_count}\")\n",
    "        quality_issues += 1\n",
    "    if missing_percentage > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing percentage values: {missing_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_pixels = (cleaned_df_lc['pixelCount'] < 0).sum()\n",
    "    negative_percentage = (cleaned_df_lc['percentage'] < 0).sum()\n",
    "    \n",
    "    if negative_pixels > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative pixel count values: {negative_pixels}\")\n",
    "        quality_issues += 1\n",
    "    if negative_percentage > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative percentage values: {negative_percentage}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check percentage sum\n",
    "    percentage_sum = cleaned_df_lc['percentage'].sum()\n",
    "    if abs(percentage_sum - 100) > 0.1:\n",
    "        print(f\"‚ö†Ô∏è  Percentage sum deviation: {percentage_sum:.1f}% (should be ~100%)\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate land cover types\n",
    "    duplicates = cleaned_df_lc['lcType'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Duplicate land cover types: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"‚úÖ No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning land cover data: {e}\")\n",
    "    print(\"Check that the land cover CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: Land Cover Type, Pixel Count\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_lc' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/lc.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_lc.columns)}\")\n",
    "    print(f\"- Data points: {len(cleaned_df_lc)} land cover types\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_lc.dtypes)}\")\n",
    "    print(f\"- Coverage range: {cleaned_df_lc['percentage'].min():.1f}% - {cleaned_df_lc['percentage'].max():.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned land cover data available\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure land cover CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: Land Cover Type, Pixel Count\")\n",
    "    print(\"   3. Verify pixel count values are numeric and positive\")\n",
    "    print(\"   4. Check that land cover types are properly named\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URBAN DEVELOPMENT DYNAMICS MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pug.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_uddm\" / \"chart_uddm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Checking prerequisite files...\n",
      "‚úÖ Population growth file found: data/processed/pg.csv\n",
      "‚úÖ Urban built area file found: data/processed/uba.csv\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚úÖ Successfully loaded population growth data: 22 records\n",
      "‚úÖ Successfully loaded urban built area data: 31 records\n",
      "‚úÖ Successfully merged datasets: 16 overlapping years\n",
      "Cleaned data saved to: data/processed/pug.csv\n",
      "Years covered: 2000 - 2015\n",
      "Total data points: 16\n",
      "Population range: 999,576.9 - 1,176,843.0\n",
      "UBA range: 143.54 - 184.92\n",
      "Density range: 6364.1 - 6963.8\n",
      "‚ö†Ô∏è  Note: 1 missing growth ratios (likely due to zero UBA growth)\n",
      "‚úÖ Population urban growth data merged and cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (16, 8)\n",
      "Cleaned data columns: ['yearName', 'population', 'populationGrowthPercentage', 'year', 'uba', 'ubaGrowthPercentage', 'density', 'populationUrbanGrowthRatio']\n",
      "Sample of cleaned data:\n",
      "   yearName  population  populationGrowthPercentage  year     uba  \\\n",
      "0      2000    999576.9                         NaN    16  143.54   \n",
      "1      2001   1012694.0                       1.312    17  145.80   \n",
      "2      2002   1025077.0                       1.223    18  147.57   \n",
      "3      2003   1036816.0                       1.145    19  149.55   \n",
      "4      2004   1048314.0                       1.109    20  151.97   \n",
      "5      2005   1059620.0                       1.078    21  154.07   \n",
      "6      2006   1074557.0                       1.410    22  156.57   \n",
      "7      2007   1088791.0                       1.325    23  159.13   \n",
      "8      2008   1102244.0                       1.236    24  162.38   \n",
      "9      2009   1114637.0                       1.124    25  166.04   \n",
      "\n",
      "   ubaGrowthPercentage   density populationUrbanGrowthRatio  \n",
      "0                1.961  6963.752                        NaN  \n",
      "1                1.574  6945.775                      0.834  \n",
      "2                1.214  6946.378                      1.007  \n",
      "3                1.342  6932.905                      0.853  \n",
      "4                1.618  6898.164                      0.685  \n",
      "5                1.382  6877.523                       0.78  \n",
      "6                1.623  6863.109                      0.869  \n",
      "7                1.635  6842.148                       0.81  \n",
      "8                2.042  6788.053                      0.605  \n",
      "9                2.254  6713.063                      0.499  \n",
      "\n",
      "Data validation:\n",
      "- Missing values: 2\n",
      "- Year range: 2000 - 2015\n",
      "- Population range: 999,576.9 - 1,176,843.0\n",
      "- UBA range: 143.54 - 184.92 sq km\n",
      "- Density range: 6364.1 - 6963.8 people/sq km\n",
      "\n",
      "Population vs Urban Growth Analysis:\n",
      "- Population growth rate range: 0.817% - 1.410%\n",
      "- UBA growth rate range: 1.033% - 2.254%\n",
      "- Average annual population growth: 1.094%\n",
      "- Average annual UBA growth: 1.720%\n",
      "- Population/Urban growth ratio range: 0.413 - 1.007\n",
      "- Average growth ratio: 0.680\n",
      "  üìâ Urban area growing faster than population (sprawl)\n",
      "\n",
      "Urban Density Analysis:\n",
      "- Starting density (2000): 6,963.8 people/sq km\n",
      "- Ending density (2015): 6,364.1 people/sq km\n",
      "- Total density change: -599.7 people/sq km (-8.6%)\n",
      "\n",
      "Data Quality Checks:\n",
      "‚ö†Ô∏è  Note: 1 missing growth ratio values (likely due to zero UBA growth)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pug.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['yearName', 'population', 'populationGrowthPercentage', 'year', 'uba', 'ubaGrowthPercentage', 'density', 'populationUrbanGrowthRatio']\n",
      "- Time series length: 16 years\n",
      "- Data types: {'yearName': dtype('int64'), 'population': dtype('float64'), 'populationGrowthPercentage': dtype('float64'), 'year': dtype('int64'), 'uba': dtype('float64'), 'ubaGrowthPercentage': dtype('float64'), 'density': dtype('float64'), 'populationUrbanGrowthRatio': dtype('O')}\n",
      "- Overlapping years between datasets: 16 out of potential maximum\n",
      "\n",
      "üìà Summary statistics:\n",
      "- Total population growth over period: 17.7%\n",
      "- Total urban area growth over period: 28.8%\n",
      "- NET population density change: -8.6%\n"
     ]
    }
   ],
   "source": [
    "# URBAN DEVELOPMENT DYNAMICS MATRIX: POPULATION URBAN GROWTH RATIO - pug.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_uddm\"/\"chart_ud\" (population vs urban growth analysis)\n",
    "\n",
    "# prereqs: ensure pg.csv and uba.csv have been generated from clean_pg and clean_uba functions\n",
    "print(\"üìã Checking prerequisite files...\")\n",
    "\n",
    "# check if required input files exist\n",
    "import os\n",
    "pg_file = 'data/processed/pg.csv'\n",
    "uba_file = 'data/processed/uba.csv'\n",
    "\n",
    "if os.path.exists(pg_file):\n",
    "    print(f\"‚úÖ Population growth file found: {pg_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Population growth file missing: {pg_file}\")\n",
    "    print(\"   Run clean_pg function first to generate this file\")\n",
    "\n",
    "if os.path.exists(uba_file):\n",
    "    print(f\"‚úÖ Urban built area file found: {uba_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Urban built area file missing: {uba_file}\")\n",
    "    print(\"   Run clean_uba function first to generate this file\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean and merge the data using the clean_pug function in clean.py\n",
    "try:\n",
    "    cleaned_df_pug = clean_pug()  # uses default paths: pg.csv and uba.csv\n",
    "    print(\"‚úÖ Population urban growth data merged and cleaned successfully!\")\n",
    "    \n",
    "    # Display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pug.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pug.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pug.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pug.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pug['yearName'].min()} - {cleaned_df_pug['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pug['population'].min():,} - {cleaned_df_pug['population'].max():,}\")\n",
    "    print(f\"- UBA range: {cleaned_df_pug['uba'].min():.2f} - {cleaned_df_pug['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Density range: {cleaned_df_pug['density'].min():.1f} - {cleaned_df_pug['density'].max():.1f} people/sq km\")\n",
    "    \n",
    "    # population vs urban growth analysis\n",
    "    print(f\"\\nPopulation vs Urban Growth Analysis:\")\n",
    "    print(f\"- Population growth rate range: {cleaned_df_pug['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pug['populationGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- UBA growth rate range: {cleaned_df_pug['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_pug['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # calculate averages (excluding NaN values)\n",
    "    avg_pop_growth = cleaned_df_pug['populationGrowthPercentage'].mean()\n",
    "    avg_uba_growth = cleaned_df_pug['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual population growth: {avg_pop_growth:.3f}%\")\n",
    "    print(f\"- Average annual UBA growth: {avg_uba_growth:.3f}%\")\n",
    "    \n",
    "    # growth ratio analysis\n",
    "    valid_ratios = cleaned_df_pug['populationUrbanGrowthRatio'].dropna()\n",
    "    if len(valid_ratios) > 0:\n",
    "        print(f\"- Population/Urban growth ratio range: {valid_ratios.min():.3f} - {valid_ratios.max():.3f}\")\n",
    "        print(f\"- Average growth ratio: {valid_ratios.mean():.3f}\")\n",
    "        \n",
    "        # interpret the growth patterns\n",
    "        if valid_ratios.mean() > 1:\n",
    "            print(\"  üìà Population growing faster than urban area (densification)\")\n",
    "        elif valid_ratios.mean() < 1:\n",
    "            print(\"  üìâ Urban area growing faster than population (sprawl)\")\n",
    "        else:\n",
    "            print(\"  ‚öñÔ∏è  Balanced population and urban growth\")\n",
    "    \n",
    "    # density analysis\n",
    "    print(f\"\\nUrban Density Analysis:\")\n",
    "    density_change = cleaned_df_pug['density'].iloc[-1] - cleaned_df_pug['density'].iloc[0]\n",
    "    density_change_pct = (density_change / cleaned_df_pug['density'].iloc[0]) * 100\n",
    "    print(f\"- Starting density ({cleaned_df_pug['yearName'].iloc[0]}): {cleaned_df_pug['density'].iloc[0]:,.1f} people/sq km\")\n",
    "    print(f\"- Ending density ({cleaned_df_pug['yearName'].iloc[-1]}): {cleaned_df_pug['density'].iloc[-1]:,.1f} people/sq km\")\n",
    "    print(f\"- Total density change: {density_change:+.1f} people/sq km ({density_change_pct:+.1f}%)\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # check for missing ratios\n",
    "    missing_ratios = cleaned_df_pug['populationUrbanGrowthRatio'].isna().sum()\n",
    "    if missing_ratios > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {missing_ratios} missing growth ratio values (likely due to zero UBA growth)\")\n",
    "        zero_uba_growth = cleaned_df_pug[cleaned_df_pug['ubaGrowthPercentage'] == 0]\n",
    "        if len(zero_uba_growth) > 0:\n",
    "            print(f\"   Years with zero UBA growth: {zero_uba_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # check for negative population growth\n",
    "    negative_pop_growth = cleaned_df_pug[cleaned_df_pug['populationGrowthPercentage'] < 0]\n",
    "    if len(negative_pop_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {len(negative_pop_growth)} years with population decline\")\n",
    "        print(f\"   Decline years: {negative_pop_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # check for negative urban growth\n",
    "    negative_uba_growth = cleaned_df_pug[cleaned_df_pug['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_uba_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(negative_uba_growth)} years with urban area decline\")\n",
    "        print(f\"   UBA decline years: {negative_uba_growth['yearName'].tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating population urban growth data: {e}\")\n",
    "    print(\"Check that both pg.csv and uba.csv files exist and have the correct format\")\n",
    "    print(\"Expected pg.csv columns: yearName, population, populationGrowthPercentage\")\n",
    "    print(\"Expected uba.csv columns: yearName, uba, ubaGrowthPercentage\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_pug' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pug.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable Notebook use\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pug.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_pug)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pug.dtypes)}\")\n",
    "    print(f\"- Overlapping years between datasets: {len(cleaned_df_pug)} out of potential maximum\")\n",
    "    \n",
    "    # summary statistics\n",
    "    print(f\"\\nüìà Summary statistics:\")\n",
    "    total_pop_growth = ((cleaned_df_pug['population'].iloc[-1] / cleaned_df_pug['population'].iloc[0]) - 1) * 100\n",
    "    total_uba_growth = ((cleaned_df_pug['uba'].iloc[-1] / cleaned_df_pug['uba'].iloc[0]) - 1) * 100\n",
    "    print(f\"- Total population growth over period: {total_pop_growth:.1f}%\")\n",
    "    print(f\"- Total urban area growth over period: {total_uba_growth:.1f}%\")\n",
    "    print(f\"- NET population density change: {density_change_pct:+.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure pg.csv exists (run clean_pg function)\")\n",
    "    print(\"   2. Ensure uba.csv exists (run clean_uba function)\")\n",
    "    print(\"   3. Check that both files have overlapping years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIMATE CONDITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pv.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pv\" / \"chart_pv\" (i.e., Seasonal availability of solar energy, January - December)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw photovoltaic potential data info:\n",
      "Shape: (12, 4)\n",
      "Columns: ['month', 'max', 'min', 'mean']\n",
      "Month range: 1 - 12\n",
      "PV max range: 4.07 - 5.47\n",
      "PV mean range: 3.92 - 5.35\n",
      "Total data points: 12\n",
      "Data preview:\n",
      "   month   max   min      mean\n",
      "0      1  5.33  5.05  5.213701\n",
      "1      2  5.47  5.17  5.350237\n",
      "2      3  5.20  4.81  5.032584\n",
      "3      4  4.72  4.20  4.486117\n",
      "4      5  4.16  3.80  3.969832\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/pv.csv\n",
      "Months covered: 12 months (full year)\n",
      "PV potential range: 4.07 - 5.47\n",
      "Peak month: Feb (5.47)\n",
      "Lowest month: Jun (4.07)\n",
      "Summer average (Jun-Aug): 4.20\n",
      "Winter average (Dec-Feb): 5.21\n",
      "Seasonal variation: -19.4% higher in summer\n",
      "‚úÖ Photovoltaic potential data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (12, 3)\n",
      "Cleaned data columns: ['month', 'monthName', 'maxPv']\n",
      "Sample of cleaned data:\n",
      "    month monthName  maxPv\n",
      "0       1       Jan   5.33\n",
      "1       2       Feb   5.47\n",
      "2       3       Mar   5.20\n",
      "3       4       Apr   4.72\n",
      "4       5       May   4.16\n",
      "5       6       Jun   4.07\n",
      "6       7       Jul   4.22\n",
      "7       8       Aug   4.31\n",
      "8       9       Sep   4.29\n",
      "9      10       Oct   4.19\n",
      "10     11       Nov   4.27\n",
      "11     12       Dec   4.84\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Month coverage: 12 months (should be 12)\n",
      "- Month range: 1 - 12\n",
      "- PV potential range: 4.07 - 5.47\n",
      "\n",
      "Solar Energy Analysis:\n",
      "- Peak solar month: Feb (5.47)\n",
      "- Lowest solar month: Jun (4.07)\n",
      "- Spring average (Mar-May): 4.69\n",
      "- Summer average (Jun-Aug): 4.20\n",
      "- Fall average (Sep-Nov): 4.25\n",
      "- Winter average (Dec-Feb): 5.21\n",
      "- Annual average: 4.59\n",
      "- Peak month deviation: +19.2% above average\n",
      "- Low month deviation: -11.3% below average\n",
      "- Summer/Winter ratio: 0.81x\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ Complete 12-month coverage\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pv.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['month', 'monthName', 'maxPv']\n",
      "- Time series type: Monthly (12 data points)\n",
      "- Data types: {'month': dtype('int64'), 'monthName': dtype('O'), 'maxPv': dtype('float64')}\n",
      "- Seasonal range: 0.81x variation from winter to summer\n",
      "\n",
      "üîã Solar Energy Data Summary:\n",
      "- Highest solar months: Feb, Jan, Mar\n",
      "- Lowest solar months: Jun, May, Oct\n"
     ]
    }
   ],
   "source": [
    "# PHOTOVOLTAIC POTENTIAL - pv.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pv\"/\"chart_pv\" (i.e., Seasonal availability of solar energy, January - December)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pv = pd.read_csv('data/raw/monthly-pv.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw photovoltaic potential data info:\")\n",
    "print(f\"Shape: {raw_df_pv.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pv.columns)}\")\n",
    "print(f\"Month range: {raw_df_pv['month'].min()} - {raw_df_pv['month'].max()}\")\n",
    "print(f\"PV max range: {raw_df_pv['max'].min():.2f} - {raw_df_pv['max'].max():.2f}\")\n",
    "print(f\"PV mean range: {raw_df_pv['mean'].min():.2f} - {raw_df_pv['mean'].max():.2f}\")\n",
    "print(f\"Total data points: {len(raw_df_pv)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pv.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_pv function in clean.py\n",
    "try:\n",
    "    cleaned_df_pv = clean_pv('data/raw/monthly-pv.csv')\n",
    "    print(\"‚úÖ Photovoltaic potential data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pv.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pv.head(12))  # Show all 12 months\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pv.isnull().sum().sum()}\")\n",
    "    print(f\"- Month coverage: {len(cleaned_df_pv)} months (should be 12)\")\n",
    "    print(f\"- Month range: {cleaned_df_pv['month'].min()} - {cleaned_df_pv['month'].max()}\")\n",
    "    print(f\"- PV potential range: {cleaned_df_pv['maxPv'].min():.2f} - {cleaned_df_pv['maxPv'].max():.2f}\")\n",
    "    \n",
    "    # solar energy analysis\n",
    "    print(f\"\\nSolar Energy Analysis:\")\n",
    "    \n",
    "    # identify peak and low months\n",
    "    peak_month = cleaned_df_pv.loc[cleaned_df_pv['maxPv'].idxmax()]\n",
    "    low_month = cleaned_df_pv.loc[cleaned_df_pv['maxPv'].idxmin()]\n",
    "    \n",
    "    print(f\"- Peak solar month: {peak_month['monthName']} ({peak_month['maxPv']:.2f})\")\n",
    "    print(f\"- Lowest solar month: {low_month['monthName']} ({low_month['maxPv']:.2f})\")\n",
    "    \n",
    "    # seasonal analysis\n",
    "    spring_months = cleaned_df_pv[cleaned_df_pv['month'].isin([3, 4, 5])]  # Mar, Apr, May\n",
    "    summer_months = cleaned_df_pv[cleaned_df_pv['month'].isin([6, 7, 8])]  # Jun, Jul, Aug\n",
    "    fall_months = cleaned_df_pv[cleaned_df_pv['month'].isin([9, 10, 11])]  # Sep, Oct, Nov\n",
    "    winter_months = cleaned_df_pv[cleaned_df_pv['month'].isin([12, 1, 2])]  # Dec, Jan, Feb\n",
    "    \n",
    "    spring_avg = spring_months['maxPv'].mean()\n",
    "    summer_avg = summer_months['maxPv'].mean()\n",
    "    fall_avg = fall_months['maxPv'].mean()\n",
    "    winter_avg = winter_months['maxPv'].mean()\n",
    "    \n",
    "    print(f\"- Spring average (Mar-May): {spring_avg:.2f}\")\n",
    "    print(f\"- Summer average (Jun-Aug): {summer_avg:.2f}\")\n",
    "    print(f\"- Fall average (Sep-Nov): {fall_avg:.2f}\")\n",
    "    print(f\"- Winter average (Dec-Feb): {winter_avg:.2f}\")\n",
    "    \n",
    "    # calculate seasonal variations\n",
    "    annual_avg = cleaned_df_pv['maxPv'].mean()\n",
    "    peak_variation = ((cleaned_df_pv['maxPv'].max() - annual_avg) / annual_avg) * 100 # i.e, \"the best month six% better than the average\"\n",
    "    low_variation = ((annual_avg - cleaned_df_pv['maxPv'].min()) / annual_avg) * 100 # i.e, \"the worst month 10% worse than the average\"\n",
    "    \n",
    "    print(f\"- Annual average: {annual_avg:.2f}\")\n",
    "    print(f\"- Peak month deviation: +{peak_variation:.1f}% above average\")\n",
    "    print(f\"- Low month deviation: -{low_variation:.1f}% below average\")\n",
    "    \n",
    "    # energy planning insights\n",
    "    summer_winter_ratio = summer_avg / winter_avg\n",
    "    print(f\"- Summer/Winter ratio: {summer_winter_ratio:.2f}x\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # check for complete month coverage\n",
    "    expected_months = set(range(1, 13))\n",
    "    actual_months = set(cleaned_df_pv['month'].unique())\n",
    "    missing_months = expected_months - actual_months\n",
    "    \n",
    "    if missing_months:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Missing months: {sorted(missing_months)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Complete 12-month coverage\")\n",
    "    \n",
    "    # check for reasonable PV values\n",
    "    if cleaned_df_pv['maxPv'].min() < 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Negative PV values detected (minimum: {cleaned_df_pv['maxPv'].min():.2f})\")\n",
    "    \n",
    "    # identify unusual patterns\n",
    "    monthly_diff = cleaned_df_pv['maxPv'].diff().abs()\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning photovoltaic potential data: {e}\")\n",
    "    print(\"Check that the monthly-pv.csv file exists and has the correct format\")\n",
    "    print(\"Expected columns: month, max, min, mean\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pv.csv, and export\n",
    "# (this is handled automatically by the clean_pv function, but confirming)\n",
    "if 'cleaned_df_pv' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pv.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pv.columns)}\")\n",
    "    print(f\"- Time series type: Monthly (12 data points)\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pv.dtypes)}\")\n",
    "    print(f\"- Seasonal range: {summer_avg/winter_avg:.2f}x variation from winter to summer\")\n",
    "    \n",
    "    # renewable energy planning insights\n",
    "    print(f\"\\nüîã Solar Energy Data Summary:\")\n",
    "    print(f\"- Highest solar months: {', '.join(cleaned_df_pv.nlargest(3, 'maxPv')['monthName'].tolist())}\")\n",
    "    print(f\"- Lowest solar months: {', '.join(cleaned_df_pv.nsmallest(3, 'maxPv')['monthName'].tolist())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure monthly-pv.csv exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: month, max, min, mean\")\n",
    "    print(\"   3. Verify data covers all 12 months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISK IDENTIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOOD EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fu.csv, pu.csv, cu.csv, and comb.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_fu\" / \"chart_fu\" (i.e., built-up area exposed to river (fluvial) flooding)\n",
    "#### 2.) \"plot_pu\" / \"chart_pu\" (i.e., built-up area exposed to rainwater (pluvial) flooding)\n",
    "#### 3.) \"plot_cu\" / \"chart_cu\" (i.e., built-up area exposed to coastal flooding)\n",
    "#### 4.) \"plot_comb\" / \"chart_comb\" (i.e., built-up area exposed to combined flooding)\n",
    "\n",
    "#### NOTE:\n",
    "#### 5.) data, raw, \"flood-events.csv\" is already ready for Observable Notebook \"plot_fe\" / \"chart_fe\" (i.e.,large flood events in city, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw flood risk data info:\n",
      "Shape: (31, 5)\n",
      "Columns: ['year', 'coastal_2020', 'comb_2020', 'fluvial_2020', 'pluvial_2020']\n",
      "Year range: 1985 - 2015\n",
      "Total data points: 31\n",
      "Available flood types: ['coastal_2020', 'comb_2020', 'fluvial_2020', 'pluvial_2020']\n",
      "Coastal risk range: 0.80 - 1.46\n",
      "Comb risk range: 20.59 - 49.09\n",
      "Fluvial risk range: 4.30 - 8.62\n",
      "Pluvial risk range: 16.62 - 41.52\n",
      "Data preview:\n",
      "   year  coastal_2020  comb_2020  fluvial_2020  pluvial_2020\n",
      "0  1985      0.802255  20.592107      4.299878     16.623193\n",
      "1  1986      0.856975  21.350234      4.404903     17.266586\n",
      "2  1987      0.887865  22.034225      4.447267     17.897623\n",
      "3  1988      0.892278  22.363424      4.524050     18.161511\n",
      "4  1989      0.919637  22.924738      4.593773     18.659280\n",
      "\n",
      "==================================================\n",
      "\n",
      "Available flood types: ['coastal', 'fluvial', 'pluvial', 'combined']\n",
      "‚úÖ Created cu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   CU range: 0.80 - 1.46\n",
      "‚úÖ Created fu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   FU range: 4.30 - 8.62\n",
      "‚úÖ Created pu.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   PU range: 16.62 - 41.52\n",
      "‚úÖ Created comb.csv: 31 records\n",
      "   Year range: 1985 - 2015\n",
      "   COMB range: 20.59 - 49.09\n",
      "\n",
      "Flood Risk Data Processing Summary:\n",
      "- Input file: data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_flood_wsf.csv\n",
      "- Output directory: data/processed\n",
      "- Files created: cu.csv, fu.csv, pu.csv, comb.csv\n",
      "- Missing flood types: set()\n",
      "\n",
      "Flood Risk Analysis:\n",
      "- Coastal flood risk:\n",
      "  Average: 1.17, Range: 0.80 - 1.46\n",
      "  Trend (1985-2015): +0.66 (+increase)\n",
      "- Fluvial flood risk:\n",
      "  Average: 6.35, Range: 4.30 - 8.62\n",
      "  Trend (1985-2015): +4.32 (+increase)\n",
      "- Pluvial flood risk:\n",
      "  Average: 28.05, Range: 16.62 - 41.52\n",
      "  Trend (1985-2015): +24.90 (+increase)\n",
      "- Combined flood risk:\n",
      "  Average: 33.75, Range: 20.59 - 49.09\n",
      "  Trend (1985-2015): +28.49 (+increase)\n",
      "- Dominant risk type (2015): Combined (49.09)\n",
      "‚úÖ Flood risk data processed successfully!\n",
      "\n",
      "üìä cu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'cu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 0.80 - 1.46\n",
      "- Sample data:\n",
      "   year  yearName    cu\n",
      "0     1      1985  0.80\n",
      "1     2      1986  0.86\n",
      "2     3      1987  0.89\n",
      "3     4      1988  0.89\n",
      "4     5      1989  0.92\n",
      "\n",
      "üìä fu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'fu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 4.30 - 8.62\n",
      "- Sample data:\n",
      "   year  yearName    fu\n",
      "0     1      1985  4.30\n",
      "1     2      1986  4.40\n",
      "2     3      1987  4.45\n",
      "3     4      1988  4.52\n",
      "4     5      1989  4.59\n",
      "\n",
      "üìä pu.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'pu']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 16.62 - 41.52\n",
      "- Sample data:\n",
      "   year  yearName     pu\n",
      "0     1      1985  16.62\n",
      "1     2      1986  17.27\n",
      "2     3      1987  17.90\n",
      "3     4      1988  18.16\n",
      "4     5      1989  18.66\n",
      "\n",
      "üìä comb.csv validation:\n",
      "- Shape: (31, 3)\n",
      "- Columns: ['year', 'yearName', 'comb']\n",
      "- Year range: 1985 - 2015\n",
      "- Risk range: 20.59 - 49.09\n",
      "- Sample data:\n",
      "   year  yearName   comb\n",
      "0     1      1985  20.59\n",
      "1     2      1986  21.35\n",
      "2     3      1987  22.03\n",
      "3     4      1988  22.36\n",
      "4     5      1989  22.92\n",
      "\n",
      "üåä Flood Risk Data Summary:\n",
      "\n",
      "- CU flood risk:\n",
      "  Average: 1.17\n",
      "  Range: 0.80 - 1.46\n",
      "  Standard deviation: 0.19\n",
      "  Change (1985-2015): +0.66\n",
      "\n",
      "- FU flood risk:\n",
      "  Average: 6.35\n",
      "  Range: 4.30 - 8.62\n",
      "  Standard deviation: 1.33\n",
      "  Change (1985-2015): +4.32\n",
      "\n",
      "- PU flood risk:\n",
      "  Average: 28.05\n",
      "  Range: 16.62 - 41.52\n",
      "  Standard deviation: 7.75\n",
      "  Change (1985-2015): +24.90\n",
      "\n",
      "- COMB flood risk:\n",
      "  Average: 33.75\n",
      "  Range: 20.59 - 49.09\n",
      "  Standard deviation: 8.86\n",
      "  Change (1985-2015): +28.50\n",
      "\n",
      "üìä 2015 Risk Values:\n",
      "- CU: 1.46\n",
      "- FU: 8.62\n",
      "- PU: 41.52\n",
      "- COMB: 49.09\n",
      "\n",
      "üîç Data Quality Checks:\n",
      "‚úÖ No data quality issues detected\n",
      "\n",
      "üìÅ Cleaned data saved to data/processed/:\n",
      "   ‚úÖ cu.csv\n",
      "   ‚úÖ fu.csv\n",
      "   ‚úÖ pu.csv\n",
      "   ‚úÖ comb.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Files created: 4\n",
      "- Time series length: 31 years\n",
      "- Year range: 1985 - 2015\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'cu': dtype('float64')}\n"
     ]
    }
   ],
   "source": [
    "# FLOODING - Multiple CSV preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_fu\"/\"chart_fu\" (i.e., built-up area exposed to river (fluvial) flooding)\n",
    "# 2.) \"plot_pu\"/\"chart_pu\" (i.e., built-up area exposed to rainwater (pluvial) flooding)\n",
    "# 3.) \"plot_cu\"/\"chart_cu\" (i.e., built-up area exposed to coastal flooding)\n",
    "# 4.) \"plot_comb\"/\"chart_comb\" (i.e., built-up area exposed to combined flooding)\n",
    "#5.) NOTE: data, raw, \"flood-events.csv\" is already ready for Observable Notebook \"plot_fe\" / \"chart_fe\" (i.e.,large flood events in city, country)\n",
    "\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "# NOTE: Flood data structure may vary by city - coastal cities have coastal flood data, inland cities do not\n",
    "raw_df_flood = pd.read_csv('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_flood_wsf.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw flood risk data info:\")\n",
    "print(f\"Shape: {raw_df_flood.shape}\")\n",
    "print(f\"Columns: {list(raw_df_flood.columns)}\")\n",
    "print(f\"Year range: {raw_df_flood['year'].min()} - {raw_df_flood['year'].max()}\")\n",
    "print(f\"Total data points: {len(raw_df_flood)}\")\n",
    "\n",
    "# identify available flood types\n",
    "flood_columns = [col for col in raw_df_flood.columns if '_2020' in col]\n",
    "print(f\"Available flood types: {flood_columns}\")\n",
    "\n",
    "# preview flood risk ranges for each type\n",
    "for col in flood_columns:\n",
    "    flood_type = col.replace('_2020', '')\n",
    "    print(f\"{flood_type.capitalize()} risk range: {raw_df_flood[col].min():.2f} - {raw_df_flood[col].max():.2f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_flood.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_flood function in clean.py\n",
    "try:\n",
    "    created_files = clean_flood('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_flood_wsf.csv')\n",
    "    print(\"‚úÖ Flood risk data processed successfully!\")\n",
    "    \n",
    "    # load and validate each created file\n",
    "    flood_dataframes = {}\n",
    "    \n",
    "    for filename in created_files:\n",
    "        file_path = f'data/processed/{filename}'\n",
    "        flood_type = filename.replace('.csv', '')\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            flood_dataframes[flood_type] = df\n",
    "            \n",
    "            print(f\"\\nüìä {filename} validation:\")\n",
    "            print(f\"- Shape: {df.shape}\")\n",
    "            print(f\"- Columns: {list(df.columns)}\")\n",
    "            print(f\"- Year range: {df['yearName'].min()} - {df['yearName'].max()}\")\n",
    "            print(f\"- Risk range: {df.iloc[:, 2].min():.2f} - {df.iloc[:, 2].max():.2f}\")  # third column is the risk value\n",
    "            print(f\"- Sample data:\")\n",
    "            print(df.head(5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "    \n",
    "    # basic flood risk analysis - just report the numbers\n",
    "    if len(flood_dataframes) > 0:\n",
    "        print(f\"\\nüåä Flood Risk Data Summary:\")\n",
    "        \n",
    "        for flood_type, df in flood_dataframes.items():\n",
    "            risk_column = df.columns[2]  # third column contains risk values\n",
    "            \n",
    "            # basic statistics\n",
    "            avg_risk = df[risk_column].mean()\n",
    "            max_risk = df[risk_column].max()\n",
    "            min_risk = df[risk_column].min()\n",
    "            std_risk = df[risk_column].std()\n",
    "            \n",
    "            # trend calculation\n",
    "            trend = df[risk_column].iloc[-1] - df[risk_column].iloc[0]\n",
    "            \n",
    "            print(f\"\\n- {flood_type.upper()} flood risk:\")\n",
    "            print(f\"  Average: {avg_risk:.2f}\")\n",
    "            print(f\"  Range: {min_risk:.2f} - {max_risk:.2f}\")\n",
    "            print(f\"  Standard deviation: {std_risk:.2f}\")\n",
    "            print(f\"  Change (1985-2015): {trend:+.2f}\")\n",
    "        \n",
    "        # current values\n",
    "        if len(flood_dataframes) > 1:\n",
    "            print(f\"\\nüìä 2015 Risk Values:\")\n",
    "            for flood_type, df in flood_dataframes.items():\n",
    "                risk_column = df.columns[2]\n",
    "                current_risk = df[risk_column].iloc[-1]\n",
    "                print(f\"- {flood_type.upper()}: {current_risk:.2f}\")\n",
    "        \n",
    "        # data quality checks\n",
    "        print(f\"\\nüîç Data Quality Checks:\")\n",
    "        \n",
    "        quality_issues = 0\n",
    "        for flood_type, df in flood_dataframes.items():\n",
    "            risk_column = df.columns[2]\n",
    "            \n",
    "            # check for missing values\n",
    "            missing_values = df[risk_column].isna().sum()\n",
    "            if missing_values > 0:\n",
    "                print(f\"‚ö†Ô∏è  {flood_type.upper()}: {missing_values} missing values\")\n",
    "                quality_issues += 1\n",
    "            \n",
    "            # check for negative values (mathematically impossible for risk)\n",
    "            negative_values = (df[risk_column] < 0).sum()\n",
    "            if negative_values > 0:\n",
    "                print(f\"‚ö†Ô∏è  {flood_type.upper()}: {negative_values} negative risk values\")\n",
    "                quality_issues += 1\n",
    "        \n",
    "        if quality_issues == 0:\n",
    "            print(\"‚úÖ No data quality issues detected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing flood risk data: {e}\")\n",
    "    print(\"Check that the flood CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, coastal_2020, fluvial_2020, pluvial_2020, comb_2020 (as available)\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'created_files' in locals() and len(created_files) > 0:\n",
    "    print(f\"\\nüìÅ Cleaned data saved to data/processed/:\")\n",
    "    for filename in created_files:\n",
    "        print(f\"   ‚úÖ {filename}\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Files created: {len(created_files)}\")\n",
    "    print(f\"- Time series length: {len(list(flood_dataframes.values())[0]) if flood_dataframes else 'N/A'} years\")\n",
    "    print(f\"- Year range: {raw_df_flood['year'].min()} - {raw_df_flood['year'].max()}\")\n",
    "    print(f\"- Data types: {dict(list(flood_dataframes.values())[0].dtypes) if flood_dataframes else 'N/A'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned flood data available\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure flood CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns with '_2020' suffix\")\n",
    "    print(\"   3. Verify data covers expected time range (1985-2015)\")\n",
    "    print(\"   4. Check that at least one flood type column exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EARTHQUAKE EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ee.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ee\" / \"chart_ee\" (i.e., Significant earthquakes within 500 km since 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw earthquake events data info:\n",
      "Shape: (15, 13)\n",
      "Columns: ['BEGAN', 'text', 'line1', 'line2', 'line3', 'line4', 'above_line', 'distance', 'eqMagnitude', 'magXdist', 'location', 'node_x', 'node_y']\n",
      "Total data points: 15\n",
      "Magnitude range: 4.9 - 7.4\n",
      "Distance range: 284 - 491 km\n",
      "Data preview:\n",
      "        BEGAN                                               text        line1  \\\n",
      "0  1919-08-19           AUGUST 1919; MNA; 424 km away; NA damage  AUGUST 1919   \n",
      "1  1950-07-09  JULY 1950; M6.1; 431 km away; Severe damage; 3...    JULY 1950   \n",
      "2  1961-06-16            JUNE 1961; M6.5; 284 km away; NA damage    JUNE 1961   \n",
      "3  1967-07-29  JULY 1967; M6.8; 491 km away; Moderate damage;...    JULY 1967   \n",
      "4  1974-04-18  APRIL 1974; M5; 484 km away; Moderate damage; ...   APRIL 1974   \n",
      "\n",
      "               line2            line3           line4  above_line    distance  \\\n",
      "0   MNA; 424 km away        NA damage             NaN          -1  423.516045   \n",
      "1  M6.1; 431 km away    Severe damage  300 fatalities           1  430.758880   \n",
      "2  M6.5; 284 km away        NA damage             NaN          -1  284.421667   \n",
      "3  M6.8; 491 km away  Moderate damage   20 fatalities           1  490.666578   \n",
      "4    M5; 484 km away  Moderate damage    4 fatalities          -1  483.666373   \n",
      "\n",
      "   eqMagnitude      magXdist                 location      node_x       node_y  \n",
      "0          NaN           NaN              , Maracaibo  1904-05-25 -4100.067699  \n",
      "1          6.1   9572.370830                Anboledas  1965-10-01  4100.067699  \n",
      "2          6.5  11151.259161               n Colombia  1946-03-23 -4100.067699  \n",
      "3          6.8  10263.467270                      NaN  1982-10-21  4100.067699  \n",
      "4          5.0   7581.668134  Ne, Cepita, San Andreas  1959-01-23 -4100.067699  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/ee.csv\n",
      "Earthquake events: 15\n",
      "Year range: 1919 - 2018\n",
      "Magnitude range: 4.9 - 7.4\n",
      "Distance range: 284 - 491 km\n",
      "‚úÖ Earthquake events data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (15, 7)\n",
      "Cleaned data columns: ['begin_year', 'distance', 'eqMagnitude', 'text', 'line1', 'line2', 'line3']\n",
      "Sample of cleaned data:\n",
      "   begin_year  distance  eqMagnitude  \\\n",
      "0        1919       424          NaN   \n",
      "1        1950       431          6.1   \n",
      "2        1961       284          6.5   \n",
      "3        1967       491          6.8   \n",
      "4        1974       484          5.0   \n",
      "5        1974       386          7.3   \n",
      "6        1976       445          6.7   \n",
      "7        1977       357          6.5   \n",
      "8        1980       426          5.0   \n",
      "9        1981       414          5.4   \n",
      "\n",
      "                                                text          line1  \\\n",
      "0           AUGUST 1919; MNA; 424 km away; NA damage    AUGUST 1919   \n",
      "1  JULY 1950; M6.1; 431 km away; Severe damage; 3...      JULY 1950   \n",
      "2            JUNE 1961; M6.5; 284 km away; NA damage      JUNE 1961   \n",
      "3  JULY 1967; M6.8; 491 km away; Moderate damage;...      JULY 1967   \n",
      "4  APRIL 1974; M5; 484 km away; Moderate damage; ...     APRIL 1974   \n",
      "5  JULY 1974; M7.3; 386 km away; Limited damage; ...      JULY 1974   \n",
      "6  JULY 1976; M6.7; 445 km away; Limited damage; ...      JULY 1976   \n",
      "7  AUGUST 1977; M6.5; 357 km away; Moderate damag...    AUGUST 1977   \n",
      "8    NOVEMBER 1980; M5; 426 km away; Moderate damage  NOVEMBER 1980   \n",
      "9  OCTOBER 1981; M5.4; 414 km away; Moderate dama...   OCTOBER 1981   \n",
      "\n",
      "               line2            line3  \n",
      "0   MNA; 424 km away        NA damage  \n",
      "1  M6.1; 431 km away    Severe damage  \n",
      "2  M6.5; 284 km away        NA damage  \n",
      "3  M6.8; 491 km away  Moderate damage  \n",
      "4    M5; 484 km away  Moderate damage  \n",
      "5  M7.3; 386 km away   Limited damage  \n",
      "6  M6.7; 445 km away   Limited damage  \n",
      "7  M6.5; 357 km away  Moderate damage  \n",
      "8    M5; 426 km away  Moderate damage  \n",
      "9  M5.4; 414 km away  Moderate damage  \n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 1919 - 2018\n",
      "- Magnitude range: 4.9 - 7.4\n",
      "- Distance range: 284 - 491 km\n",
      "\n",
      "Earthquake Data Summary:\n",
      "- Total events: 15\n",
      "- Average magnitude: 6.2\n",
      "- Average distance: 426 km\n",
      "- Standard deviation (magnitude): 0.8\n",
      "- Standard deviation (distance): 55 km\n",
      "- Time span: 99 years\n",
      "- Average events per decade: 1.5\n",
      "- Strongest earthquake: 7.4 magnitude in 1992\n",
      "- Closest earthquake: 284 km in 1961\n",
      "\n",
      "Data Quality Checks:\n",
      "‚ö†Ô∏è  Missing magnitude values: 1\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/ee.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['begin_year', 'distance', 'eqMagnitude', 'text', 'line1', 'line2', 'line3']\n",
      "- Time series length: 15 events\n",
      "- Year range: 1919 - 2018\n",
      "- Data types: {'begin_year': dtype('int64'), 'distance': Int64Dtype(), 'eqMagnitude': dtype('float64'), 'text': dtype('O'), 'line1': dtype('O'), 'line2': dtype('O'), 'line3': dtype('O')}\n"
     ]
    }
   ],
   "source": [
    "# EARTHQUAKE EVENTS - ee.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ee\"/\"chart_ee\" (Significant Earthquakes within 500 km since 1900)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_ee = pd.read_csv('data/raw/earthquake-events.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw earthquake events data info:\")\n",
    "print(f\"Shape: {raw_df_ee.shape}\")\n",
    "print(f\"Columns: {list(raw_df_ee.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_ee)}\")\n",
    "\n",
    "# preview key data ranges if available\n",
    "if 'eqMagnitude' in raw_df_ee.columns:\n",
    "    print(f\"Magnitude range: {raw_df_ee['eqMagnitude'].min():.1f} - {raw_df_ee['eqMagnitude'].max():.1f}\")\n",
    "if 'distance' in raw_df_ee.columns:\n",
    "    print(f\"Distance range: {raw_df_ee['distance'].min():.0f} - {raw_df_ee['distance'].max():.0f} km\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_ee.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_ee function in clean.py\n",
    "try:\n",
    "    cleaned_df_ee = clean_ee('data/raw/earthquake-events.csv')\n",
    "    print(\"‚úÖ Earthquake events data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_ee.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_ee.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_ee.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_ee.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_ee['begin_year'].min()} - {cleaned_df_ee['begin_year'].max()}\")\n",
    "    print(f\"- Magnitude range: {cleaned_df_ee['eqMagnitude'].min():.1f} - {cleaned_df_ee['eqMagnitude'].max():.1f}\")\n",
    "    print(f\"- Distance range: {cleaned_df_ee['distance'].min():.0f} - {cleaned_df_ee['distance'].max():.0f} km\")\n",
    "    \n",
    "    # earthquake data analysis\n",
    "    print(f\"\\nEarthquake Data Summary:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    avg_magnitude = cleaned_df_ee['eqMagnitude'].mean()\n",
    "    avg_distance = cleaned_df_ee['distance'].mean()\n",
    "    \n",
    "    print(f\"- Total events: {len(cleaned_df_ee)}\")\n",
    "    print(f\"- Average magnitude: {avg_magnitude:.1f}\")\n",
    "    print(f\"- Average distance: {avg_distance:.0f} km\")\n",
    "    print(f\"- Standard deviation (magnitude): {cleaned_df_ee['eqMagnitude'].std():.1f}\")\n",
    "    print(f\"- Standard deviation (distance): {cleaned_df_ee['distance'].std():.0f} km\")\n",
    "    \n",
    "    # temporal distribution\n",
    "    years_span = cleaned_df_ee['begin_year'].max() - cleaned_df_ee['begin_year'].min()\n",
    "    events_per_decade = (len(cleaned_df_ee) / years_span) * 10 if years_span > 0 else 0\n",
    "    \n",
    "    print(f\"- Time span: {years_span} years\")\n",
    "    print(f\"- Average events per decade: {events_per_decade:.1f}\")\n",
    "    \n",
    "    # identify extremes\n",
    "    strongest_eq = cleaned_df_ee.loc[cleaned_df_ee['eqMagnitude'].idxmax()]\n",
    "    closest_eq = cleaned_df_ee.loc[cleaned_df_ee['distance'].idxmin()]\n",
    "    \n",
    "    print(f\"- Strongest earthquake: {strongest_eq['eqMagnitude']:.1f} magnitude in {strongest_eq['begin_year']}\")\n",
    "    print(f\"- Closest earthquake: {closest_eq['distance']:.0f} km in {closest_eq['begin_year']}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_magnitude = cleaned_df_ee['eqMagnitude'].isna().sum()\n",
    "    missing_distance = cleaned_df_ee['distance'].isna().sum()\n",
    "    missing_year = cleaned_df_ee['begin_year'].isna().sum()\n",
    "    \n",
    "    if missing_magnitude > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing magnitude values: {missing_magnitude}\")\n",
    "        quality_issues += 1\n",
    "    if missing_distance > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing distance values: {missing_distance}\")\n",
    "        quality_issues += 1\n",
    "    if missing_year > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing year values: {missing_year}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_magnitude = (cleaned_df_ee['eqMagnitude'] < 0).sum()\n",
    "    negative_distance = (cleaned_df_ee['distance'] < 0).sum()\n",
    "    \n",
    "    if negative_magnitude > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative magnitude values: {negative_magnitude}\")\n",
    "        quality_issues += 1\n",
    "    if negative_distance > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative distance values: {negative_distance}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate events (same year, magnitude, and distance)\n",
    "    duplicates = cleaned_df_ee.duplicated(subset=['begin_year', 'eqMagnitude', 'distance']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Potential duplicate events: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"‚úÖ No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning earthquake events data: {e}\")\n",
    "    print(\"Check that the earthquake-events.csv file exists and has the correct format\")\n",
    "    print(\"Expected columns: BEGAN, eqMagnitude, distance, text, line1, line2, line3\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_ee' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/ee.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_ee.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_ee)} events\")\n",
    "    print(f\"- Year range: {cleaned_df_ee['begin_year'].min()} - {cleaned_df_ee['begin_year'].max()}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_ee.dtypes)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned earthquake data available\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure earthquake-events.csv exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: BEGAN, eqMagnitude, distance, text, line1, line2, line3\")\n",
    "    print(\"   3. Verify data contains parseable dates in BEGAN column\")\n",
    "    print(\"   4. Check that magnitude and distance values are numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELEVATION (need alternative to donut chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# elevation data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "\n",
    "elevation = [\n",
    "      { \"bin\": \"-5-40m\", \"count\": 413599, \"total\": 549697, \"percentage\": 75.24},\n",
    "      { \"bin\": \"40-90m\", \"count\": 94379, \"total\": 549697, \"percentage\": 17.17 },\n",
    "      { \"bin\": \"90-135m\", \"count\": 32786 , \"total\": 549697, \"percentage\": 5.96 },\n",
    "      { \"bin\": \"135-185m\", \"count\": 8043, \"total\": 549697, \"percentage\": 1.46 },\n",
    "      { \"bin\": \"135-235\", \"count\": 890, \"total\": 549697, \"percentage\": 0.16 },\n",
    "]\n",
    "\n",
    "# convert elevation list to dataframe, elevation_df\n",
    "elevation_df = pd.DataFrame(elevation)\n",
    "\n",
    "# create output CSV of elevation_df for plotting\n",
    "elevation_output_df = pd.DataFrame({\n",
    "    'bin': elevation_df['bin'],\n",
    "    'count': elevation_df['count'],\n",
    "    'total': elevation_df['total'], \n",
    "    'percentage': elevation_df['percentage']\n",
    "})\n",
    "\n",
    "# save elevation_output_df for elevation data to CSV\n",
    "elevation_output_df.to_csv('data/processed/elevation.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLOPE (need alternative to donut chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# slope data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "\n",
    "slope = [\n",
    "      { \"bin\": \"0-2\", \"count\": 428343, \"total\": 549702, \"percentage\": 77.92 },\n",
    "      { \"bin\": \"2-5\", \"count\": 79034, \"total\": 549702, \"percentage\": 14.38 },\n",
    "      { \"bin\": \"5-10\", \"count\": 31121, \"total\": 549702, \"percentage\": 5.66 },\n",
    "      { \"bin\": \"10-20\", \"count\": 10147, \"total\": 549702, \"percentage\": 1.85 },\n",
    "      { \"bin\": \"20+\", \"count\": 1057, \"total\": 549702, \"percentage\": 0.19 },\n",
    "]\n",
    "\n",
    "# convert slope list to dataframe, slope_df\n",
    "slope_df = pd.DataFrame(slope)\n",
    "\n",
    "# create output CSV of slope_df for plotting\n",
    "slope_output_df = pd.DataFrame({\n",
    "    'bin': slope_df['bin'],\n",
    "    'count': slope_df['count'],\n",
    "    'total': slope_df['total'], \n",
    "    'percentage': slope_df['percentage']\n",
    "})\n",
    "\n",
    "# save slope_output_df for slope data to CSV\n",
    "slope_output_df.to_csv('data/processed/slope.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRE WEATHER INDEX (FWI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fwi.csv\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_fwi\" / \"chart_fwi\" (i.e., Fire Weather Index (FWI), January - December)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw fire weather index data info:\n",
      "Shape: (53, 2)\n",
      "Columns: ['week', 'pctile_95']\n",
      "Total data points: 53\n",
      "Week range: 1 - 53\n",
      "FWI range: 0.79 - 39.11\n",
      "Data preview:\n",
      "   week  pctile_95\n",
      "0     1  23.661456\n",
      "1     2  24.501692\n",
      "2     3  29.096334\n",
      "3     4  31.582048\n",
      "4     5  31.770628\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/fwi.csv\n",
      "Weeks covered: 53 weeks\n",
      "Week range: 1 - 53\n",
      "FWI range: 0.79 - 39.11\n",
      "Peak fire weather month: Feb (max FWI: 39.11)\n",
      "‚úÖ Fire weather index data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (53, 3)\n",
      "Cleaned data columns: ['week', 'monthName', 'fwi']\n",
      "Sample of cleaned data:\n",
      "   week monthName    fwi\n",
      "0     1       Jan  23.66\n",
      "1     2       Jan  24.50\n",
      "2     3       Jan  29.10\n",
      "3     4       Jan  31.58\n",
      "4     5       Feb  31.77\n",
      "5     6       Feb  36.42\n",
      "6     7       Feb  37.98\n",
      "7     8       Feb  39.05\n",
      "8     9       Feb  39.11\n",
      "9    10       Mar  37.26\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Week coverage: 53 weeks\n",
      "- Week range: 1 - 53\n",
      "- FWI range: 0.79 - 39.11\n",
      "\n",
      "Fire Weather Data Summary:\n",
      "- Total weeks: 53\n",
      "- Average FWI: 13.31\n",
      "- Median FWI: 8.11\n",
      "- Standard deviation: 13.29\n",
      "- Highest FWI: 39.11 (Week 9, Feb)\n",
      "- Lowest FWI: 0.79 (Week 44, Nov)\n",
      "\n",
      "Monthly FWI Statistics:\n",
      "- Jan: Mean 27.21, Range 23.66 - 31.58\n",
      "- Feb: Mean 36.87, Range 31.77 - 39.11\n",
      "- Mar: Mean 36.29, Range 35.00 - 37.26\n",
      "- Apr: Mean 17.18, Range 6.98 - 27.30\n",
      "- May: Mean 4.76, Range 2.43 - 8.60\n",
      "- Jun: Mean 6.02, Range 2.91 - 9.37\n",
      "- Jul: Mean 9.01, Range 5.96 - 11.20\n",
      "- Aug: Mean 3.59, Range 2.85 - 4.70\n",
      "- Sep: Mean 0.90, Range 0.84 - 0.95\n",
      "- Oct: Mean 1.14, Range 0.81 - 1.70\n",
      "- Nov: Mean 1.54, Range 0.79 - 2.37\n",
      "- Dec: Mean 13.69, Range 4.89 - 19.92\n",
      "\n",
      "Data Quality Checks:\n",
      "‚úÖ No data quality issues detected\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/fwi.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['week', 'monthName', 'fwi']\n",
      "- Time series length: 53 weeks\n",
      "- Data types: {'week': dtype('int64'), 'monthName': dtype('O'), 'fwi': dtype('float64')}\n",
      "- FWI value range: 0.79 - 39.11\n"
     ]
    }
   ],
   "source": [
    "# FIRE WEATHER INDEX (FWI) - fwi.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_fwi\"/\"chart_fwi\" (Fire Weather Index (FWI), January - December)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_fwi = pd.read_csv('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_fwi.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw fire weather index data info:\")\n",
    "print(f\"Shape: {raw_df_fwi.shape}\")\n",
    "print(f\"Columns: {list(raw_df_fwi.columns)}\")\n",
    "print(f\"Total data points: {len(raw_df_fwi)}\")\n",
    "\n",
    "# preview key data ranges if available\n",
    "if 'week' in raw_df_fwi.columns:\n",
    "    print(f\"Week range: {raw_df_fwi['week'].min()} - {raw_df_fwi['week'].max()}\")\n",
    "if 'pctile_95' in raw_df_fwi.columns:\n",
    "    print(f\"FWI range: {raw_df_fwi['pctile_95'].min():.2f} - {raw_df_fwi['pctile_95'].max():.2f}\")\n",
    "\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_fwi.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using the clean_fwi function in clean.py\n",
    "try:\n",
    "    cleaned_df_fwi = clean_fwi('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_fwi.csv')\n",
    "    print(\"‚úÖ Fire weather index data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_fwi.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_fwi.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_fwi.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_fwi.isnull().sum().sum()}\")\n",
    "    print(f\"- Week coverage: {len(cleaned_df_fwi)} weeks\")\n",
    "    print(f\"- Week range: {cleaned_df_fwi['week'].min()} - {cleaned_df_fwi['week'].max()}\")\n",
    "    print(f\"- FWI range: {cleaned_df_fwi['fwi'].min():.2f} - {cleaned_df_fwi['fwi'].max():.2f}\")\n",
    "    \n",
    "    # fire weather analysis\n",
    "    print(f\"\\nFire Weather Data Summary:\")\n",
    "    \n",
    "    # basic statistics\n",
    "    avg_fwi = cleaned_df_fwi['fwi'].mean()\n",
    "    median_fwi = cleaned_df_fwi['fwi'].median()\n",
    "    std_fwi = cleaned_df_fwi['fwi'].std()\n",
    "    \n",
    "    print(f\"- Total weeks: {len(cleaned_df_fwi)}\")\n",
    "    print(f\"- Average FWI: {avg_fwi:.2f}\")\n",
    "    print(f\"- Median FWI: {median_fwi:.2f}\")\n",
    "    print(f\"- Standard deviation: {std_fwi:.2f}\")\n",
    "    \n",
    "    # identify extremes\n",
    "    peak_week = cleaned_df_fwi.loc[cleaned_df_fwi['fwi'].idxmax()]\n",
    "    lowest_week = cleaned_df_fwi.loc[cleaned_df_fwi['fwi'].idxmin()]\n",
    "    \n",
    "    print(f\"- Highest FWI: {peak_week['fwi']:.2f} (Week {peak_week['week']}, {peak_week['monthName']})\")\n",
    "    print(f\"- Lowest FWI: {lowest_week['fwi']:.2f} (Week {lowest_week['week']}, {lowest_week['monthName']})\")\n",
    "    \n",
    "    # monthly statistics\n",
    "    monthly_stats = cleaned_df_fwi.groupby('monthName')['fwi'].agg(['mean', 'max', 'min']).round(2)\n",
    "    \n",
    "    print(f\"\\nMonthly FWI Statistics:\")\n",
    "    for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']:\n",
    "        if month in monthly_stats.index:\n",
    "            stats = monthly_stats.loc[month]\n",
    "            print(f\"- {month}: Mean {stats['mean']:.2f}, Range {stats['min']:.2f} - {stats['max']:.2f}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    quality_issues = 0\n",
    "    \n",
    "    # check for missing values in key columns\n",
    "    missing_week = cleaned_df_fwi['week'].isna().sum()\n",
    "    missing_fwi = cleaned_df_fwi['fwi'].isna().sum()\n",
    "    missing_month = cleaned_df_fwi['monthName'].isna().sum()\n",
    "    \n",
    "    if missing_week > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing week values: {missing_week}\")\n",
    "        quality_issues += 1\n",
    "    if missing_fwi > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing FWI values: {missing_fwi}\")\n",
    "        quality_issues += 1\n",
    "    if missing_month > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing month values: {missing_month}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for impossible values\n",
    "    negative_fwi = (cleaned_df_fwi['fwi'] < 0).sum()\n",
    "    if negative_fwi > 0:\n",
    "        print(f\"‚ö†Ô∏è  Negative FWI values: {negative_fwi}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for week sequence\n",
    "    expected_weeks = set(range(1, 54))  # 53 weeks in a year\n",
    "    actual_weeks = set(cleaned_df_fwi['week'].unique())\n",
    "    missing_weeks = expected_weeks - actual_weeks\n",
    "    if len(missing_weeks) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing weeks: {sorted(list(missing_weeks))}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    # check for duplicate weeks\n",
    "    duplicates = cleaned_df_fwi['week'].duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Duplicate week entries: {duplicates}\")\n",
    "        quality_issues += 1\n",
    "    \n",
    "    if quality_issues == 0:\n",
    "        print(\"‚úÖ No data quality issues detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning fire weather index data: {e}\")\n",
    "    print(\"Check that the FWI CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: week, pctile_95\")\n",
    "\n",
    "# save confirmation and next steps\n",
    "if 'cleaned_df_fwi' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/fwi.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_fwi.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_fwi)} weeks\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_fwi.dtypes)}\")\n",
    "    print(f\"- FWI value range: {cleaned_df_fwi['fwi'].min():.2f} - {cleaned_df_fwi['fwi'].max():.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No cleaned fire weather data available\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Ensure FWI CSV file exists in data/raw/\")\n",
    "    print(\"   2. Check file has correct columns: week, pctile_95\")\n",
    "    print(\"   3. Verify week numbers are sequential (1-53)\")\n",
    "    print(\"   4. Check that FWI values are numeric and non-negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HISTORICAL BURNT AREA & FIRE WEATHER INDEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate historical burnt area & fire weather index data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "fwi = [\n",
    "      { \"week\": 1, \"monthName\": \"Jan\", \"fwi\": 36.03855628967285},\n",
    "      { \"week\": 2, \"monthName\": \"Jan\", \"fwi\": 28.35186767578125},\n",
    "      { \"week\": 3, \"monthName\": \"Jan\", \"fwi\": 34.48613758087156},\n",
    "      { \"week\": 4, \"monthName\": \"Jan\", \"fwi\": 35.160119628906244},\n",
    "      { \"week\": 5, \"monthName\": \"Feb\", \"fwi\": 41.2155460357666},\n",
    "      { \"week\": 6, \"monthName\": \"Feb\", \"fwi\": 42.91906299591064},\n",
    "      { \"week\": 7, \"monthName\": \"Feb\", \"fwi\": 40.35708732604981},\n",
    "      { \"week\": 8, \"monthName\": \"Feb\", \"fwi\": 35.13482322692871},\n",
    "      { \"week\": 9, \"monthName\": \"Feb\", \"fwi\": 43.7328405380249},\n",
    "      { \"week\": 10, \"monthName\": \"Mar\", \"fwi\": 55.629536437988264},\n",
    "      { \"week\": 11, \"monthName\": \"Mar\", \"fwi\": 51.963145637512206},\n",
    "      { \"week\": 12, \"monthName\": \"Mar\", \"fwi\": 48.64410858154295},\n",
    "      { \"week\": 13, \"monthName\": \"Mar\", \"fwi\": 48.45940856933592},\n",
    "      { \"week\": 14, \"monthName\": \"Apr\", \"fwi\": 42.525428390502924},\n",
    "      { \"week\": 15, \"monthName\": \"Apr\", \"fwi\": 48.989934921264634},\n",
    "      { \"week\": 16, \"monthName\": \"Apr\", \"fwi\": 47.94815864562989},\n",
    "      { \"week\": 17, \"monthName\": \"Apr\", \"fwi\": 59.693392562866215},\n",
    "      { \"week\": 18, \"monthName\": \"May\", \"fwi\": 53.26485347747803},\n",
    "      { \"week\": 19, \"monthName\": \"May\", \"fwi\": 67.04015121459962},\n",
    "      { \"week\": 20, \"monthName\": \"May\", \"fwi\": 66.2925880432129},\n",
    "      { \"week\": 21, \"monthName\": \"May\", \"fwi\": 63.51103172302246},\n",
    "      { \"week\": 22, \"monthName\": \"May\", \"fwi\": 57.59551124572754},\n",
    "      { \"week\": 23, \"monthName\": \"Jun\", \"fwi\": 66.97727813720704},\n",
    "      { \"week\": 24, \"monthName\": \"Jun\", \"fwi\": 75.7531303405762},\n",
    "      { \"week\": 25, \"monthName\": \"Jun\", \"fwi\": 80.30134506225586},\n",
    "      { \"week\": 26, \"monthName\": \"Jun\", \"fwi\": 90.69736862182619},\n",
    "      { \"week\": 27, \"monthName\": \"Jul\", \"fwi\": 75.26012268066407},\n",
    "      { \"week\": 28, \"monthName\": \"Jul\", \"fwi\": 95.59054870605469},\n",
    "      { \"week\": 29, \"monthName\": \"Jul\", \"fwi\": 82.06852722167967},\n",
    "      { \"week\": 30, \"monthName\": \"Jul\", \"fwi\": 81.8968620300293},\n",
    "      { \"week\": 31, \"monthName\": \"Aug\", \"fwi\": 81.7047821044922},\n",
    "      { \"week\": 32, \"monthName\": \"Aug\", \"fwi\": 81.58447265625001}, \n",
    "      { \"week\": 33, \"monthName\": \"Aug\", \"fwi\": 65.29224243164063},  \n",
    "      { \"week\": 34, \"monthName\": \"Aug\", \"fwi\": 67.29769515991212},  \n",
    "      { \"week\": 35, \"monthName\": \"Aug\", \"fwi\": 64.21281738281252},  \n",
    "      { \"week\": 36, \"monthName\": \"Sep\", \"fwi\": 69.20558013916019},  \n",
    "      { \"week\": 37, \"monthName\": \"Sep\", \"fwi\": 59.376176834106474},  \n",
    "      { \"week\": 38, \"monthName\": \"Sep\", \"fwi\": 50.01441955566406},  \n",
    "      { \"week\": 39, \"monthName\": \"Sep\", \"fwi\": 40.38814010620118 },  \n",
    "      { \"week\": 40, \"monthName\": \"Oct\", \"fwi\": 48.369334793090815},  \n",
    "      { \"week\": 41, \"monthName\": \"Oct\", \"fwi\": 43.82190437316895},  \n",
    "      { \"week\": 42, \"monthName\": \"Oct\", \"fwi\": 37.03949813842773},  \n",
    "      { \"week\": 43, \"monthName\": \"Oct\", \"fwi\": 50.04811096191406},  \n",
    "      { \"week\": 44, \"monthName\": \"Nov\", \"fwi\": 47.38101158142093},  \n",
    "      { \"week\": 45, \"monthName\": \"Nov\", \"fwi\": 37.50416679382325},  \n",
    "      { \"week\": 46, \"monthName\": \"Nov\", \"fwi\": 29.76080322265625},  \n",
    "      { \"week\": 47, \"monthName\": \"Nov\", \"fwi\": 36.063685607910124},  \n",
    "      { \"week\": 48, \"monthName\": \"Nov\", \"fwi\": 34.42437210083008},  \n",
    "      { \"week\": 49, \"monthName\": \"Dec\", \"fwi\": 32.008924865722626},  \n",
    "      { \"week\": 50, \"monthName\": \"Dec\", \"fwi\": 33.579549407958986},  \n",
    "      { \"week\": 51, \"monthName\": \"Dec\", \"fwi\": 31.927024841308594},  \n",
    "      { \"week\": 52, \"monthName\": \"Dec\", \"fwi\": 34.5278169631958},  \n",
    "      { \"week\": 53, \"monthName\": \"Dec\", \"fwi\": 31.089004516601562}  \n",
    "\n",
    "]\n",
    "\n",
    "# convert fwi list to dataframe, fwi_df\n",
    "fwi_df = pd.DataFrame(fwi)\n",
    "\n",
    "# create output CSV of fwi_df for plotting\n",
    "fwi_output_df = pd.DataFrame({\n",
    "    'week': fwi_df['week'],\n",
    "    'monthName': fwi_df['monthName'],\n",
    "    'fwi': fwi_df['fwi'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save fwi_output_df for fwi data to CSV\n",
    "fwi_output_df.to_csv('data/processed/fwi.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   week monthName    fwi\n",
      "0     1       Jan  36.04\n",
      "1     2       Jan  28.35\n",
      "2     3       Jan  34.49\n",
      "3     4       Jan  35.16\n",
      "4     5       Feb  41.22\n",
      "5     6       Feb  42.92\n",
      "6     7       Feb  40.36\n",
      "7     8       Feb  35.13\n",
      "8     9       Feb  43.73\n",
      "9    10       Mar  55.63\n",
      "\n",
      "Total number of records: 53\n",
      "Month names: ['Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep' 'Oct' 'Nov' 'Dec']\n",
      "fwi values: [36.04 28.35 34.49 35.16 41.22 42.92 40.36 35.13 43.73 55.63 51.96 48.64\n",
      " 48.46 42.53 48.99 47.95 59.69 53.26 67.04 66.29 63.51 57.6  66.98 75.75\n",
      " 80.3  90.7  75.26 95.59 82.07 81.9  81.7  81.58 65.29 67.3  64.21 69.21\n",
      " 59.38 50.01 40.39 48.37 43.82 37.04 50.05 47.38 37.5  29.76 36.06 34.42\n",
      " 32.01 33.58 31.93 34.53 31.09]\n"
     ]
    }
   ],
   "source": [
    "# fwi data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(fwi_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(fwi_output_df)}\")\n",
    "print(f\"Month names: {fwi_output_df['monthName'].unique()}\")\n",
    "print(f\"fwi values: {fwi_output_df['fwi'].unique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

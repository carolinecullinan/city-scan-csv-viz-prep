{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Scan Data Cleaning\n",
    "##### June 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data cleaning pipeline for appropriate CSV preparation necessary for City Scan JavaScript plots with Cartagena, Colombia as the case study example city for pipeline scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory changes\n",
      "current working directory is: /Users/carolinecullinan/dev/wb/city-scan-csv-viz-prep\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# change to project root directory\n",
    "os.chdir('../')\n",
    "print(\"directory changes\")\n",
    "print(f\"current working directory is:\", os.getcwd())\n",
    "\n",
    "# local imports (after changing directory)\n",
    "from src.clean import clean_pg, clean_pas, clean_uba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POPULATION AND DEMOGRAPHIC TRENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pg.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pga\" / \"chart_pga\" ; and\n",
    "#### 2.) \"plot_pgp\" / \"chart_pg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population growth data info:\n",
      "Shape: (22, 7)\n",
      "Columns: ['Group', 'Location', 'Country', 'Year', 'Population', 'Source', 'Method']\n",
      "Date range: 2000 - 2021\n",
      "Data preview:\n",
      "   Group Location  Country  Year  Population  Source  Method\n",
      "0  Tunis    Tunis  Tunisia  2000     1969032  Oxford  Oxford\n",
      "1  Tunis    Tunis  Tunisia  2001     1984750  Oxford  Oxford\n",
      "2  Tunis    Tunis  Tunisia  2002     2000614  Oxford  Oxford\n",
      "3  Tunis    Tunis  Tunisia  2003     2016605  Oxford  Oxford\n",
      "4  Tunis    Tunis  Tunisia  2004     2035590  Oxford  Oxford\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/pg.csv\n",
      "Years covered: 2000 - 2021\n",
      "Total data points: 22\n",
      "Population range: 1,969,032 - 2,696,439\n",
      "‚úÖ Population growth data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (22, 3)\n",
      "Cleaned data columns: ['yearName', 'population', 'populationGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   yearName  population  populationGrowthPercentage\n",
      "0      2000     1969032                         NaN\n",
      "1      2001     1984750                       0.798\n",
      "2      2002     2000614                       0.799\n",
      "3      2003     2016605                       0.799\n",
      "4      2004     2035590                       0.941\n",
      "5      2005     2070274                       1.704\n",
      "6      2006     2105593                       1.706\n",
      "7      2007     2141514                       1.706\n",
      "8      2008     2178094                       1.708\n",
      "9      2009     2215198                       1.704\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 2000 - 2021\n",
      "- Population range: 1,969,032 - 2,696,439\n",
      "- Growth rate range: 0.798% - 1.708%\n",
      "‚ö†Ô∏è  Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pg.csv\n",
      "‚úÖ Ready for Observable visualization!\n"
     ]
    }
   ],
   "source": [
    "# POPULATION & DEMOGRAPHIC TRENDS - pg.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pga\"/\"chart_pga\" (absolute population growth); and \n",
    "# 2.) \"plot_pgp\"/\"chart_pgp\" (population growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "# NOTE: right now, csv file is for Tunis, Tunisia because no access to population-growth.csv for Cartagena, Colombia - fix later with appropriate data - this is just a placeholder for pipeline purposes\n",
    "raw_df_pg = pd.read_csv('data/raw/population-growth.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw population growth data info:\")\n",
    "print(f\"Shape: {raw_df_pg.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pg.columns)}\")\n",
    "print(f\"Date range: {raw_df_pg['Year'].min()} - {raw_df_pg['Year'].max()}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pg.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using our clean_pg function\n",
    "try:\n",
    "    cleaned_df_pg = clean_pg('data/raw/population-growth.csv')\n",
    "    print(\"‚úÖ Population growth data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pg.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pg.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pg.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pg.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_pg['yearName'].min()} - {cleaned_df_pg['yearName'].max()}\")\n",
    "    print(f\"- Population range: {cleaned_df_pg['population'].min():,} - {cleaned_df_pg['population'].max():,}\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_pg['populationGrowthPercentage'].min():.3f}% - {cleaned_df_pg['populationGrowthPercentage'].max():.3f}%\")\n",
    "    \n",
    "    # check for any potential data quality issues\n",
    "    if cleaned_df_pg['populationGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {cleaned_df_pg['populationGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning population growth data: {e}\")\n",
    "    print(\"Check that 'data/raw/population-growth.csv' exists and has the correct format\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pg.csv, and export\n",
    "# (This is handled automatically by the clean_pg function, but confirming)\n",
    "if 'cleaned_df_pg' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pg.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pas.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_pas\" / \"chart_pas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw population age structure data info:\n",
      "Shape: (36, 3)\n",
      "Columns: ['age_group', 'sex', 'population']\n",
      "Age groups: ['0-1', '1-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "Sex categories: ['f' 'm']\n",
      "Total population: 1,319,285\n",
      "Data preview:\n",
      "  age_group sex    population\n",
      "0       1-4   f  42167.329278\n",
      "1       1-4   m  45029.741031\n",
      "2       0-1   f  10386.217327\n",
      "3       0-1   m  11119.716253\n",
      "4       5-9   f  50745.108513\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  Warning: Could not sort by age bracket (\"['age_sort'] not found in axis\"). Using default sorting.\n",
      "Cleaned data saved to: data/processed/pas.csv\n",
      "Total population: 1,319,285\n",
      "Age brackets: 17\n",
      "Sex categories: 2\n",
      "Total records: 34\n",
      "‚úÖ Population age structure data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (34, 5)\n",
      "Cleaned data columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "Sample of cleaned data:\n",
      "  ageBracket     sex     count  percentage  yearName\n",
      "0        0-4  female  52553.55    3.983486      2021\n",
      "1        0-4    male  56149.46    4.256051      2021\n",
      "2      10-14  female  53631.29    4.065178      2021\n",
      "3      10-14    male  56959.22    4.317430      2021\n",
      "4      15-19  female  55799.87    4.229553      2021\n",
      "5      15-19    male  58415.47    4.427812      2021\n",
      "6      20-24  female  54857.68    4.158137      2021\n",
      "7      20-24    male  54082.97    4.099414      2021\n",
      "8      25-29  female  55018.60    4.170334      2021\n",
      "9      25-29    male  52944.24    4.013101      2021\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 0\n",
      "- Age brackets: ['0-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '5-9', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']\n",
      "- Sex categories: ['female', 'male']\n",
      "- Population count range: 7,923 - 58,415\n",
      "- Percentage range: 0.601% - 4.428%\n",
      "- Year: 2021\n",
      "- Total percentage sum: 100.000% (should be ~100%)\n",
      "- Population by sex: Female: 663,596, Male: 655,689\n",
      "- Age brackets: 17, Total records: 34\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/pas.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['ageBracket', 'sex', 'count', 'percentage', 'yearName']\n",
      "- Records per sex: 17, 17\n",
      "- Data types: {'ageBracket': dtype('O'), 'sex': dtype('O'), 'count': dtype('float64'), 'percentage': dtype('float64'), 'yearName': dtype('int64')}\n"
     ]
    }
   ],
   "source": [
    "# POPULATION AGE SEX - pas.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_pas\"/\"chart_pas\" (population age sex, i.e., population by sex and age bracket, (i.e., Population Distribution by Age & Sex, xxxx))\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_pas = pd.read_csv('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_demographics.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw population age structure data info:\")\n",
    "print(f\"Shape: {raw_df_pas.shape}\")\n",
    "print(f\"Columns: {list(raw_df_pas.columns)}\")\n",
    "print(f\"Age groups: {sorted(raw_df_pas['age_group'].unique())}\")\n",
    "print(f\"Sex categories: {raw_df_pas['sex'].unique()}\")\n",
    "print(f\"Total population: {raw_df_pas['population'].sum():,.0f}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_pas.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using our clean_pas function\n",
    "try:\n",
    "    cleaned_df_pas = clean_pas('data/raw/2025-04-colombia-cartagena_02-process-output_tabular_cartagena_demographics.csv')\n",
    "    print(\"‚úÖ Population age structure data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_pas.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_pas.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_pas.isnull().sum().sum()}\")\n",
    "    print(f\"- Age brackets: {sorted(cleaned_df_pas['ageBracket'].unique())}\")\n",
    "    print(f\"- Sex categories: {sorted(cleaned_df_pas['sex'].unique())}\")\n",
    "    print(f\"- Population count range: {cleaned_df_pas['count'].min():,.0f} - {cleaned_df_pas['count'].max():,.0f}\")\n",
    "    print(f\"- Percentage range: {cleaned_df_pas['percentage'].min():.3f}% - {cleaned_df_pas['percentage'].max():.3f}%\")\n",
    "    print(f\"- Year: {cleaned_df_pas['yearName'].iloc[0]}\")\n",
    "    \n",
    "    # data quality checks\n",
    "    total_percentage = cleaned_df_pas['percentage'].sum()\n",
    "    print(f\"- Total percentage sum: {total_percentage:.3f}% (should be ~100%)\")\n",
    "    \n",
    "    if abs(total_percentage - 100) > 0.1:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Percentage sum deviates from 100% by {abs(total_percentage - 100):.3f}%\")\n",
    "    \n",
    "    # check for balanced sex representation\n",
    "    sex_counts = cleaned_df_pas.groupby('sex')['count'].sum()\n",
    "    print(f\"- Population by sex: Female: {sex_counts.get('female', 0):,.0f}, Male: {sex_counts.get('male', 0):,.0f}\")\n",
    "    \n",
    "    # check age bracket coverage\n",
    "    expected_brackets = len(cleaned_df_pas['ageBracket'].unique())\n",
    "    actual_records = len(cleaned_df_pas)\n",
    "    print(f\"- Age brackets: {expected_brackets}, Total records: {actual_records}\")\n",
    "    \n",
    "    if actual_records != expected_brackets * 2:  # Should be 2 records per age bracket (male/female)\n",
    "        print(f\"‚ö†Ô∏è  Note: Expected {expected_brackets * 2} records (2 per age bracket), found {actual_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning population age structure data: {e}\")\n",
    "    print(\"Check that the demographics CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: age_group, sex, population\")\n",
    "\n",
    "# save the cleaned data as a CSV file - pas.csv, and export\n",
    "# (This is handled automatically by the clean_pas function, but confirming)\n",
    "if 'cleaned_df_pas' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/pas.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_pas.columns)}\")\n",
    "    print(f\"- Records per sex: {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'female'])}, {len(cleaned_df_pas[cleaned_df_pas['sex'] == 'male'])}\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_pas.dtypes)}\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILT FORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uba.csv preparation\n",
    "### Observable Notebook functions/charts:\n",
    "#### 1.) \"plot_ubaa\" / \"chart_ubaa\" ; and\n",
    "#### 2.) \"plot_ubap\" / \"chart_ubap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw urban built area data info:\n",
      "Shape: (31, 2)\n",
      "Columns: ['year', 'cumulative sq km']\n",
      "Year range: 1985 - 2015\n",
      "UBA range: 98.56 - 184.92 sq km\n",
      "Total data points: 31\n",
      "Data preview:\n",
      "   year  cumulative sq km\n",
      "0  1985         98.562692\n",
      "1  1986        100.892675\n",
      "2  1987        103.206772\n",
      "3  1988        104.745090\n",
      "4  1989        106.680565\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned data saved to: data/processed/uba.csv\n",
      "Years covered: 1985 - 2015\n",
      "Total data points: 31\n",
      "UBA range: 98.56 - 184.92 sq km\n",
      "‚úÖ Urban built area data cleaned successfully!\n",
      "\n",
      "Cleaned data shape: (31, 4)\n",
      "Cleaned data columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "Sample of cleaned data:\n",
      "   year  yearName     uba  ubaGrowthPercentage\n",
      "0     1      1985   98.56                  NaN\n",
      "1     2      1986  100.89                2.364\n",
      "2     3      1987  103.21                2.300\n",
      "3     4      1988  104.75                1.492\n",
      "4     5      1989  106.68                1.842\n",
      "5     6      1990  109.99                3.103\n",
      "6     7      1991  115.45                4.964\n",
      "7     8      1992  117.07                1.403\n",
      "8     9      1993  119.32                1.922\n",
      "9    10      1994  120.69                1.148\n",
      "\n",
      "Data validation:\n",
      "- Missing values: 1\n",
      "- Year range: 1985 - 2015\n",
      "- UBA range: 98.56 - 184.92 sq km\n",
      "- Growth rate range: 0.655% - 5.367%\n",
      "- Total urban expansion: 86.36 sq km over 30 years\n",
      "\n",
      "Urban growth analysis:\n",
      "- Average annual UBA growth rate: 2.125%\n",
      "‚ö†Ô∏è  Note: 1 missing growth rate values (expected for first year)\n",
      "\n",
      "üìÅ Cleaned data saved to: data/processed/uba.csv\n",
      "‚úÖ Ready for Observable visualization!\n",
      "\n",
      "üìä Data structure summary for Observable:\n",
      "- Columns: ['year', 'yearName', 'uba', 'ubaGrowthPercentage']\n",
      "- Time series length: 31 years\n",
      "- Data types: {'year': dtype('int64'), 'yearName': dtype('int64'), 'uba': dtype('float64'), 'ubaGrowthPercentage': dtype('float64')}\n",
      "- Urban expansion factor: 1.88x growth over period\n"
     ]
    }
   ],
   "source": [
    "# URBAN BUILT AREA - uba.csv preparation for Observable Notebook plot functions/charts:\n",
    "# 1.) \"plot_ubaa\"/\"chart_ubaa\" (absolute urban extenet and change)\n",
    "# 2.) \"plot_ubap\"/\"chart_ubap\" (urban extent and change growth percentage)\n",
    "\n",
    "# load \"raw\" (i.e. \"dirty\") tabular output data\n",
    "raw_df_uba = pd.read_csv('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_wsf_stats.csv')\n",
    "\n",
    "# display basic info about the raw data\n",
    "print(\"Raw urban built area data info:\")\n",
    "print(f\"Shape: {raw_df_uba.shape}\")\n",
    "print(f\"Columns: {list(raw_df_uba.columns)}\")\n",
    "print(f\"Year range: {raw_df_uba['year'].min()} - {raw_df_uba['year'].max()}\")\n",
    "print(f\"UBA range: {raw_df_uba['cumulative sq km'].min():.2f} - {raw_df_uba['cumulative sq km'].max():.2f} sq km\")\n",
    "print(f\"Total data points: {len(raw_df_uba)}\")\n",
    "print(f\"Data preview:\")\n",
    "print(raw_df_uba.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# clean the data using our clean_uba function\n",
    "try:\n",
    "    cleaned_df_uba = clean_uba('data/raw/2025-04-colombia-cartagena_other_02-process-output_tabular_cartagena_other_wsf_stats.csv')\n",
    "    print(\"‚úÖ Urban built area data cleaned successfully!\")\n",
    "    \n",
    "    # display cleaned data info\n",
    "    print(f\"\\nCleaned data shape: {cleaned_df_uba.shape}\")\n",
    "    print(f\"Cleaned data columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"Sample of cleaned data:\")\n",
    "    print(cleaned_df_uba.head(10))\n",
    "    \n",
    "    # basic data validation\n",
    "    print(f\"\\nData validation:\")\n",
    "    print(f\"- Missing values: {cleaned_df_uba.isnull().sum().sum()}\")\n",
    "    print(f\"- Year range: {cleaned_df_uba['yearName'].min()} - {cleaned_df_uba['yearName'].max()}\")\n",
    "    print(f\"- UBA range: {cleaned_df_uba['uba'].min():.2f} - {cleaned_df_uba['uba'].max():.2f} sq km\")\n",
    "    print(f\"- Growth rate range: {cleaned_df_uba['ubaGrowthPercentage'].min():.3f}% - {cleaned_df_uba['ubaGrowthPercentage'].max():.3f}%\")\n",
    "    print(f\"- Total urban expansion: {cleaned_df_uba['uba'].max() - cleaned_df_uba['uba'].min():.2f} sq km over {cleaned_df_uba['yearName'].max() - cleaned_df_uba['yearName'].min()} years\")\n",
    "    \n",
    "    # data quality checks\n",
    "    print(f\"\\nUrban growth analysis:\")\n",
    "    # Calculate average annual growth rate\n",
    "    avg_growth = cleaned_df_uba['ubaGrowthPercentage'].mean()\n",
    "    print(f\"- Average annual UBA growth rate: {avg_growth:.3f}%\")\n",
    "    \n",
    "    # Check for any potential data quality issues\n",
    "    if cleaned_df_uba['ubaGrowthPercentage'].isna().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {cleaned_df_uba['ubaGrowthPercentage'].isna().sum()} missing growth rate values (expected for first year)\")\n",
    "    \n",
    "    # Check for negative growth (urban area should generally increase)\n",
    "    negative_growth = cleaned_df_uba[cleaned_df_uba['ubaGrowthPercentage'] < 0]\n",
    "    if len(negative_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(negative_growth)} years with negative UBA growth detected\")\n",
    "        print(f\"   Years with decline: {negative_growth['yearName'].tolist()}\")\n",
    "    \n",
    "    # Check for extremely high growth rates (potential data errors)\n",
    "    high_growth = cleaned_df_uba[cleaned_df_uba['ubaGrowthPercentage'] > 20]  # >20% annual growth\n",
    "    if len(high_growth) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Note: {len(high_growth)} years with very high UBA growth (>20%)\")\n",
    "        print(f\"   High growth years: {high_growth['yearName'].tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cleaning urban built area data: {e}\")\n",
    "    print(\"Check that the UBA CSV file exists and has the correct format\")\n",
    "    print(\"Expected columns: year, cumulative sq km\")\n",
    "\n",
    "# save the cleaned data as a CSV file - uba.csv, and export\n",
    "# (This is handled automatically by the clean_uba function, but confirming)\n",
    "if 'cleaned_df_uba' in locals():\n",
    "    print(f\"\\nüìÅ Cleaned data saved to: data/processed/uba.csv\")\n",
    "    print(f\"‚úÖ Ready for Observable visualization!\")\n",
    "    \n",
    "    # quick preview of the structure for Observable\n",
    "    print(f\"\\nüìä Data structure summary for Observable:\")\n",
    "    print(f\"- Columns: {list(cleaned_df_uba.columns)}\")\n",
    "    print(f\"- Time series length: {len(cleaned_df_uba)} years\")\n",
    "    print(f\"- Data types: {dict(cleaned_df_uba.dtypes)}\")\n",
    "    print(f\"- Urban expansion factor: {cleaned_df_uba['uba'].max() / cleaned_df_uba['uba'].min():.2f}x growth over period\")\n",
    "else:\n",
    "    print(\"‚ùå No cleaned data available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URBAN EXTENT AND CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate urban extent and change data for Tunis, Tunisia given \"tabular\" output from City Scan GCP process\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "uba = [\n",
    "      { \"year\": 1, \"yearName\": \"1985\", \"uba\": 166.08101814331798},\n",
    "      { \"year\": 2, \"yearName\": \"1986\", \"uba\": 172.44786586480038},\n",
    "      { \"year\": 3, \"yearName\": \"1987\", \"uba\": 185.11900630951914},\n",
    "      { \"year\": 4, \"yearName\": \"1988\", \"uba\": 202.44611527369096},\n",
    "      { \"year\": 5, \"yearName\": \"1989\", \"uba\": 209.3484337801595},\n",
    "      { \"year\": 6, \"yearName\": \"1990\", \"uba\": 218.8655512133127},\n",
    "      { \"year\": 7, \"yearName\": \"1991\", \"uba\": 226.97080233605266},\n",
    "      { \"year\": 8, \"yearName\": \"1992\", \"uba\": 231.32775796388964},\n",
    "      { \"year\": 9, \"yearName\": \"1993\", \"uba\": 235.31000915223282},\n",
    "      { \"year\": 10, \"yearName\": \"1994\", \"uba\": 239.59690318203425},\n",
    "      { \"year\": 11, \"yearName\": \"1995\", \"uba\": 244.28727695052265},\n",
    "      { \"year\": 12, \"yearName\": \"1996\", \"uba\": 249.05396781687122},\n",
    "      { \"year\": 13, \"yearName\": \"1997\", \"uba\": 253.29457114797057},\n",
    "      { \"year\": 14, \"yearName\": \"1998\", \"uba\": 256.0263479213753},\n",
    "      { \"year\": 15, \"yearName\": \"1999\", \"uba\": 258.9113844404829},\n",
    "      { \"year\": 16, \"yearName\": \"2000\", \"uba\": 261.66818321318607},\n",
    "      { \"year\": 17, \"yearName\": \"2001\", \"uba\": 264.4919158340125},\n",
    "      { \"year\": 18, \"yearName\": \"2002\", \"uba\": 267.0854460612935},\n",
    "      { \"year\": 19, \"yearName\": \"2003\", \"uba\": 270.3870988687197},\n",
    "      { \"year\": 20, \"yearName\": \"2004\", \"uba\": 276.21722470525106},\n",
    "      { \"year\": 21, \"yearName\": \"2005\", \"uba\": 281.41742170944474},\n",
    "      { \"year\": 22, \"yearName\": \"2006\", \"uba\": 288.76825955333743},\n",
    "      { \"year\": 23, \"yearName\": \"2007\", \"uba\": 294.66531923799204},\n",
    "      { \"year\": 24, \"yearName\": \"2008\", \"uba\": 301.9654875333054},\n",
    "      { \"year\": 25, \"yearName\": \"2009\", \"uba\": 310.2853023000293},\n",
    "      { \"year\": 26, \"yearName\": \"2010\", \"uba\": 317.42908309972756},\n",
    "      { \"year\": 27, \"yearName\": \"2011\", \"uba\": 322.2602056142696},\n",
    "      { \"year\": 28, \"yearName\": \"2012\", \"uba\": 327.013134381004},\n",
    "      { \"year\": 29, \"yearName\": \"2013\", \"uba\": 332.55550722560355},\n",
    "      { \"year\": 30, \"yearName\": \"2014\", \"uba\": 337.01380195059915},\n",
    "      { \"year\": 31, \"yearName\": \"2015\", \"uba\": 341.3507399789974}\n",
    "]\n",
    "\n",
    "# convert uba list to dataframe, uba_df\n",
    "uba_df = pd.DataFrame(uba)\n",
    "\n",
    "# create output CSV of df for plotting\n",
    "uba_output_df = pd.DataFrame({\n",
    "    'year': uba_df['year'],\n",
    "    'yearName': uba_df['yearName'],\n",
    "    'uba': uba_df['uba'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# calculate uba growth rate as a percentage for each year and round it to 3 decimal places\n",
    "uba_output_df['ubaGrowthPercentage'] = uba_output_df['uba'].pct_change() * 100\n",
    "uba_output_df['ubaGrowthPercentage'] = uba_output_df['ubaGrowthPercentage'].round(3)\n",
    "\n",
    "# save uba_output_df for uba data to CSV\n",
    "uba_output_df.to_csv('data/processed/uba.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   year yearName     uba  ubaGrowthPercentage\n",
      "0     1     1985  166.08                  NaN\n",
      "1     2     1986  172.45                3.836\n",
      "2     3     1987  185.12                7.347\n",
      "3     4     1988  202.45                9.361\n",
      "4     5     1989  209.35                3.408\n",
      "5     6     1990  218.87                4.547\n",
      "6     7     1991  226.97                3.701\n",
      "7     8     1992  231.33                1.921\n",
      "8     9     1993  235.31                1.720\n",
      "9    10     1994  239.60                1.823\n",
      "\n",
      "Total number of records: 31\n",
      "Year names: ['1985' '1986' '1987' '1988' '1989' '1990' '1991' '1992' '1993' '1994'\n",
      " '1995' '1996' '1997' '1998' '1999' '2000' '2001' '2002' '2003' '2004'\n",
      " '2005' '2006' '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014'\n",
      " '2015']\n",
      "UBA values: [166.08 172.45 185.12 202.45 209.35 218.87 226.97 231.33 235.31 239.6\n",
      " 244.29 249.05 253.29 256.03 258.91 261.67 264.49 267.09 270.39 276.22\n",
      " 281.42 288.77 294.67 301.97 310.29 317.43 322.26 327.01 332.56 337.01\n",
      " 341.35]\n"
     ]
    }
   ],
   "source": [
    "# uba data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(uba_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(uba_output_df)}\")\n",
    "print(f\"Year names: {uba_output_df['yearName'].unique()}\")\n",
    "print(f\"UBA values: {uba_output_df['uba'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DENSITY & POPULATION-URBAN GROWTH RATIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate pug data (population urban growth) and population-urban growth for Tunis, Tunisia given pg.csv genergated via df and uba.csv generated via uba_output_df\n",
    "\n",
    "# Note: the poulation growth rate / urban growth rate ratio can only be calculated if the yearName is the same in both pg.csv and uba.csv (i.e., for dataset years that overlap, 2000-2015 in the case of Tunis, Tunisia) - can we call MORE Oxford/WorldPop data for the years 1985-onward for more information regaridng this ratio (i.e., given that the UBA data goes back to 1985)?\n",
    "\n",
    "# read pg.csv and uba.csv\n",
    "pg_df = pd.read_csv('data/processed/pg.csv')\n",
    "uba_df = pd.read_csv('data/processed/uba.csv')\n",
    "\n",
    "# merge pg_df and uba_df on yearName to create pug\n",
    "pug_df = pd.merge(pg_df, uba_df, on='yearName', how='inner')\n",
    "\n",
    "# calculate density\n",
    "pug_df['density'] = pug_df['population'] / pug_df['uba']\n",
    "pug_df['density'] = pug_df['density'].round(3)\n",
    "\n",
    "# calculate population-urban growth percentage ratio\n",
    "pug_df['populationUrbanGrowthRatio'] = pug_df['populationGrowthPercentage'] / pug_df['ubaGrowthPercentage']\n",
    "pug_df['populationUrbanGrowthRatio'] = pug_df['populationUrbanGrowthRatio'].round(3)\n",
    "\n",
    "\n",
    "# save pug_df for population urban growth data to CSV\n",
    "pug_df.to_csv('data/processed/pug.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   yearName  population  populationGrowthPercentage  year     uba  \\\n",
      "0      2000     1969032                         NaN    16  261.67   \n",
      "1      2001     1984750                       0.798    17  264.49   \n",
      "2      2002     2000614                       0.799    18  267.09   \n",
      "3      2003     2016605                       0.799    19  270.39   \n",
      "4      2004     2035590                       0.941    20  276.22   \n",
      "5      2005     2070274                       1.704    21  281.42   \n",
      "6      2006     2105593                       1.706    22  288.77   \n",
      "7      2007     2141514                       1.706    23  294.67   \n",
      "8      2008     2178094                       1.708    24  301.97   \n",
      "9      2009     2215198                       1.704    25  310.29   \n",
      "\n",
      "   ubaGrowthPercentage   density  populationUrbanGrowthRatio  \n",
      "0                1.066  7524.867                         NaN  \n",
      "1                1.078  7504.064                       0.740  \n",
      "2                0.983  7490.411                       0.813  \n",
      "3                1.236  7458.135                       0.646  \n",
      "4                2.156  7369.452                       0.436  \n",
      "5                1.883  7356.528                       0.905  \n",
      "6                2.612  7291.592                       0.653  \n",
      "7                2.043  7267.499                       0.835  \n",
      "8                2.477  7212.948                       0.690  \n",
      "9                2.755  7139.121                       0.619  \n",
      "\n",
      "Total number of records: 16\n",
      "Year names: [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n",
      " 2014 2015]\n",
      "Population urban growth values: [  nan 0.74  0.813 0.646 0.436 0.905 0.653 0.835 0.69  0.619 0.741 1.121\n",
      " 1.159 1.004 1.274 1.259]\n"
     ]
    }
   ],
   "source": [
    "# pug data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(pug_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(pug_df)}\")\n",
    "print(f\"Year names: {pug_df['yearName'].unique()}\")\n",
    "print(f\"Population urban growth values: {pug_df['populationUrbanGrowthRatio'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAND COVER (need alternative to donut chart - do tree map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_land_cover_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use the clean_land_cover_csv function from lc_cleanup.py to clean the tabular-output land cover csv file so that it can be plotted\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m clean_land_cover_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/2025-02-tunisia-tunis_02-process-output_tabular_tunis_lc.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_land_cover_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# use the clean_land_cover_csv function from lc_cleanup.py to clean the tabular-output land cover csv file so that it can be plotted\n",
    "clean_land_cover_csv('data/raw/2025-02-tunisia-tunis_02-process-output_tabular_tunis_lc.csv', 'data/processed/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# land cover data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "# (edit lc so that percentage is calculated from pixelCount and pixelTotal)\n",
    "lc = [\n",
    "      { \"lcType\": \"Built up\", \"pixelCount\": 3438092.2117647, \"pixelTotal\": 6068006.37255, \"percentage\": 56.66 },\n",
    "      { \"lcType\": \"Grassland\", \"pixelCount\": 1022591.69411765, \"pixelTotal\": 6068006.37255, \"percentage\": 16.85 },\n",
    "      { \"lcType\": \"Permanent water bodies\", \"pixelCount\": 563266.486274511, \"pixelTotal\": 6068006.37255, \"percentage\": 9.28},\n",
    "      { \"lcType\": \"Tree cover\", \"pixelCount\": 385035.090196078, \"pixelTotal\": 6068006.37255, \"percentage\": 6.345 },\n",
    "      { \"lcType\": \"Cropland\", \"pixelCount\": 346731.71372549, \"pixelTotal\": 6068006.37255, \"percentage\": 5.71},\n",
    "      { \"lcType\": \"Bare sparse vegetation\", \"pixelCount\": 168537.729411765, \"pixelTotal\": 6068006.37255, \"percentage\": 2.78},\n",
    "      { \"lcType\": \"Shrubland\", \"pixelCount\": 15153.5294117647, \"pixelTotal\": 6068006.37255, \"percentage\": 2.49},\n",
    "      { \"lcType\": \"Herbaceous wetland\", \"pixelCount\": 128597.917647059, \"pixelTotal\": 6068006.37255, \"percentage\": 2.12},\n",
    "      { \"lcType\": \"Snow and ice\", \"pixelCount\": 0, \"pixelTotal\": 6068006.37255, \"percentage\": 0},\n",
    "      { \"lcType\": \"Mangroves\", \"pixelCount\": 0, \"pixelTotal\": 6068006.37255, \"percentage\": 0},\n",
    "      { \"lcType\": \"Moss and lichens\", \"pixelCount\": 0, \"pixelTotal\": 6068006.37255, \"percentage\": 0},\n",
    "]\n",
    "\n",
    "# convert lc list to dataframe, lc_df\n",
    "lc_df = pd.DataFrame(lc)\n",
    "\n",
    "# create output CSV of lc_df for plotting\n",
    "lc_output_df = pd.DataFrame({\n",
    "    'lcType': lc_df['lcType'],\n",
    "    'pixelCount': lc_df['pixelCount'],\n",
    "    'pixelTotal': lc_df['pixelTotal'], \n",
    "    'percentage': lc_df['percentage']\n",
    "})\n",
    "\n",
    "# save lc_output_df for lc data to CSV\n",
    "lc_output_df.to_csv('data/processed/lc.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 CLIMATE CONDITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHOTOVOLTAIC POWER POTENTIAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate photovoltaic power potential data (i.e., seasonal availa bility of solar energy, plotting the \"max\" value ) for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "pv = [\n",
    "      { \"month\": 1, \"monthName\": \"Jan\", \"max\": 3.31, \"min\": 3.04, \"mean\": 3.20},\n",
    "      { \"month\": 2, \"monthName\": \"Feb\", \"max\": 3.94, \"min\": 3.72, \"mean\": 3.84},\n",
    "      { \"month\": 3, \"monthName\": \"Mar\", \"max\": 4.53, \"min\": 4.32, \"mean\": 4.44},\n",
    "      { \"month\": 4, \"monthName\": \"Apr\", \"max\": 4.87, \"min\": 4.70, \"mean\": 4.79},\n",
    "      { \"month\": 5, \"monthName\": \"May\", \"max\": 5.17, \"min\": 4.99, \"mean\": 5.09},\n",
    "      { \"month\": 6, \"monthName\": \"Jun\", \"max\": 5.47, \"min\": 5.30, \"mean\": 5.39},\n",
    "      { \"month\": 7, \"monthName\": \"Jul\", \"max\": 5.68, \"min\": 5.52, \"mean\": 5.60},\n",
    "      { \"month\": 8, \"monthName\": \"Aug\", \"max\": 5.38, \"min\": 5.25, \"mean\": 5.31},\n",
    "      { \"month\": 9, \"monthName\": \"Sep\", \"max\": 4.59, \"min\": 4.40, \"mean\": 4.52},\n",
    "      { \"month\": 10, \"monthName\": \"Oct\", \"max\": 4.11, \"min\": 3.91, \"mean\": 4.03},\n",
    "      { \"month\": 11, \"monthName\": \"Nov\", \"max\": 3.44, \"min\": 3.13, \"mean\": 3.32},\n",
    "      { \"month\": 12, \"monthName\": \"Dec\", \"max\": 3.14, \"min\": 2.84, \"mean\": 3.03}\n",
    "]\n",
    "\n",
    "# convert pv list to dataframe, pv_df\n",
    "pv_df = pd.DataFrame(pv)\n",
    "\n",
    "# create output CSV of pv_df for plotting\n",
    "pv_output_df = pd.DataFrame({\n",
    "    'month': pv_df['month'],\n",
    "    'monthName': pv_df['monthName'],\n",
    "    'maxPv': pv_df['max'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save pv_output_df for pv data to CSV\n",
    "pv_output_df.to_csv('data/processed/pv.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   month monthName  maxPv\n",
      "0      1       Jan   3.31\n",
      "1      2       Feb   3.94\n",
      "2      3       Mar   4.53\n",
      "3      4       Apr   4.87\n",
      "4      5       May   5.17\n",
      "5      6       Jun   5.47\n",
      "6      7       Jul   5.68\n",
      "7      8       Aug   5.38\n",
      "8      9       Sep   4.59\n",
      "9     10       Oct   4.11\n",
      "\n",
      "Total number of records: 12\n",
      "Month names: ['Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep' 'Oct' 'Nov' 'Dec']\n",
      "PV values: [3.31 3.94 4.53 4.87 5.17 5.47 5.68 5.38 4.59 4.11 3.44 3.14]\n"
     ]
    }
   ],
   "source": [
    "# pv data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(pv_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(pv_output_df)}\")\n",
    "print(f\"Month names: {pv_output_df['monthName'].unique()}\")\n",
    "print(f\"PV values: {pv_output_df['maxPv'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 RISK IDENTIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URBAN BUILT-UP AREA EXPOSED TO RIVER FLOODING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate built-up area exposed to river flooding data (i.e., built-up area exposed to fluvial flooding) for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "fu = [\n",
    "      { \"year\": 1, \"yearName\": \"1985\", \"fu\": 3.351537941381818},\n",
    "      { \"year\": 2, \"yearName\": \"1986\", \"fu\": 3.4746915665244567},\n",
    "      { \"year\": 3, \"yearName\": \"1987\", \"fu\": 3.6828798375989176},\n",
    "      { \"year\": 4, \"yearName\": \"1988\", \"fu\": 3.9775688691902324},\n",
    "      { \"year\": 5, \"yearName\": \"1989\", \"fu\": 4.116849754768217},\n",
    "      { \"year\": 6, \"yearName\": \"1990\", \"fu\": 4.237071150740793},\n",
    "      { \"year\": 7, \"yearName\": \"1991\", \"fu\": 4.441594135352676},\n",
    "      { \"year\": 8, \"yearName\": \"1992\", \"fu\": 4.598468395951038},\n",
    "      { \"year\": 9, \"yearName\": \"1993\", \"fu\": 4.801525265977889},\n",
    "      { \"year\": 10, \"yearName\": \"1994\", \"fu\": 5.022908568317633},\n",
    "      { \"year\": 11, \"yearName\": \"1995\", \"fu\": 5.185647287256121},\n",
    "      { \"year\": 12, \"yearName\": \"1996\", \"fu\": 5.427556193786304},\n",
    "      { \"year\": 13, \"yearName\": \"1997\", \"fu\": 5.631346121105671},\n",
    "      { \"year\": 14, \"yearName\": \"1998\", \"fu\": 5.777957579608812},\n",
    "      { \"year\": 15, \"yearName\": \"1999\", \"fu\": 6.023531772601575},\n",
    "      { \"year\": 16, \"yearName\": \"2000\", \"fu\": 6.172342402982264},\n",
    "      { \"year\": 17, \"yearName\": \"2001\", \"fu\": 6.3563397834037065},\n",
    "      { \"year\": 18, \"yearName\": \"2002\", \"fu\": 6.53667187736257},\n",
    "      { \"year\": 19, \"yearName\": \"2003\", \"fu\": 6.648829643117474},\n",
    "      { \"year\": 20, \"yearName\": \"2004\", \"fu\": 6.747059320314579},\n",
    "      { \"year\": 21, \"yearName\": \"2005\", \"fu\": 6.824763393321244},\n",
    "      { \"year\": 22, \"yearName\": \"2006\", \"fu\": 6.928857528858474},\n",
    "      { \"year\": 23, \"yearName\": \"2007\", \"fu\": 7.034417778980736},\n",
    "      { \"year\": 24, \"yearName\": \"2008\", \"fu\": 7.114321023864949},\n",
    "      { \"year\": 25, \"yearName\": \"2009\", \"fu\": 7.191292039579098},\n",
    "      { \"year\": 26, \"yearName\": \"2010\", \"fu\": 7.28658948760614},\n",
    "      { \"year\": 27, \"yearName\": \"2011\", \"fu\": 7.3327720970346295},\n",
    "      { \"year\": 28, \"yearName\": \"2012\", \"fu\": 7.450061263837143},\n",
    "      { \"year\": 29, \"yearName\": \"2013\", \"fu\": 7.5050405607758215},\n",
    "      { \"year\": 30, \"yearName\": \"2014\", \"fu\": 7.6149991546531774},\n",
    "      { \"year\": 31, \"yearName\": \"2015\", \"fu\": 7.672177623469403},  \n",
    "\n",
    "]\n",
    "\n",
    "# convert fu list to dataframe, fu_df\n",
    "fu_df = pd.DataFrame(fu)\n",
    "\n",
    "# create output CSV of fu_df for plotting\n",
    "fu_output_df = pd.DataFrame({\n",
    "    'year': fu_df['year'],\n",
    "    'yearName': fu_df['yearName'],\n",
    "    'fu': fu_df['fu'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save fu_output_df for fu data to CSV\n",
    "fu_output_df.to_csv('data/processed/fu.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   year yearName    fu\n",
      "0     1     1985  3.35\n",
      "1     2     1986  3.47\n",
      "2     3     1987  3.68\n",
      "3     4     1988  3.98\n",
      "4     5     1989  4.12\n",
      "5     6     1990  4.24\n",
      "6     7     1991  4.44\n",
      "7     8     1992  4.60\n",
      "8     9     1993  4.80\n",
      "9    10     1994  5.02\n",
      "\n",
      "Total number of records: 31\n",
      "Year names: ['1985' '1986' '1987' '1988' '1989' '1990' '1991' '1992' '1993' '1994'\n",
      " '1995' '1996' '1997' '1998' '1999' '2000' '2001' '2002' '2003' '2004'\n",
      " '2005' '2006' '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014'\n",
      " '2015']\n",
      "fu values: [3.35 3.47 3.68 3.98 4.12 4.24 4.44 4.6  4.8  5.02 5.19 5.43 5.63 5.78\n",
      " 6.02 6.17 6.36 6.54 6.65 6.75 6.82 6.93 7.03 7.11 7.19 7.29 7.33 7.45\n",
      " 7.51 7.61 7.67]\n"
     ]
    }
   ],
   "source": [
    "# fu data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(fu_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(fu_output_df)}\")\n",
    "print(f\"Year names: {fu_output_df['yearName'].unique()}\")\n",
    "print(f\"fu values: {fu_output_df['fu'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URBAN BUILT-UP AREA EXPOSED TO RAINWATER FLOODING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate built-up area exposed to rainwater flooding data (i.e., built-up area exposed to pluvial flooding) for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "pu = [\n",
    "      { \"year\": 1, \"yearName\": \"1985\", \"pu\": 29.78265168032819},\n",
    "      { \"year\": 2, \"yearName\": \"1986\", \"pu\": 31.410038869713063},\n",
    "      { \"year\": 3, \"yearName\": \"1987\", \"pu\": 33.22875401244453},\n",
    "      { \"year\": 4, \"yearName\": \"1988\", \"pu\": 34.975629540509466},\n",
    "      { \"year\": 5, \"yearName\": \"1989\", \"pu\": 36.07668159386806},\n",
    "      { \"year\": 6, \"yearName\": \"1990\", \"pu\": 37.00913046994804},\n",
    "      { \"year\": 7, \"yearName\": \"1991\", \"pu\": 38.23993366408192},\n",
    "      { \"year\": 8, \"yearName\": \"1992\", \"pu\": 39.27867584757668},\n",
    "      { \"year\": 9, \"yearName\": \"1993\", \"pu\": 40.277099879983076},\n",
    "      { \"year\": 10, \"yearName\": \"1994\", \"pu\": 41.435330402157895},\n",
    "      { \"year\": 11, \"yearName\": \"1995\", \"pu\": 42.235095908292536},\n",
    "      { \"year\": 12, \"yearName\": \"1996\", \"pu\": 43.18880344585547},\n",
    "      { \"year\": 13, \"yearName\": \"1997\", \"pu\": 44.074336655214445},\n",
    "      { \"year\": 14, \"yearName\": \"1998\", \"pu\": 44.960602921865934},\n",
    "      { \"year\": 15, \"yearName\": \"1999\", \"pu\": 45.93630217820434},\n",
    "      { \"year\": 16, \"yearName\": \"2000\", \"pu\": 46.66862641342754},\n",
    "      { \"year\": 17, \"yearName\": \"2001\", \"pu\": 47.94121387323481},\n",
    "      { \"year\": 18, \"yearName\": \"2002\", \"pu\": 49.155156749640824},\n",
    "      { \"year\": 19, \"yearName\": \"2003\", \"pu\": 50.08247422467319},\n",
    "      { \"year\": 20, \"yearName\": \"2004\", \"pu\": 50.62566967842733},\n",
    "      { \"year\": 21, \"yearName\": \"2005\", \"pu\": 51.11975029358292},\n",
    "      { \"year\": 22, \"yearName\": \"2006\", \"pu\": 51.73405230471108},\n",
    "      { \"year\": 23, \"yearName\": \"2007\", \"pu\": 52.47737239932201},\n",
    "      { \"year\": 24, \"yearName\": \"2008\", \"pu\": 53.22948918144312},\n",
    "      { \"year\": 25, \"yearName\": \"2009\", \"pu\": 53.95374978644864},\n",
    "      { \"year\": 26, \"yearName\": \"2010\", \"pu\": 54.567318740284286},\n",
    "      { \"year\": 27, \"yearName\": \"2011\", \"pu\": 54.98369528243321},\n",
    "      { \"year\": 28, \"yearName\": \"2012\", \"pu\": 55.5386196528676},\n",
    "      { \"year\": 29, \"yearName\": \"2013\", \"pu\": 56.0422300128259},\n",
    "      { \"year\": 30, \"yearName\": \"2014\", \"pu\": 56.627209732253434},\n",
    "      { \"year\": 31, \"yearName\": \"2015\", \"pu\": 56.92336487842978},  \n",
    "\n",
    "]\n",
    "\n",
    "# convert pu list to dataframe, pu_df\n",
    "pu_df = pd.DataFrame(pu)\n",
    "\n",
    "# create output CSV of pu_df for plotting\n",
    "pu_output_df = pd.DataFrame({\n",
    "    'year': pu_df['year'],\n",
    "    'yearName': pu_df['yearName'],\n",
    "    'pu': pu_df['pu'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save pu_output_df for pu data to CSV\n",
    "pu_output_df.to_csv('data/processed/pu.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   year yearName     pu\n",
      "0     1     1985  29.78\n",
      "1     2     1986  31.41\n",
      "2     3     1987  33.23\n",
      "3     4     1988  34.98\n",
      "4     5     1989  36.08\n",
      "5     6     1990  37.01\n",
      "6     7     1991  38.24\n",
      "7     8     1992  39.28\n",
      "8     9     1993  40.28\n",
      "9    10     1994  41.44\n",
      "\n",
      "Total number of records: 31\n",
      "Year names: ['1985' '1986' '1987' '1988' '1989' '1990' '1991' '1992' '1993' '1994'\n",
      " '1995' '1996' '1997' '1998' '1999' '2000' '2001' '2002' '2003' '2004'\n",
      " '2005' '2006' '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014'\n",
      " '2015']\n",
      "pu values: [29.78 31.41 33.23 34.98 36.08 37.01 38.24 39.28 40.28 41.44 42.24 43.19\n",
      " 44.07 44.96 45.94 46.67 47.94 49.16 50.08 50.63 51.12 51.73 52.48 53.23\n",
      " 53.95 54.57 54.98 55.54 56.04 56.63 56.92]\n"
     ]
    }
   ],
   "source": [
    "# pu data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(pu_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(pu_output_df)}\")\n",
    "print(f\"Year names: {pu_output_df['yearName'].unique()}\")\n",
    "print(f\"pu values: {pu_output_df['pu'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URBAN BUILT-UP AREA EXPOSED TO COASTAL FLOODING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate built-up area exposed to coastal flooding data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "cu = [\n",
    "      { \"year\": 1, \"yearName\": \"1985\", \"cu\": 2.2739437213837266},\n",
    "      { \"year\": 2, \"yearName\": \"1986\", \"cu\": 2.5451749196145386},\n",
    "      { \"year\": 3, \"yearName\": \"1987\", \"cu\": 2.833266435573212},\n",
    "      { \"year\": 4, \"yearName\": \"1988\", \"cu\": 2.955687003423335},\n",
    "      { \"year\": 5, \"yearName\": \"1989\", \"cu\": 3.063446425423144},\n",
    "      { \"year\": 6, \"yearName\": \"1990\", \"cu\": 3.1396843838447777},\n",
    "      { \"year\": 7, \"yearName\": \"1991\", \"cu\": 3.2349818318718198},\n",
    "      { \"year\": 8, \"yearName\": \"1992\", \"cu\": 3.317084248633579},\n",
    "      { \"year\": 9, \"yearName\": \"1993\", \"cu\": 3.3815932903749615},\n",
    "      { \"year\": 10, \"yearName\": \"1994\", \"cu\": 3.4636957071367207},\n",
    "      { \"year\": 11, \"yearName\": \"1995\", \"cu\": 3.53040392075565},\n",
    "      { \"year\": 12, \"yearName\": \"1996\", \"cu\": 3.600777420837158},\n",
    "      { \"year\": 13, \"yearName\": \"1997\", \"cu\": 3.6652864625785404},\n",
    "      { \"year\": 14, \"yearName\": \"1998\", \"cu\": 3.7437235928777213},\n",
    "      { \"year\": 15, \"yearName\": \"1999\", \"cu\": 3.811164863789166},\n",
    "      { \"year\": 16, \"yearName\": \"2000\", \"cu\": 3.8639449888502972},\n",
    "      { \"year\": 17, \"yearName\": \"2001\", \"cu\": 3.942382119149478},\n",
    "      { \"year\": 18, \"yearName\": \"2002\", \"cu\": 4.03254816612891},\n",
    "      { \"year\": 19, \"yearName\": \"2003\", \"cu\": 4.176960452754504},\n",
    "      { \"year\": 20, \"yearName\": \"2004\", \"cu\": 4.2385372653258235},\n",
    "      { \"year\": 21, \"yearName\": \"2005\", \"cu\": 4.319906624795067},\n",
    "      { \"year\": 22, \"yearName\": \"2006\", \"cu\": 4.385881781121481},\n",
    "      { \"year\": 23, \"yearName\": \"2007\", \"cu\": 4.469450312468272},\n",
    "      { \"year\": 24, \"yearName\": \"2008\", \"cu\": 4.6079981407537405},\n",
    "      { \"year\": 25, \"yearName\": \"2009\", \"cu\": 4.754609599256882},\n",
    "      { \"year\": 26, \"yearName\": \"2010\", \"cu\": 4.844042588943799},\n",
    "      { \"year\": 27, \"yearName\": \"2011\", \"cu\": 4.96499704220889},\n",
    "      { \"year\": 28, \"yearName\": \"2012\", \"cu\": 5.050764745433228},\n",
    "      { \"year\": 29, \"yearName\": \"2013\", \"cu\": 5.168053912235742},\n",
    "      { \"year\": 30, \"yearName\": \"2014\", \"cu\": 5.225232381051967},\n",
    "      { \"year\": 31, \"yearName\": \"2015\", \"cu\": 5.311733141568821},  \n",
    "\n",
    "]\n",
    "\n",
    "# convert cu list to dataframe, cu_df\n",
    "cu_df = pd.DataFrame(cu)\n",
    "\n",
    "# create output CSV of cu_df for plotting\n",
    "cu_output_df = pd.DataFrame({\n",
    "    'year': cu_df['year'],\n",
    "    'yearName': cu_df['yearName'],\n",
    "    'cu': cu_df['cu'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save cu_output_df for cu data to CSV\n",
    "cu_output_df.to_csv('data/processed/cu.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   year yearName    cu\n",
      "0     1     1985  2.27\n",
      "1     2     1986  2.55\n",
      "2     3     1987  2.83\n",
      "3     4     1988  2.96\n",
      "4     5     1989  3.06\n",
      "5     6     1990  3.14\n",
      "6     7     1991  3.23\n",
      "7     8     1992  3.32\n",
      "8     9     1993  3.38\n",
      "9    10     1994  3.46\n",
      "\n",
      "Total number of records: 31\n",
      "Year names: ['1985' '1986' '1987' '1988' '1989' '1990' '1991' '1992' '1993' '1994'\n",
      " '1995' '1996' '1997' '1998' '1999' '2000' '2001' '2002' '2003' '2004'\n",
      " '2005' '2006' '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014'\n",
      " '2015']\n",
      "cu values: [2.27 2.55 2.83 2.96 3.06 3.14 3.23 3.32 3.38 3.46 3.53 3.6  3.67 3.74\n",
      " 3.81 3.86 3.94 4.03 4.18 4.24 4.32 4.39 4.47 4.61 4.75 4.84 4.96 5.05\n",
      " 5.17 5.23 5.31]\n"
     ]
    }
   ],
   "source": [
    "# cu data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(cu_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(cu_output_df)}\")\n",
    "print(f\"Year names: {cu_output_df['yearName'].unique()}\")\n",
    "print(f\"cu values: {cu_output_df['cu'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URBAN BUILT-UP AREA EXPOSED TO COMBINED RIVER, RAINWATER, AND COASTAL FLOODING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate built-up area exposed to combined flooding data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "comb = [\n",
    "      { \"year\": 1, \"yearName\": \"1985\", \"comb\": 32.76106345981951},\n",
    "      { \"year\": 2, \"yearName\": \"1986\", \"comb\": 34.641355415122305},\n",
    "      { \"year\": 3, \"yearName\": \"1987\", \"comb\": 36.74669595922742},\n",
    "      { \"year\": 4, \"yearName\": \"1988\", \"comb\": 38.72814982089738},\n",
    "      { \"year\": 5, \"yearName\": \"1989\", \"comb\": 39.97801250463666},\n",
    "      { \"year\": 6, \"yearName\": \"1990\", \"comb\": 41.01895386000896},\n",
    "      { \"year\": 7, \"yearName\": \"1991\", \"comb\": 42.41176271578881},\n",
    "      { \"year\": 8, \"yearName\": \"1992\", \"comb\": 43.57512463901124},\n",
    "      { \"year\": 9, \"yearName\": \"1993\", \"comb\": 44.71063038511807},\n",
    "      { \"year\": 10, \"yearName\": \"1994\", \"comb\": 46.004476506408295},\n",
    "      { \"year\": 11, \"yearName\": \"1995\", \"comb\": 46.92373035122299},\n",
    "      { \"year\": 12, \"yearName\": \"1996\", \"comb\": 48.01525265977888},\n",
    "      { \"year\": 13, \"yearName\": \"1997\", \"comb\": 49.01514280677031},\n",
    "      { \"year\": 14, \"yearName\": \"1998\", \"comb\": 50.03116021419708},\n",
    "      { \"year\": 15, \"yearName\": \"1999\", \"comb\": 51.17326347593655},\n",
    "      { \"year\": 16, \"yearName\": \"2000\", \"comb\": 52.02727522171735},\n",
    "      { \"year\": 17, \"yearName\": \"2001\", \"comb\": 53.43621133793255},\n",
    "      { \"year\": 18, \"yearName\": \"2002\", \"comb\": 54.78357064157642},\n",
    "      { \"year\": 19, \"yearName\": \"2003\", \"comb\": 55.887554924105075},\n",
    "      { \"year\": 20, \"yearName\": \"2004\", \"comb\": 56.53850979985902},\n",
    "      { \"year\": 21, \"yearName\": \"2005\", \"comb\": 57.14034983701442},\n",
    "      { \"year\": 22, \"yearName\": \"2006\", \"comb\": 57.87780547328522},\n",
    "      { \"year\": 23, \"yearName\": \"2007\", \"comb\": 58.721554416970804},\n",
    "      { \"year\": 24, \"yearName\": \"2008\", \"comb\": 59.591693423186946},\n",
    "      { \"year\": 25, \"yearName\": \"2009\", \"comb\": 60.43690848145756},\n",
    "      { \"year\": 26, \"yearName\": \"2010\", \"comb\": 61.1435757114427},\n",
    "      { \"year\": 27, \"yearName\": \"2011\", \"comb\": 61.66038110266628},\n",
    "      { \"year\": 28, \"yearName\": \"2012\", \"comb\": 62.301073176325005},\n",
    "      { \"year\": 29, \"yearName\": \"2013\", \"comb\": 62.89631569784776},\n",
    "      { \"year\": 30, \"yearName\": \"2014\", \"comb\": 63.53700777150649},\n",
    "      { \"year\": 31, \"yearName\": \"2015\", \"comb\": 63.918197563614655},  \n",
    "\n",
    "]\n",
    "\n",
    "# convert comb list to dataframe, comb_df\n",
    "comb_df = pd.DataFrame(comb)\n",
    "\n",
    "# create output CSV of comb_df for plotting\n",
    "comb_output_df = pd.DataFrame({\n",
    "    'year': comb_df['year'],\n",
    "    'yearName': comb_df['yearName'],\n",
    "    'comb': comb_df['comb'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save comb_output_df for comb data to CSV\n",
    "comb_output_df.to_csv('data/processed/comb.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   year yearName   comb\n",
      "0     1     1985  32.76\n",
      "1     2     1986  34.64\n",
      "2     3     1987  36.75\n",
      "3     4     1988  38.73\n",
      "4     5     1989  39.98\n",
      "5     6     1990  41.02\n",
      "6     7     1991  42.41\n",
      "7     8     1992  43.58\n",
      "8     9     1993  44.71\n",
      "9    10     1994  46.00\n",
      "\n",
      "Total number of records: 31\n",
      "Year names: ['1985' '1986' '1987' '1988' '1989' '1990' '1991' '1992' '1993' '1994'\n",
      " '1995' '1996' '1997' '1998' '1999' '2000' '2001' '2002' '2003' '2004'\n",
      " '2005' '2006' '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014'\n",
      " '2015']\n",
      "comb values: [32.76 34.64 36.75 38.73 39.98 41.02 42.41 43.58 44.71 46.   46.92 48.02\n",
      " 49.02 50.03 51.17 52.03 53.44 54.78 55.89 56.54 57.14 57.88 58.72 59.59\n",
      " 60.44 61.14 61.66 62.3  62.9  63.54 63.92]\n"
     ]
    }
   ],
   "source": [
    "# comb data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(comb_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(comb_output_df)}\")\n",
    "print(f\"Year names: {comb_output_df['yearName'].unique()}\")\n",
    "print(f\"comb values: {comb_output_df['comb'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELEVATION (need alternative to donut chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# elevation data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "\n",
    "elevation = [\n",
    "      { \"bin\": \"-5-40m\", \"count\": 413599, \"total\": 549697, \"percentage\": 75.24},\n",
    "      { \"bin\": \"40-90m\", \"count\": 94379, \"total\": 549697, \"percentage\": 17.17 },\n",
    "      { \"bin\": \"90-135m\", \"count\": 32786 , \"total\": 549697, \"percentage\": 5.96 },\n",
    "      { \"bin\": \"135-185m\", \"count\": 8043, \"total\": 549697, \"percentage\": 1.46 },\n",
    "      { \"bin\": \"135-235\", \"count\": 890, \"total\": 549697, \"percentage\": 0.16 },\n",
    "]\n",
    "\n",
    "# convert elevation list to dataframe, elevation_df\n",
    "elevation_df = pd.DataFrame(elevation)\n",
    "\n",
    "# create output CSV of elevation_df for plotting\n",
    "elevation_output_df = pd.DataFrame({\n",
    "    'bin': elevation_df['bin'],\n",
    "    'count': elevation_df['count'],\n",
    "    'total': elevation_df['total'], \n",
    "    'percentage': elevation_df['percentage']\n",
    "})\n",
    "\n",
    "# save elevation_output_df for elevation data to CSV\n",
    "elevation_output_df.to_csv('data/processed/elevation.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLOPE (need alternative to donut chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# slope data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "## This needs to be automated given the tabular-output from the GCP - hard coded for now\n",
    "\n",
    "slope = [\n",
    "      { \"bin\": \"0-2\", \"count\": 428343, \"total\": 549702, \"percentage\": 77.92 },\n",
    "      { \"bin\": \"2-5\", \"count\": 79034, \"total\": 549702, \"percentage\": 14.38 },\n",
    "      { \"bin\": \"5-10\", \"count\": 31121, \"total\": 549702, \"percentage\": 5.66 },\n",
    "      { \"bin\": \"10-20\", \"count\": 10147, \"total\": 549702, \"percentage\": 1.85 },\n",
    "      { \"bin\": \"20+\", \"count\": 1057, \"total\": 549702, \"percentage\": 0.19 },\n",
    "]\n",
    "\n",
    "# convert slope list to dataframe, slope_df\n",
    "slope_df = pd.DataFrame(slope)\n",
    "\n",
    "# create output CSV of slope_df for plotting\n",
    "slope_output_df = pd.DataFrame({\n",
    "    'bin': slope_df['bin'],\n",
    "    'count': slope_df['count'],\n",
    "    'total': slope_df['total'], \n",
    "    'percentage': slope_df['percentage']\n",
    "})\n",
    "\n",
    "# save slope_output_df for slope data to CSV\n",
    "slope_output_df.to_csv('data/processed/slope.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EARTHQUAKE EVENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate earthquake event data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "# manually added earthquake event data for Tunis, Tunisia\n",
    "ee = [\n",
    "    {\"begin_year\": 1903, \"distance\": 316, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"MAY 1903; MNA; 316 km away; NA damage\", \"line1\": \"MAY 1903\", \"line2\": \"M5.3, 316 km away\", \"line3\": \"Palermo\"},\n",
    "    {\"begin_year\": 1906, \"distance\": 335, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"SEPTEMBER 1906; MNA; 336 km away; NA damage\", \"line1\": \"SEPTEMBER 1906\", \"line2\": \"M5.6, 335 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1907, \"distance\": 335, \"eqMagnitude\": 5.1, \"severity\": \"Moderate\", \"text\": \"FEBRUARY 1907; MNA; 336 km away; NA damage\", \"line1\": \"FEBRUARY 1907\", \"line2\": \"M5.1, 335 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1908, \"distance\": 497, \"eqMagnitude\": 7.0, \"severity\": \"Very Large\", \"text\": \"DECEMBER 1908; M7; 498 km away; Extreme damage; 78,000 fatalities\", \"line1\": \"DECEMBER 1908\", \"line2\": \"M7.0, 498 km away, 78,000 fatalities\", \"line3\": \"Messina, Sicily, Calabria\"},\n",
    "    {\"begin_year\": 1909, \"distance\": 452, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"OCTOBER 1909; MNA; 453 km away; NA damage\", \"line1\": \"OCTOBER 1909\", \"line2\": \"M5.3, 453 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1911, \"distance\": 652, \"eqMagnitude\": 4.3, \"severity\": \"Small\", \"text\": \"OCTOBER 1911; M4.3; 453 km away; NA damage\", \"line1\": \"OCTOBER 1911\", \"line2\": \"M4.3, 453 km away\", \"line3\": \"Etna\"},\n",
    "    {\"begin_year\": 1914, \"distance\": 452, \"eqMagnitude\": 4.9, \"severity\": \"Small\", \"text\": \"MAY 1914; M4.9; 453 km away; Severe damage; 120 fatalities\", \"line1\": \"MAY 1914\", \"line2\": \"M4.9, 453 km away, 120 fatalities\", \"line3\": \"Catania, Etna\"},\n",
    "    {\"begin_year\": 1916, \"distance\": 494, \"eqMagnitude\": 5.1, \"severity\": \"Moderate\", \"text\": \"JULY 1916; M5.1; 494 km away; Limited damage\", \"line1\": \"JULY 1916\", \"line2\": \"M5.1, 494 km away\", \"line3\": \"Stromboli Island\"},\n",
    "    {\"begin_year\": 1924, \"distance\": 489, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"MARCH 1924; M5.6; 489 km away; NA damage\", \"line1\": \"MARCH 1924\", \"line2\": \"M5.6, 489 km away\", \"line3\": \"Batna\"},\n",
    "    {\"begin_year\": 1926, \"distance\": 451, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"AUGUST 1926; M5.3; 451 km away; Severe damage\", \"line1\": \"AUGUST 1926\", \"line2\": \"M5.3, 451 km away\", \"line3\": \"Salina Island\"},\n",
    "    {\"begin_year\": 1930, \"distance\": 434, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"MARCH 1930; MNA; 434 km away; Moderate damage\", \"line1\": \"MARCH 1930\", \"line2\": \"M5.0, 434 km away\", \"line3\": \"Filicudi Island\"},\n",
    "    {\"begin_year\": 1931, \"distance\": 444, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"JULY 1931; MNA; 444 km away; NA damage\", \"line1\": \"JULY 1931\", \"line2\": \"M5.0, 444 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1933, \"distance\": 256, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"FEBRUARY 1933; MNA; 256 km away; NA damage\", \"line1\": \"FEBRUARY 1933\", \"line2\": \"M5.0, 256 km away\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1939, \"distance\": 437, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"JANUARY 1939; MNA; 438 km away; NA damage\", \"line1\": \"JANUARY 1939\", \"line2\": \"M5.0, 438 km away\", \"line3\": \"Calabria\"},\n",
    "    {\"begin_year\": 1940, \"distance\": 323, \"eqMagnitude\": 4.8, \"severity\": \"Small\", \"text\": \"JANUARY 1940; M4.8; 324 km away; NA damage\", \"line1\": \"JANUARY 1940\", \"line2\": \"M4.8, 324 km away\", \"line3\": \"NA\"},\n",
    "    {\"begin_year\": 1941, \"distance\": 240, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"MARCH 1941; MNA; 241 km away; NA damage\", \"line1\": \"MARCH 1941\", \"line2\": \"M5.0, 241 km away\", \"line3\": \"Calabria\"},\n",
    "    {\"begin_year\": 1946, \"distance\": 481, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"FEBRUARY 1946; M5.6; 481 km away; Severe damage; 264 fatalities\", \"line1\": \"FEBRUARY 1946\", \"line2\": \"M5.6, 481 km away, 264 fatalities\", \"line3\": \"Hodna Mountains\"},\n",
    "    {\"begin_year\": 1947, \"distance\": 196, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"AUGUST 1947; M5.3; 197 km away; Moderate damage; 3 fatalities\", \"line1\": \"AUGUST 1947\", \"line2\": \"M5.3, 197 km away, 3 fatalities\", \"line3\": \"NA\"},\n",
    "    {\"begin_year\": 1957, \"distance\": 135, \"eqMagnitude\": 5.6, \"severity\": \"Large\", \"text\": \"FEBRUARY 1957; M5.6; 135 km away; Moderate damage; 13 fatalities\", \"line1\": \"FEBRUARY 1957\", \"line2\": \"M5.6, 135 km away, 13 fatalities\", \"line3\": \"Sidi Abd,Sidi Toul\"},\n",
    "    {\"begin_year\": 1961, \"distance\": 468, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"MARCH 1961; MNA; 469 km away; Moderate damage; 15 fatalities\", \"line1\": \"MARCH 1961\", \"line2\": \"M5.0, 469 km away, 15 fatalities\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1962, \"distance\": 141, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"FEBRUARY 1962; M5.3; 141 km away; Moderate damage\", \"line1\": \"FEBRUARY 1962\", \"line2\": \"M5.3, 141 km away\", \"line3\": \"Gafour,OUM-Zid,EL Akhou–∞—Ç\"},\n",
    "    {\"begin_year\": 1968, \"distance\": 282, \"eqMagnitude\": 6.0, \"severity\": \"Very Large\", \"text\": \"JANUARY 1968; M6; 283 km away; Extreme damage; 216 fatalities\", \"line1\": \"JANUARY 1968\", \"line2\": \"M6.0, 283 km away, 216 fatalities\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1968, \"distance\": 466, \"eqMagnitude\": 4.9, \"severity\": \"Small\", \"text\": \"FEBRUARY 1968; M4.9; 466 km away; Moderate damage; 1 fatality\", \"line1\": \"FEBRUARY 1968\", \"line2\": \"M4.9, 466 km away, 1 fatality\", \"line3\": \"El Asn (BABORD)\"},\n",
    "    {\"begin_year\": 1975, \"distance\": 446, \"eqMagnitude\": 4.3, \"severity\": \"Small\", \"text\": \"JULY 1975; M4.3; 446 km away; Moderate damage; 1 fatality\", \"line1\": \"JULY 1975\", \"line2\": \"M4.3, 446 km away, 1 fatality\", \"line3\": \"Djebel Babor\"},\n",
    "    {\"begin_year\": 1978, \"distance\": 462, \"eqMagnitude\": 5.7, \"severity\": \"Large\", \"text\": \"APRIL 1978; M5.7; 463 km away; Moderate damage; 5 fatalities\", \"line1\": \"APRIL 1978\", \"line2\": \"M5.7, 463 km away, 5 fatalities\", \"line3\": \"Sicily\"},\n",
    "    {\"begin_year\": 1990, \"distance\": 467, \"eqMagnitude\": 5.3, \"severity\": \"Moderate\", \"text\": \"DECEMBER 1990; M5.3; 468 km away; Extreme damage; 19 fatalities\", \"line1\": \"DECEMBER 1990\", \"line2\": \"M5.3, 468 km away, 19 fatalities\", \"line3\": \"Sicily: Carlentini\"},\n",
    "    {\"begin_year\": 2002, \"distance\": 553, \"eqMagnitude\": 6.0, \"severity\": \"Very Large\", \"text\": \"SEPTEMBER 2002; M6; 354 km away; Extreme damage; 2 fatalities\", \"line1\": \"SEPTEMBER 2002\", \"line2\": \"M6.0, 354 km away, 2 fatalities\", \"line3\": \"Sicily: Palermo\"},\n",
    "    {\"begin_year\": 2018, \"distance\": 442, \"eqMagnitude\": 5.0, \"severity\": \"Moderate\", \"text\": \"DECEMBER 2018; M5; 443 km away; Extreme damage\", \"line1\": \"DECEMBER 2018\", \"line2\": \"M5.0, 443 km away\", \"line3\": \"Sicily: Catpana\"},\n",
    "    {\"begin_year\": 2021, \"distance\": 444, \"eqMagnitude\": 6.0, \"severity\": \"Very Large\", \"text\": \"MARCH 2021; M6; 445 km away; Limited damage\", \"line1\": \"MARCH 2021\", \"line2\": \"M6.0, 445 km away\", \"line3\": \"NA\"}\n",
    "]\n",
    "\n",
    "# convert ee list to dataframe, ee_df\n",
    "ee_df = pd.DataFrame(ee)\n",
    "\n",
    "\n",
    "# save ee_output_df for ee data to CSV\n",
    "ee_df.to_csv('data/processed/ee.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   begin_year  distance  eqMagnitude    severity           line1  \\\n",
      "0        1903       316          5.3    Moderate        MAY 1903   \n",
      "1        1906       335          5.6       Large  SEPTEMBER 1906   \n",
      "2        1907       335          5.1    Moderate   FEBRUARY 1907   \n",
      "3        1908       497          7.0  Very Large   DECEMBER 1908   \n",
      "4        1909       452          5.3    Moderate    OCTOBER 1909   \n",
      "5        1911       652          4.3       Small    OCTOBER 1911   \n",
      "6        1914       452          4.9       Small        MAY 1914   \n",
      "7        1916       494          5.1    Moderate       JULY 1916   \n",
      "8        1924       489          5.6       Large      MARCH 1924   \n",
      "9        1926       451          5.3    Moderate     AUGUST 1926   \n",
      "\n",
      "                                  line2                      line3  \n",
      "0                     M5.3, 316 km away                    Palermo  \n",
      "1                     M5.6, 335 km away                     Sicily  \n",
      "2                     M5.1, 335 km away                     Sicily  \n",
      "3  M7.0, 498 km away, 78,000 fatalities  Messina, Sicily, Calabria  \n",
      "4                     M5.3, 453 km away                     Sicily  \n",
      "5                     M4.3, 453 km away                       Etna  \n",
      "6     M4.9, 453 km away, 120 fatalities              Catania, Etna  \n",
      "7                     M5.1, 494 km away           Stromboli Island  \n",
      "8                     M5.6, 489 km away                      Batna  \n",
      "9                     M5.3, 451 km away              Salina Island  \n"
     ]
    }
   ],
   "source": [
    "# ee data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(ee_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HISTORICAL BURNT AREA & FIRE WEATHER INDEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# generate historical burnt area & fire weather index data for Tunis, Tunisia\n",
    "# Note: City Scan GitHub (https://github.com/rosemaryturtle/city-scan-automation/)\n",
    "\n",
    "fwi = [\n",
    "      { \"week\": 1, \"monthName\": \"Jan\", \"fwi\": 36.03855628967285},\n",
    "      { \"week\": 2, \"monthName\": \"Jan\", \"fwi\": 28.35186767578125},\n",
    "      { \"week\": 3, \"monthName\": \"Jan\", \"fwi\": 34.48613758087156},\n",
    "      { \"week\": 4, \"monthName\": \"Jan\", \"fwi\": 35.160119628906244},\n",
    "      { \"week\": 5, \"monthName\": \"Feb\", \"fwi\": 41.2155460357666},\n",
    "      { \"week\": 6, \"monthName\": \"Feb\", \"fwi\": 42.91906299591064},\n",
    "      { \"week\": 7, \"monthName\": \"Feb\", \"fwi\": 40.35708732604981},\n",
    "      { \"week\": 8, \"monthName\": \"Feb\", \"fwi\": 35.13482322692871},\n",
    "      { \"week\": 9, \"monthName\": \"Feb\", \"fwi\": 43.7328405380249},\n",
    "      { \"week\": 10, \"monthName\": \"Mar\", \"fwi\": 55.629536437988264},\n",
    "      { \"week\": 11, \"monthName\": \"Mar\", \"fwi\": 51.963145637512206},\n",
    "      { \"week\": 12, \"monthName\": \"Mar\", \"fwi\": 48.64410858154295},\n",
    "      { \"week\": 13, \"monthName\": \"Mar\", \"fwi\": 48.45940856933592},\n",
    "      { \"week\": 14, \"monthName\": \"Apr\", \"fwi\": 42.525428390502924},\n",
    "      { \"week\": 15, \"monthName\": \"Apr\", \"fwi\": 48.989934921264634},\n",
    "      { \"week\": 16, \"monthName\": \"Apr\", \"fwi\": 47.94815864562989},\n",
    "      { \"week\": 17, \"monthName\": \"Apr\", \"fwi\": 59.693392562866215},\n",
    "      { \"week\": 18, \"monthName\": \"May\", \"fwi\": 53.26485347747803},\n",
    "      { \"week\": 19, \"monthName\": \"May\", \"fwi\": 67.04015121459962},\n",
    "      { \"week\": 20, \"monthName\": \"May\", \"fwi\": 66.2925880432129},\n",
    "      { \"week\": 21, \"monthName\": \"May\", \"fwi\": 63.51103172302246},\n",
    "      { \"week\": 22, \"monthName\": \"May\", \"fwi\": 57.59551124572754},\n",
    "      { \"week\": 23, \"monthName\": \"Jun\", \"fwi\": 66.97727813720704},\n",
    "      { \"week\": 24, \"monthName\": \"Jun\", \"fwi\": 75.7531303405762},\n",
    "      { \"week\": 25, \"monthName\": \"Jun\", \"fwi\": 80.30134506225586},\n",
    "      { \"week\": 26, \"monthName\": \"Jun\", \"fwi\": 90.69736862182619},\n",
    "      { \"week\": 27, \"monthName\": \"Jul\", \"fwi\": 75.26012268066407},\n",
    "      { \"week\": 28, \"monthName\": \"Jul\", \"fwi\": 95.59054870605469},\n",
    "      { \"week\": 29, \"monthName\": \"Jul\", \"fwi\": 82.06852722167967},\n",
    "      { \"week\": 30, \"monthName\": \"Jul\", \"fwi\": 81.8968620300293},\n",
    "      { \"week\": 31, \"monthName\": \"Aug\", \"fwi\": 81.7047821044922},\n",
    "      { \"week\": 32, \"monthName\": \"Aug\", \"fwi\": 81.58447265625001}, \n",
    "      { \"week\": 33, \"monthName\": \"Aug\", \"fwi\": 65.29224243164063},  \n",
    "      { \"week\": 34, \"monthName\": \"Aug\", \"fwi\": 67.29769515991212},  \n",
    "      { \"week\": 35, \"monthName\": \"Aug\", \"fwi\": 64.21281738281252},  \n",
    "      { \"week\": 36, \"monthName\": \"Sep\", \"fwi\": 69.20558013916019},  \n",
    "      { \"week\": 37, \"monthName\": \"Sep\", \"fwi\": 59.376176834106474},  \n",
    "      { \"week\": 38, \"monthName\": \"Sep\", \"fwi\": 50.01441955566406},  \n",
    "      { \"week\": 39, \"monthName\": \"Sep\", \"fwi\": 40.38814010620118 },  \n",
    "      { \"week\": 40, \"monthName\": \"Oct\", \"fwi\": 48.369334793090815},  \n",
    "      { \"week\": 41, \"monthName\": \"Oct\", \"fwi\": 43.82190437316895},  \n",
    "      { \"week\": 42, \"monthName\": \"Oct\", \"fwi\": 37.03949813842773},  \n",
    "      { \"week\": 43, \"monthName\": \"Oct\", \"fwi\": 50.04811096191406},  \n",
    "      { \"week\": 44, \"monthName\": \"Nov\", \"fwi\": 47.38101158142093},  \n",
    "      { \"week\": 45, \"monthName\": \"Nov\", \"fwi\": 37.50416679382325},  \n",
    "      { \"week\": 46, \"monthName\": \"Nov\", \"fwi\": 29.76080322265625},  \n",
    "      { \"week\": 47, \"monthName\": \"Nov\", \"fwi\": 36.063685607910124},  \n",
    "      { \"week\": 48, \"monthName\": \"Nov\", \"fwi\": 34.42437210083008},  \n",
    "      { \"week\": 49, \"monthName\": \"Dec\", \"fwi\": 32.008924865722626},  \n",
    "      { \"week\": 50, \"monthName\": \"Dec\", \"fwi\": 33.579549407958986},  \n",
    "      { \"week\": 51, \"monthName\": \"Dec\", \"fwi\": 31.927024841308594},  \n",
    "      { \"week\": 52, \"monthName\": \"Dec\", \"fwi\": 34.5278169631958},  \n",
    "      { \"week\": 53, \"monthName\": \"Dec\", \"fwi\": 31.089004516601562}  \n",
    "\n",
    "]\n",
    "\n",
    "# convert fwi list to dataframe, fwi_df\n",
    "fwi_df = pd.DataFrame(fwi)\n",
    "\n",
    "# create output CSV of fwi_df for plotting\n",
    "fwi_output_df = pd.DataFrame({\n",
    "    'week': fwi_df['week'],\n",
    "    'monthName': fwi_df['monthName'],\n",
    "    'fwi': fwi_df['fwi'].round(2),  # round the count to 2 decimal places\n",
    "})\n",
    "\n",
    "# save fwi_output_df for fwi data to CSV\n",
    "fwi_output_df.to_csv('data/processed/fwi.csv', index=False)\n",
    "\n",
    "print(\"csv file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of the output:\n",
      "   week monthName    fwi\n",
      "0     1       Jan  36.04\n",
      "1     2       Jan  28.35\n",
      "2     3       Jan  34.49\n",
      "3     4       Jan  35.16\n",
      "4     5       Feb  41.22\n",
      "5     6       Feb  42.92\n",
      "6     7       Feb  40.36\n",
      "7     8       Feb  35.13\n",
      "8     9       Feb  43.73\n",
      "9    10       Mar  55.63\n",
      "\n",
      "Total number of records: 53\n",
      "Month names: ['Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep' 'Oct' 'Nov' 'Dec']\n",
      "fwi values: [36.04 28.35 34.49 35.16 41.22 42.92 40.36 35.13 43.73 55.63 51.96 48.64\n",
      " 48.46 42.53 48.99 47.95 59.69 53.26 67.04 66.29 63.51 57.6  66.98 75.75\n",
      " 80.3  90.7  75.26 95.59 82.07 81.9  81.7  81.58 65.29 67.3  64.21 69.21\n",
      " 59.38 50.01 40.39 48.37 43.82 37.04 50.05 47.38 37.5  29.76 36.06 34.42\n",
      " 32.01 33.58 31.93 34.53 31.09]\n"
     ]
    }
   ],
   "source": [
    "# fwi data check\n",
    "print(\"\\nFirst 10 rows of the output:\")\n",
    "print(fwi_output_df.head(10))\n",
    "\n",
    "# summary statistics\n",
    "print(f\"\\nTotal number of records: {len(fwi_output_df)}\")\n",
    "print(f\"Month names: {fwi_output_df['monthName'].unique()}\")\n",
    "print(f\"fwi values: {fwi_output_df['fwi'].unique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
